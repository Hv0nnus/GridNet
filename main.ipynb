{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Torch related package\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#cuda related package\n",
    "import torch.cuda\n",
    "import torch.backends.cudnn as cudnn\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other package\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run Annexe.ipynb #Good trick to launch another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire pour la suite (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le batch normalisation, il y en existe plusieurs, regarder exactement lequel on veut.\n",
    "\n",
    "On peut enlever le biais dans les convolutions avant la normalisation\n",
    "\n",
    "Le diminution de la taille des images, pas clair cette division par deux...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gridNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features maps for the input\n",
    "    (2) = nOutput : number of features maps for the output\n",
    "    (3) = nColumns : number of columns of the gridNet, this number should be divisible by two.\n",
    "    It count the number of bloc +1\n",
    "    (4) = nFeatMaps : number of feature at each row of the gridNet\n",
    "    (5) = dropFactor : factor witch control the dropout of an entire bloc \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, steps=1):\n",
    "        super(testNet, self).__init__()\n",
    "        self.steps = steps\n",
    "        self.i2h = ListModle(self, 'i2h_')\n",
    "        self.h2h = ListModle(self, 'h2h_')\n",
    "        for i in range(steps):\n",
    "            self.i2h.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.h2h.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        for inp, i2h, h2h in zip(inputs, self.i2h, self.h2h):\n",
    "            print(4)\n",
    "            hidden = F.tanh(i2h(inp) + h2h(hidden))\n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def __init__(self,nInputs, nOutputs, nColumns, nFeatMaps, dropFactor):\n",
    "        super(gridNet, self).__init__()\n",
    "        len_nfeatureMaps = len(nFeatMaps)\n",
    "        self.nColumns = nColumns\n",
    "        self.nFeatMaps = nFeatMaps\n",
    "        \n",
    "        \n",
    "        # A normalisation before any computation\n",
    "        self.batchNormInitial = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        # The first convolution before entering into the grid.\n",
    "        self.firstConv = firstConv(nInputs = nInputs, nOutputs = nFeatMaps[0])\n",
    "        \n",
    "        \n",
    "        # We create the Grid \n",
    "        for i in range(len(nFeatMaps)):\n",
    "            for j in range(nColumns):\n",
    "                #We don t creat a residual bloc on the last column\n",
    "                if(j < (nColumns - 1)):\n",
    "                    setattr(self, \"convSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i) + \"_\" + str(j + 1),\n",
    "                            convSequence(nFeatMaps[i], nFeatMaps[i],dropFactor))\n",
    "                \n",
    "                #We creat subSampling only on half of the grid and not in the last row\n",
    "                if(j < (nColumns // 2) and i < (len(nFeatMaps)-1)):\n",
    "                    setattr(self, \"subSamplingSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i + 1) + \"_\" + str(j),\n",
    "                            subSamplingSequence(nFeatMaps[i], nFeatMaps[i+1]))\n",
    "                \n",
    "                if(j >= (nColumns // 2) and i > 0):\n",
    "                    setattr(self, \"upSamplingSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i - 1) + \"_\" + str(j),\n",
    "                            upSamplingSequence(nFeatMaps[i], nFeatMaps[i-1]))\n",
    "\n",
    "        # The last convolution before the result.\n",
    "        self.lastConv = lastConv(nInputs = nFeatMaps[0], nOutputs = nOutputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        len_nfeatureMaps = len(self.nFeatMaps)\n",
    "\n",
    "        x = self.batchNormInitial(x)\n",
    "        x = self.firstConv(x)\n",
    "        \n",
    "        X = [[0 for i in range(self.nColumns)] for j in range(len_nfeatureMaps)] \n",
    "        X[0][0] = x\n",
    "        \n",
    "\n",
    "        for i in range(1,len_nfeatureMaps-1):\n",
    "            X[i][0] = getattr(self,\"subSamplingSequence\" + str(i-1) + \"_\" + str(0) + \"to\" + str(i) + \"_\" + str(0))\n",
    "\n",
    "        for j in range(1,nColumns//2):\n",
    "            for i in range(len_nfeatureMaps):\n",
    "                    X[i][j] = self.X_residual[i-1][j]\n",
    "        \n",
    "        for j in range(self.nColumns):\n",
    "            X[0][j] = getattr(self, prefix + str(i))\n",
    "            For example, getattr(x, 'foobar') is equivalent to x.foobar.\n",
    "    \n",
    "    \n",
    "        return X[0][self.nColumns-1]\n",
    "\n",
    "    \n",
    "torch.manual_seed(7)\n",
    "    \n",
    "network = gridNet(nInputs = 3,nOutputs = 3, nColumns = 5, nFeatMaps = [3,6,12,24], dropFactor = 0.1)\n",
    "#print(network)\n",
    "a = torch.randn(2, 3, 33, 33)\n",
    "inputs = Variable(a)\n",
    "targets = Variable(a)\n",
    "\n",
    "\n",
    "outputs = network(inputs)\n",
    "print(outputs.size())\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(outputs, targets)\n",
    "print(loss)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "for i in range(100):\n",
    "    outputs = network(inputs)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    #if(i%100==99):\n",
    "        #print(loss.data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatriceModuleResidual(object):\n",
    "    #Should work with all kind of module\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "\n",
    "    def append(self, new_module,i1,j1,i2,j2):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(i) + str(j) + str(i+1) + str(j+1), new_module)\n",
    "\n",
    "    def __getitem__(self, i,j):\n",
    "        return getattr(self.module, self.prefix + str(i) + str(j) + str(i+1) + str(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testNet (\n",
      "  (i2h_0): Linear (128 -> 256)\n",
      "  (h2h_0): Linear (256 -> 256)\n",
      "  (i2h_1): Linear (128 -> 256)\n",
      "  (h2h_1): Linear (256 -> 256)\n",
      "  (i2h_2): Linear (128 -> 256)\n",
      "  (h2h_2): Linear (256 -> 256)\n",
      ")\n",
      "4\n",
      "4\n",
      "4\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "class ListModle(object):\n",
    "    #Should work with all kind of module\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        self.num_module = 0\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "\n",
    "    def append(self, new_module):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
    "            self.num_module += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_module\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i < 0 or i >= self.num_module:\n",
    "            raise IndexError('Out of bound')\n",
    "        return getattr(self.module, self.prefix + str(i))\n",
    "\n",
    "\n",
    "class testNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, steps=1):\n",
    "        super(testNet, self).__init__()\n",
    "        self.steps = steps\n",
    "        self.i2h = ListModle(self, 'i2h_')\n",
    "        self.h2h = ListModle(self, 'h2h_')\n",
    "        for i in range(steps):\n",
    "            self.i2h.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.h2h.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        for inp, i2h, h2h in zip(inputs, self.i2h, self.h2h):\n",
    "            print(4)\n",
    "            hidden = F.tanh(i2h(inp) + h2h(hidden))\n",
    "        return hidden\n",
    "\n",
    "net = testNet(128, 256, 3)\n",
    "print(net)\n",
    "inp = Variable(torch.randn(6, 4, 128))\n",
    "init = Variable(torch.randn(4, 256))\n",
    "out = net(inp, init)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testNet (\n",
      "  (i2h_0): Linear (3 -> 3)\n",
      "  (h2h_0): Linear (3 -> 3)\n",
      "  (i2h_1): Linear (3 -> 3)\n",
      "  (h2h_1): Linear (3 -> 3)\n",
      "  (i2h_2): Linear (3 -> 3)\n",
      "  (h2h_2): Linear (3 -> 3)\n",
      "  (i2h_3): Linear (3 -> 3)\n",
      "  (h2h_3): Linear (3 -> 3)\n",
      "  (i2h_4): Linear (3 -> 3)\n",
      "  (h2h_4): Linear (3 -> 3)\n",
      ")\n",
      "<__main__.AttrProxy object at 0x7fcae1631cf8>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AttrProxy' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-4fda736e4d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-4fda736e4d8c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2h\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-4fda736e4d8c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#    return getattr(self.module, self.prefix + str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index {} is out of range'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AttrProxy' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "class AttrProxy(object):\n",
    "    \"\"\"Translates index lookups into attribute lookups.\"\"\"\n",
    "    def __init__(self, module, prefix):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "\n",
    "    #def __getitem__(self, i):\n",
    "    #    return getattr(self.module, self.prefix + str(i))\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "class testNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, steps=1):\n",
    "        super(testNet, self).__init__()\n",
    "        self.steps = steps\n",
    "        for i in range(steps):\n",
    "            self.add_module('i2h_' + str(i), nn.Linear(input_dim, hidden_dim))\n",
    "            self.add_module('h2h_' + str(i), nn.Linear(hidden_dim, input_dim))\n",
    "        self.i2h = AttrProxy(self, 'i2h_')\n",
    "        self.h2h = AttrProxy(self, 'h2h_')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # here, use self.i2h[t] and self.h2h[t] to index \n",
    "        # input2hidden and hidden2hidden modules for each step,\n",
    "        # or loop over them, like in the example below\n",
    "        # (assuming first dim of input is sequence length)\n",
    "        x=0\n",
    "        y=0\n",
    "        print(self.i2h)\n",
    "        for inp, i2h, h2h in zip(inputs, self.i2h, self.h2h):\n",
    "            x += F.tanh(i2h(inputs))\n",
    "        i = 0\n",
    "        for i2ha in self.i2h:\n",
    "            print(self.i2h)\n",
    "            print(i)\n",
    "            i = i + 1\n",
    "            print(i2ha)\n",
    "            y += F.tanh(i2ha(inputs))\n",
    "            \n",
    "        #for i2h,h2h in zip(self.i2h,self.h2h):\n",
    "            #print(i2h)\n",
    "            #y = F.tanh(i2h(inputs))\n",
    "            \n",
    "        return y\n",
    "print(testNet(3,3,5))\n",
    "z = testNet(3,3,5)\n",
    "a = torch.randn(3, 3)\n",
    "inputs = Variable(a)\n",
    "out = z(inputs)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstConv (\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (dropOut): Dropout2d (p=0.5)\n",
      ")\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.2448  0.0000  0.0000\n",
      "  0.2194  0.0000  0.0000  ...   0.4013  0.1989  0.1344\n",
      "  0.1668  0.3783  0.0000  ...   0.0000  0.1836  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.2413  0.4212  0.0000  ...   0.3038  0.0000  0.0635\n",
      "  0.0000  0.0000  0.0000  ...   0.3256  0.0000  0.0000\n",
      "  0.0000  0.1842  0.3408  ...   0.3764  0.1104  0.0000\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.1016  0.0749  ...   0.0000  0.0181  0.0000\n",
      "  0.0494  0.0005  0.0644  ...   0.1323  0.0904  0.1407\n",
      "  0.0000  0.0000  0.0000  ...   0.0499  0.0000  0.0421\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.1420  0.0000  ...   0.2384  0.0895  0.1182\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0873\n",
      "  0.0000  0.0000  0.0142  ...   0.0000  0.0294  0.0634\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.1017  0.0000\n",
      "  0.2978  0.7175  0.0081  ...   0.0000  1.2972  0.0000\n",
      "  0.0000  0.0000  0.7916  ...   0.5764  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.2553  0.0000  0.0000\n",
      "  0.2032  0.4036  0.0000  ...   0.3834  0.1374  0.0000\n",
      "  0.0043  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "     ⋮ \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0598  0.0000  0.0000  ...   0.0000  0.0000  0.0039\n",
      "  0.4040  0.0000  0.3592  ...   0.1730  0.0000  0.0000\n",
      "  0.0351  0.0000  0.3440  ...   0.0000  0.0859  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.1974  0.0000  ...   0.1327  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0223  0.0000  0.0000\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      "  0.1476  0.0000  0.1619  ...   0.0000  0.2239  0.0295\n",
      "  0.0000  0.0704  0.0865  ...   0.0000  0.0236  0.0000\n",
      "  0.0000  0.0143  0.0000  ...   0.0355  0.0428  0.0264\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0546  0.1223  ...   0.0901  0.0000  0.1024\n",
      "  0.0000  0.0000  0.0000  ...   0.0257  0.0409  0.0543\n",
      "  0.0000  0.0586  0.0019  ...   0.0456  0.0201  0.0000\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      "  0.4224  0.0000  0.0000  ...   0.1885  0.0000  0.0000\n",
      "  0.8413  0.5554  0.2009  ...   0.0000  0.3113  0.0000\n",
      "  0.0000  0.5397  0.4961  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.2357  0.1439  1.7095  ...   0.0000  0.1058  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.3121  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 2x3x16x16]\n",
      "\n",
      "Variable containing:\n",
      " 1.0562\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class firstConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    (3) = nMiddleput : number of features map after the first Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs):\n",
    "        super(firstConv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "                \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = firstConv(nInputs = 3,nOutputs = 3)\n",
    "print(net)\n",
    "a = torch.randn(2, 3, 16, 16)\n",
    "inputs = Variable(a)\n",
    "out = net(inputs)\n",
    "print(out)\n",
    "\n",
    "target = Variable(a)  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double_conv (\n",
      "  (conv): Sequential (\n",
      "    (0): Conv2d(4, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = f.ReLU(inplace=True)\n",
    "        return x\n",
    "print(double_conv(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstConv (\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (dropOut): Dropout2d (p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class firstConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    (3) = nMiddleput : number of features map after the first Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs):\n",
    "        super(firstConv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.dropOut = nn.Dropout2d(0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = firstConv(nInputs = 3,nOutputs = 3)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityscapesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GridNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class color():\n",
    "    convolution        = \"darkgoldenrod1\"\n",
    "    subSampling        = \"darkgoldenrod\" \n",
    "    fullConvolution    = \"firebrick1\"\n",
    "    upSampling         = \"firebrick\"\n",
    "    batchNormalization = \"deepskyblue3\"\n",
    "    relu               = \"darkolivegreen3\"\n",
    "    add                = \"bisque3\"\n",
    "    dropout            = \"darkviolet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstConv (\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU1): ReLU ()\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU2): ReLU ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class firstConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInputs : number of features map for the input\n",
    "    (2) = nOutputs : number of features map for the output\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs):\n",
    "        super(firstConv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "                \n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = firstConv(nInputs = 3,nOutputs = 3)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convSequence (\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU1): ReLU ()\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU2): ReLU ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class convSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    (3) = dropFactor : number of features map after the first Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs,dropFactor):\n",
    "        super(convSequence, self).__init__()\n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x_init):\n",
    "        x = self.batch1(x_init)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = (torch.rand(1) > dropFactor)*x\n",
    "        x = x_init + x\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = convSequence(nInputs = 3,nOutputs = 3,dropFactor = 0.1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subSamplingSequence (\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (ReLU1): ReLU ()\n",
      "  (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU2): ReLU ()\n",
      ")\n",
      "torch.Size([2, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class subSamplingSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    This class represente a bloc that reduce the resolution of each feature map(factor2)\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(subSamplingSequence, self).__init__()\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(2,2), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "network = subSamplingSequence(nInputs = 3,nOutputs = 6)\n",
    "print(network)\n",
    "a = torch.randn(2, 3, 7, 7)\n",
    "inputs = Variable(a)\n",
    "out = network(inputs)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upSamplingSequence (\n",
      "  (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convTranspose1): ConvTranspose2d(6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (ReLU1): ReLU ()\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU2): ReLU ()\n",
      ")\n",
      "torch.Size([2, 3, 31, 31])\n"
     ]
    }
   ],
   "source": [
    "class upSamplingSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    This class represente a bloc that increase the resolution of each feature map(factor2)\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(upSamplingSequence, self).__init__()\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.convTranspose1 = nn.ConvTranspose2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(2,2), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.convTranspose1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "network = upSamplingSequence(nInputs = 6,nOutputs = 3)\n",
    "print(network)\n",
    "a = torch.randn(2, 6, 16, 16)\n",
    "inputs = Variable(a)\n",
    "out = network(inputs)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lastConv (\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU1): ReLU ()\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU2): ReLU ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class lastConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInputs : number of features map for the input\n",
    "    (2) = nOutputs : number of features map for the output\n",
    "    This class represente the last Convolution of the network before the prediction\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs):\n",
    "        super(lastConv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "                \n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = lastConv(nInputs = 3,nOutputs = 3)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-58c6073f4252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-58c6073f4252>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_residual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "class gridNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features maps for the input\n",
    "    (2) = nOutput : number of features maps for the output\n",
    "    (3) = nColumns : number of columns of the gridNet, this number should be divisible by two.\n",
    "    It count the number of bloc +1\n",
    "    (4) = nFeatMaps : number of feature at each row of the gridNet\n",
    "    (5) = dropFactor : factor witch control the dropout of an entire bloc \n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs, nOutputs, nColumns, nFeatMaps, dropFactor):\n",
    "        super(gridNet, self).__init__()\n",
    "        len_nfeatureMaps = len(nFeatMaps)\n",
    "        self.nColumns = nColumns\n",
    "        self.nFeatMaps = nFeatMaps\n",
    "        \n",
    "        \n",
    "        # A normalisation before any computation\n",
    "        self.batchNormInitial = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        # The first convolution before entering into the grid.\n",
    "        self.firstConv = firstConv(nInputs = nInputs, nOutputs = nFeatMaps[0])\n",
    "        \n",
    "        \n",
    "        # We create the Grid. We will creat conv and sub/up sequences with different name.\n",
    "        # The name is : \"sequenceName\" + starting position of the sequence(i,j) + \"to\" + ending position (k,l)\n",
    "        for i in range(len(nFeatMaps)):\n",
    "            for j in range(nColumns):\n",
    "                #We don t creat a residual bloc on the last column\n",
    "                if(j < (nColumns - 1)):\n",
    "                    setattr(self, \"convSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i) + \"_\" + str(j + 1),\n",
    "                            convSequence(nFeatMaps[i], nFeatMaps[i],dropFactor))\n",
    "                \n",
    "                #We creat subSampling only on half of the grid and not in the last row\n",
    "                if(j < (nColumns // 2) and i < (len(nFeatMaps)-1)):\n",
    "                    setattr(self, \"subSamplingSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i + 1) + \"_\" + str(j),\n",
    "                            subSamplingSequence(nFeatMaps[i], nFeatMaps[i+1]))\n",
    "                \n",
    "                #Welook a the other half but not the first row\n",
    "                if(j >= (nColumns // 2) and i > 0):\n",
    "                    setattr(self, \"upSamplingSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i - 1) + \"_\" + str(j),\n",
    "                            upSamplingSequence(nFeatMaps[i], nFeatMaps[i-1]))\n",
    "\n",
    "        # The last convolution before the result.\n",
    "        self.lastConv = lastConv(nInputs = nFeatMaps[0], nOutputs = nOutputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        len_nfeatureMaps = len(self.nFeatMaps)\n",
    "\n",
    "        # A normalisation before any computation\n",
    "        x = self.batchNormInitial(x)\n",
    "        # The first convolution before entering into the grid.\n",
    "        x = self.firstConv(x)\n",
    "        \n",
    "        # X is the matrix that represente the values of the features maps at the point (i,j) in the grid.\n",
    "        X = [[0 for i in range(self.nColumns)] for j in range(len_nfeatureMaps)]\n",
    "        #The input of the grid is on (0,0)\n",
    "        X[0][0] = x\n",
    "        \n",
    "        #We did the special case for j=0\n",
    "        for i in range(1,len_nfeatureMaps-1):\n",
    "            X[i][0] = getattr(self,\"subSamplingSequence\" + str(i-1) + \"_\" + str(0) + \"to\" + str(i) + \"_\" + str(0))\n",
    "\n",
    "        for j in range(1,nColumns//2):\n",
    "            for i in range(len_nfeatureMaps):\n",
    "                    X[i][j] = self.X_residual[i-1][j]\n",
    "\n",
    "        for j in range(self.nColumns):\n",
    "            X[0][j] = getattr(self.prefix + str(i))\n",
    "    \n",
    "    \n",
    "        return X[0][self.nColumns-1]\n",
    "\n",
    "    \n",
    "torch.manual_seed(7)\n",
    "    \n",
    "network = gridNet(nInputs = 3,nOutputs = 3, nColumns = 5, nFeatMaps = [3,6,12,24], dropFactor = 0.1)\n",
    "#print(network)\n",
    "a = torch.randn(2, 3, 33, 33)\n",
    "inputs = Variable(a)\n",
    "targets = Variable(a)\n",
    "\n",
    "\n",
    "outputs = network(inputs)\n",
    "print(outputs.size())\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(outputs, targets)\n",
    "print(loss)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "for i in range(100):\n",
    "    outputs = network(inputs)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    #if(i%100==99):\n",
    "        #print(loss.data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeroTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
