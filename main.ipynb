{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Torch related package\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6021\n"
     ]
    }
   ],
   "source": [
    "#cuda related package\n",
    "import torch.cuda\n",
    "import torch.backends.cudnn as cudnn\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other package\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run Annexe.ipynb #Good trick to launch another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire pour la suite (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le batch normalisation, il y en existe plusieurs, regarder exactement lequel on veut.\n",
    "\n",
    "On peut enlever le biais dans les convolutions avant la normalisation\n",
    "\n",
    "Le diminution de la taille des images, pas clair cette division par deux....\n",
    "\n",
    "Change la fonction de cout\n",
    "\n",
    "Il faut enlever les biais quand il y a une batch normalisation apres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchvision.datasets.cifar.CIFAR10 object at 0x7f018f3d2978>\n"
     ]
    }
   ],
   "source": [
    "print(trainset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityscapesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class color():\n",
    "    convolution        = \"darkgoldenrod1\"\n",
    "    subSampling        = \"darkgoldenrod\" \n",
    "    fullConvolution    = \"firebrick1\"\n",
    "    upSampling         = \"firebrick\"\n",
    "    batchNormalization = \"deepskyblue3\"\n",
    "    relu               = \"darkolivegreen3\"\n",
    "    add                = \"bisque3\"\n",
    "    dropout            = \"darkviolet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['road', 'sidewalk','building', 'wall', 'fence', 'pole', 'traffic light', 'traffic sign',\n",
    "           'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus','train', 'motorcycle', 'bicycle']\n",
    "number_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GridNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstConv(\n",
      "  (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class firstConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInputs : number of features map for the input\n",
    "    (2) = nOutputs : number of features map for the output\n",
    "    This is the first convolution used to enter into the grid.\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs):\n",
    "        super(firstConv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "                \n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = firstConv(nInputs = 3,nOutputs = 3)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convSequence(\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU1): ReLU()\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class convSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    (3) = dropFactor : Total Dropout on the entire Sequence, there is a probability p = dropFactor that\n",
    "        the residual is deleted.\n",
    "    This class reprensent a residual bloc that doesn't change number nor the size of the features maps\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs,dropFactor):\n",
    "        super(convSequence, self).__init__()\n",
    "        self.dropFactor = dropFactor\n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x_init):\n",
    "        x = self.batch1(x_init)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        # Small trick that transform boolean into integer\n",
    "        x = ((random.random() > self.dropFactor) * 1) * x\n",
    "        x = x_init + x\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = convSequence(nInputs = 3,nOutputs = 3,dropFactor = 0.1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subSamplingSequence(\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d (3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (ReLU1): ReLU()\n",
      "  (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU2): ReLU()\n",
      ")\n",
      "torch.Size([2, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class subSamplingSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    This class represente a bloc that reduce the resolution of each feature map(factor2)\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(subSamplingSequence, self).__init__()\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(2,2), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "network = subSamplingSequence(nInputs = 3,nOutputs = 6)\n",
    "print(network)\n",
    "a = torch.randn(2, 3, 7, 7)\n",
    "inputs = Variable(a)\n",
    "out = network(inputs)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upSamplingSequence(\n",
      "  (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convTranspose1): ConvTranspose2d (6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (ReLU1): ReLU()\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU2): ReLU()\n",
      ")\n",
      "torch.Size([2, 3, 31, 31])\n"
     ]
    }
   ],
   "source": [
    "class upSamplingSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    This class represente a bloc that increase the resolution of each feature map(factor2)\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(upSamplingSequence, self).__init__()\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.convTranspose1 = nn.ConvTranspose2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(2,2), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.convTranspose1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "network = upSamplingSequence(nInputs = 6,nOutputs = 3)\n",
    "print(network)\n",
    "a = torch.randn(2, 6, 16, 16)\n",
    "inputs = Variable(a)\n",
    "out = network(inputs)\n",
    "print(out.size())\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lastConv(\n",
      "  (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class lastConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInputs : number of features map for the input\n",
    "    (2) = nOutputs : number of features map for the output\n",
    "    This class represente the last Convolution of the network before the prediction\n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs,nOutputs):\n",
    "        super(lastConv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "                \n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = lastConv(nInputs = 3,nOutputs = 3)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridNet(\n",
      "  (batchNormInitial): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (firstConv): firstConv(\n",
      "    (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence0_0to0_1): convSequence(\n",
      "    (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (subSamplingSequence0_0to1_0): subSamplingSequence(\n",
      "    (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence0_1to0_2): convSequence(\n",
      "    (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (subSamplingSequence0_1to1_1): subSamplingSequence(\n",
      "    (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence0_2to0_3): convSequence(\n",
      "    (batch1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence1_0to1_1): convSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (subSamplingSequence1_0to2_0): subSamplingSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence1_1to1_2): convSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (subSamplingSequence1_1to2_1): subSamplingSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence1_2to1_3): convSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (upSamplingSequence1_2to0_2): upSamplingSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (convTranspose1): ConvTranspose2d (6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (upSamplingSequence1_3to0_3): upSamplingSequence(\n",
      "    (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (convTranspose1): ConvTranspose2d (6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence2_0to2_1): convSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (subSamplingSequence2_0to3_0): subSamplingSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence2_1to2_2): convSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (subSamplingSequence2_1to3_1): subSamplingSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence2_2to2_3): convSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (upSamplingSequence2_2to1_2): upSamplingSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (convTranspose1): ConvTranspose2d (12, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (upSamplingSequence2_3to1_3): upSamplingSequence(\n",
      "    (batch1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (convTranspose1): ConvTranspose2d (12, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence3_0to3_1): convSequence(\n",
      "    (batch1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence3_1to3_2): convSequence(\n",
      "    (batch1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (convSequence3_2to3_3): convSequence(\n",
      "    (batch1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (upSamplingSequence3_2to2_2): upSamplingSequence(\n",
      "    (batch1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (convTranspose1): ConvTranspose2d (24, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (upSamplingSequence3_3to2_3): upSamplingSequence(\n",
      "    (batch1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (convTranspose1): ConvTranspose2d (24, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (ReLU1): ReLU()\n",
      "    (batch2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv2): Conv2d (12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (lastConv): lastConv(\n",
      "    (conv1): Conv2d (3, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (conv2): Conv2d (19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (batchNormFinal): BatchNorm2d(3, eps=1e-05, momentum=0, affine=True)\n",
      "  (logsoftmax1): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class gridNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features maps for the input\n",
    "    (2) = nOutput : number of features maps for the output\n",
    "    (3) = nColumns : number of columns of the gridNet, this number should be divisible by two.\n",
    "    It count the number of bloc +1\n",
    "    (4) = nFeatMaps : number of feature at each row of the gridNet\n",
    "    (5) = dropFactor : factor witch control the dropout of an entire bloc \n",
    "    \"\"\"\n",
    "    def __init__(self,nInputs, nOutputs, nColumns, nFeatMaps, dropFactor):\n",
    "        super(gridNet, self).__init__()\n",
    "        \n",
    "        #Define some parameters as an attribut of the class\n",
    "        len_nfeatureMaps = len(nFeatMaps)\n",
    "        self.nColumns = nColumns\n",
    "        self.nFeatMaps = nFeatMaps\n",
    "        self.len_nfeatureMaps = len_nfeatureMaps\n",
    "        \n",
    "        # A normalisation before any computation\n",
    "        self.batchNormInitial = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        # The first convolution before entering into the grid.\n",
    "        self.firstConv = firstConv(nInputs = nInputs, nOutputs = nInputs)\n",
    "        \n",
    "        \n",
    "        # We create the Grid. We will creat conv and sub/up sequences with different name.\n",
    "        # The name is : \"sequenceName\" + starting position of the sequence(i,j) + \"to\" + ending position (k,l)\n",
    "        for i in range(len(nFeatMaps)):\n",
    "            for j in range(nColumns):\n",
    "                #We don t creat a residual bloc on the last column\n",
    "                if(j < (nColumns - 1)):\n",
    "                    setattr(self, \"convSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i) + \"_\" + str(j + 1),\n",
    "                            convSequence(nFeatMaps[i], nFeatMaps[i],dropFactor))\n",
    "                \n",
    "                #We creat subSampling only on half of the grid and not in the last row\n",
    "                if(j < (nColumns // 2) and i < (len(nFeatMaps)-1)):\n",
    "                    setattr(self, \"subSamplingSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i + 1) + \"_\" + str(j),\n",
    "                            subSamplingSequence(nFeatMaps[i], nFeatMaps[i+1]))\n",
    "                \n",
    "                #Welook a the other half but not the first row\n",
    "                if(j >= (nColumns // 2) and i > 0):\n",
    "                    setattr(self, \"upSamplingSequence\" + str(i) + \"_\" + str(j) + \"to\" + str(i - 1) + \"_\" + str(j),\n",
    "                            upSamplingSequence(nFeatMaps[i], nFeatMaps[i-1]))\n",
    "\n",
    "        # The last convolution before the result.\n",
    "        self.lastConv = lastConv(nInputs = nFeatMaps[0], nOutputs = nOutputs)    \n",
    "    \n",
    "        self.batchNormFinal = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0,affine=True)\n",
    "        \n",
    "        self.logsoftmax1 = nn.LogSoftmax()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"This function return the fusion of the actual value on (i,j) and the new data which come from the sampling\n",
    "    (1) = X_i_j : The value on the grid a the position (i,j)\n",
    "    (2) = SamplingSequence : The sampling that should be added to the point (i,j)\n",
    "    \"\"\"\n",
    "    def addTransform(self,X_i_j,SamplingSequence):\n",
    "        return(X_i_j + SamplingSequence)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # A normalisation before any computation\n",
    "        x = self.batchNormInitial(x)\n",
    "        # The first convolution before entering into the grid.\n",
    "        x = self.firstConv(x)\n",
    "        \n",
    "        # X is the matrix that represente the values of the features maps at the point (i,j) in the grid.\n",
    "        X = [[0 for i in range(self.nColumns)] for j in range(self.len_nfeatureMaps)]\n",
    "        #The input of the grid is on (0,0)\n",
    "        X[0][0] = x\n",
    "        \n",
    "        # Looking on half of the grid, with sumsampling and convolution sequence\n",
    "        for j in range(self.nColumns//2):\n",
    "            for i in range(self.len_nfeatureMaps):\n",
    "                #For the first column, there is only subsampling\n",
    "                if(j > 0):\n",
    "                    #This syntaxe call self.conSequencei_(j-1)toi_j(X[i][j-1])\n",
    "                    X[i][j] = getattr(self,\"convSequence\"\n",
    "                                      + str(i) + \"_\" + str(j-1) + \"to\" + str(i) + \"_\" + str(j))(X[i][j-1])\n",
    "                \n",
    "                # For the first row, there is only ConvSequence (residual bloc)\n",
    "                if(i > 0):\n",
    "                    X[i][j] = self.addTransform(X[i][j] , getattr(self,\"subSamplingSequence\"\n",
    "                                                + str(i-1) + \"_\" + str(j) + \"to\" + str(i) + \"_\" + str(j))(X[i-1][j]))\n",
    "\n",
    "        # Looking on the other half of the grid\n",
    "        for j in range(self.nColumns//2,self.nColumns):\n",
    "            for i in range(self.len_nfeatureMaps-1,-1,-1):\n",
    "                X[i][j] = getattr(self,\"convSequence\" +\n",
    "                                      str(i) + \"_\" + str(j-1) + \"to\" + str(i) + \"_\" + str(j))(X[i][j-1])\n",
    "\n",
    "                \n",
    "                # There is no upSampling on the last row\n",
    "                if(i < (self.len_nfeatureMaps - 1)):\n",
    "                    X[i][j] = self.addTransform(X[i][j], getattr(self,\"upSamplingSequence\"\n",
    "                                                + str(i+1) + \"_\" + str(j) + \"to\" + str(i) + \"_\" + str(j))(X[i+1][j]))\n",
    "\n",
    "        x_final = self.lastConv(X[0][self.nColumns - 1])\n",
    "\n",
    "        if(False):\n",
    "            print(\"Size of different X_i_j\")\n",
    "            for i1,i2 in enumerate(X):\n",
    "                for j1,j2 in enumerate(i2):\n",
    "                    print(\"Dim(X(\" + str(i1) + \")(\" + str(j1) + \")) : \",j2.size())\n",
    "             \n",
    "\n",
    "        return x_final\n",
    "\n",
    "\n",
    "network = gridNet(nInputs = 3,nOutputs = number_classes, nColumns = 4, nFeatMaps = [3,6,12,24], dropFactor = 0.1)\n",
    "print(network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self,nColumns, nFeatMaps, dropFactor,learning_rate,number_classes, beta1, beta2,epsilon,\n",
    "                weight_decay,width_image, height_image,nFeatureMaps_init,batch_size,epoch_number):\n",
    "        super(Parameters, self).__init__()\n",
    "        # Image\n",
    "        self.number_classes = number_classes\n",
    "        self.width_image = width_image\n",
    "        self.height_image = height_image\n",
    "        # Number of feature map at the begining, if RGB image it would be 3\n",
    "        self.nFeatureMaps_init = nFeatureMaps_init\n",
    "        \n",
    "        # GridNet\n",
    "        self.nColumns = nColumns\n",
    "        self.nFeatMaps = nFeatMaps\n",
    "        \n",
    "        # Learning\n",
    "        self.dropFactor = dropFactor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_number = epoch_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the loss function between the y_train_estimated and y_train\"\"\"\n",
    "def criterion(y_estimated, y):\n",
    "    nllcrit = nn.NLLLoss2d()\n",
    "    return nllcrit(F.log_softmax(y_estimated, dim = 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train return\n",
    "    (0) = parameters : list of parameters of the network\n",
    "    (1) = x_train : inputs of the training set\n",
    "    (2) = y_train : outputs of the training set\n",
    "    (3) = x_validation : inputs of the validation set\n",
    "    (4) = y_validation : outputs of the validation set\n",
    "\"\"\"\n",
    "def train(parameters,network,x_train,y_train,x_validation,y_validation):\n",
    "     \n",
    "    # We will use this one a lot, so we compute it now\n",
    "    x_train_len = x_train.size()[0]\n",
    "        \n",
    "    # create your optimizer\n",
    "    optimizer = optim.Adam(params = network.parameters(), lr=parameters.learning_rate,\n",
    "                           betas = (parameters.beta1, parameters.beta2),\n",
    "                           eps = parameters.epsilon, weight_decay = parameters.weight_decay)\n",
    "\n",
    "    #list\n",
    "    conf_mat_train = np.zeros((parameters.epoch_number,parameters.number_classes,parameters.number_classes))\n",
    "    IoU_classes_train = np.zeros((parameters.epoch_number,parameters.number_classes))\n",
    "    IoU_total_train = np.zeros((parameters.epoch_number))\n",
    "    loss_train = np.zeros((parameters.epoch_number))\n",
    "    \n",
    "    for epoch in range(parameters.epoch_number):\n",
    "        \n",
    "        # We apply a random permutation at each epoch\n",
    "        random_perm = torch.randperm(x_train_len)\n",
    "        \n",
    "        #trainloader=torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=8)\n",
    "        #testloader=torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=8)\n",
    "        \n",
    "        for batch in range(0,x_train.size()[0], batch_size):\n",
    "            # zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            indices = random_perm[batch:min(batch+batch_size,x_train_len-1)]\n",
    "            x_batch, y_batch = x_train[indices], y_train[indices]\n",
    "            \n",
    "            # Compute the forward function\n",
    "            y_batch_estimated = network(x_batch)\n",
    "\n",
    "            #Get the error\n",
    "            loss = criterion(y_batch_estimated, y_batch)            \n",
    "            \n",
    "            #Compute the backward function\n",
    "            loss.backward()\n",
    "\n",
    "            # Does the update according to the optimizer define above\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save the error\n",
    "        save_error(x_train,y_train,x_validation,y_validation,network,True)\n",
    "        \n",
    "        \n",
    "    \n",
    "        conf_mat_train[epoch], b, IoU_total_train[epoch] = IoU(y_train_estimated,y_train)\n",
    "\n",
    "        IoU_classes_train = pd.DataFrame(columns=[\"Value\",\"Target\",\"Prediction\",\"Epoch\"])\n",
    "        epoch_axis = pd.DataFrame([epoch]*parameters.number_classes,columns=[\"Epoch\"])\n",
    "        b = pd.concat[(b,epoch_axis),axis = 1]\n",
    "        IoU_classes_train = pd.concat[(IoU_classes_train,b)]\n",
    "        \n",
    "        \n",
    "    return(loss_train,conf_mat_train,pd.DataFrame(IoU_classes_train,columns=[\"t\" + str(i) for i in range(19)]),IoU_total_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Generator expression must be parenthesized if not sole argument (<ipython-input-272-1af47f4cfe97>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-272-1af47f4cfe97>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pd.DataFrame([10,4] for i in range(parameters.number_classes),columns=[\"Epoch\"])\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Generator expression must be parenthesized if not sole argument\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame([10,4] for i in range(parameters.number_classes),columns=[\"Epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ok</th>\n",
       "      <th>bla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ok, bla]\n",
       "Index: []"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(columns=[\"ok\",\"bla\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'a'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a\".encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Data\n",
    "RESULTS = ['apple','cherry','orange','pineapple','strawberry']\n",
    "\n",
    "# Open File\n",
    "resultFyle = open(\"output.csv\",'w')\n",
    "\n",
    "# Write data to file\n",
    "for r in RESULTS:\n",
    "    resultFyle.write(r + \",\")\n",
    "resultFyle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv    \n",
    "\n",
    "with open('v.csv', 'w') as csvfile:\n",
    "    cwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    cwriter.writerow(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-3eee1cb58147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresultFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mRESULTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mresultFyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "RESULTS = ['apple','cherry']\n",
    "#RESULTS = ['apple','cherry','orange','pineapple','strawberry']\n",
    "\n",
    "with open(\"output.csv\",'wb') as resultFile:\n",
    "    for r in RESULTS:\n",
    "        resultFyle.write(r + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('<U10') and format specifier ('%.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not numpy.str_",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-c637f1e73933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"True\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Group\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"foo.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[1;32m   1218\u001b[0m                                     \u001b[0;34m\"format specifier ('%s')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                                     % (str(X.dtype), format))\n\u001b[0m\u001b[1;32m   1220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0mfooter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfooter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('<U10') and format specifier ('%.18e')"
     ]
    }
   ],
   "source": [
    "a = np.asarray([\"Value\",\"True\",\"Prediction\",\"Type\", \"Group\",\"Epoch\"])\n",
    "np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_scv(name_network,train_number):\n",
    "    \n",
    "    if(exists(\"CSV/CSV_confMat_\" + \"train\" + \"_\" + name_network + str(train_number) + \".csv\")):\n",
    "        print(\"TODO ici il faudra faire arreter le programme ! On ecrase les fichiers !\")\n",
    "        #TODO ici il faudra faire arreter le programme !\n",
    "    header_confMat = [\"Value\",\"Target\",\"Prediction\",\"Epoch\"]\n",
    "    header_loss = [\"Value\"]\n",
    "    \n",
    "    for t in [\"train\",\"validation\"]:\n",
    "        with open(\"CSV/CSV_confMat_\" + t + \"_\" + name_network + str(train_number) + \".csv\", 'w') as csvfile:\n",
    "            cwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "            cwriter.writerow(header_confMat)\n",
    "\n",
    "        with open(\"CSV/CSV_loss_\" + t + \"_\" + name_network + str(train_number) + \".csv\", 'w') as csvfile:\n",
    "            cwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "            cwriter.writerow(header_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO ici il faudra faire arreter le programme ! On ecrase les fichiers !\n"
     ]
    }
   ],
   "source": [
    "init_scv(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('foo.csv', 'a') as csvfile:\n",
    "    writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    writer.writerows([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"foo.csv\", [1,2,3], delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_error(x_train,y_train,x_validation,y_validation,network,epoch):\n",
    "    \n",
    "    y_train_estimated = network(x_train)\n",
    "    y_validation_estimated = network(y_validation)\n",
    "    conf_mat_train = IoU(y_train_estimated,y_train)\n",
    "    conf_mat_validation = IoU(y_validation_estimated,y_validation)\n",
    "    \n",
    "    for t in [\"train\",\"validation\"]:\n",
    "        with open(\"CSV/CSV_confMat_\" + t + \"_\" + name_network + str(train_number) + \".csv\", 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writerows()\n",
    "            \n",
    "    loss_train = criterion(y_estimated = y_train_estimated,y = y_train)\n",
    "    loss_validation = criterion(y_estimated = y_validation_estimated,y = y_validation)\n",
    "    \n",
    "    \n",
    "    for t in [\"train\",\"validation\"]:\n",
    "        with open(\"CSV/CSV_confMat_\" + t + \"_\" + name_network + str(train_number) + \".csv\", 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writerows()\n",
    "    \n",
    "    loss_train[epoch] = loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test function : IoU\n",
    "    return a matrix of confusion\n",
    "\"\"\"\n",
    "def IoU(y_train_estimated, y_train):\n",
    "    \n",
    "    #We keep only the higest value, which is the prediction\n",
    "    pred = torch.max(y_train_estimated, dim=1)[1]\n",
    "\n",
    "    confusion_matrix = [[0] * parameters.number_classes for i in range(parameters.number_classes)]\n",
    "    \n",
    "    #For each classes : [0] = TP [1] = FP [2] = FN\n",
    "    IoU_each_classes = np.zeros(shape = (3,parameters.number_classes))\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    target = y_train.view(-1)\n",
    "\n",
    "    # Double loop over the number of classes at each iteration we add the intersection\n",
    "    for cls1 in range(parameters.number_classes):\n",
    "        pred_inds = pred == cls1\n",
    "        for cls2 in range(parameters.number_classes):\n",
    "            target_inds = target == cls2\n",
    "            intersection = (pred_inds*target_inds).long().sum().data.cpu()[0]\n",
    "            confusion_matrix[cls1][cls2] = intersection\n",
    "\n",
    "            # TP here for each classes\n",
    "            if(cls1 == cls2):\n",
    "                IoU_each_classes[0,cls1] = intersection\n",
    "            # FN and FP here for each classes\n",
    "            else:\n",
    "                IoU_each_classes[1,cls1] = IoU_each_classes[1,cls1] + intersection\n",
    "                IoU_each_classes[2,cls2] = IoU_each_classes[2,cls1] + intersection\n",
    "    \n",
    "    # compute the IoU value\n",
    "    IoU_each_classes_total = IoU_each_classes[0,:]/np.sum(IoU_each_classes)\n",
    "\n",
    "    return(confusion_matrix,IoU_each_classes_total, np.mean((np.float32(IoU_each_classes_total))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'x_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-1ad435c7ebdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#y_train = y_train.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mplot_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconf_mat_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIoU_classes_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIoU_total_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'x_validation'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "random.seed(465)\n",
    "\n",
    "batch_size = 3\n",
    "image_size = 33\n",
    "a = torch.randn(batch_size, 3, image_size, image_size)\n",
    "\n",
    "b = (torch.rand(batch_size,image_size,image_size)*1000)%19\n",
    "\n",
    "x_train = Variable(a)\n",
    "y_train = Variable(b).long()\n",
    "\n",
    "x_validation = Variable(b)\n",
    "y_validation = Variable(b)\n",
    "\n",
    "\n",
    "parameters = Parameters(nColumns = 4, learning_rate=0.01, nFeatMaps = [3,6],dropFactor = 0.1,\n",
    "                        number_classes = 19, weight_decay = 5*10**(-6), beta1 = 0.9,\n",
    "                        beta2 = 0.999, epsilon = 1*10**(-8), width_image = image_size, height_image = image_size,\n",
    "                        nFeatureMaps_init = len(x_train[0,:,0,0]), batch_size = batch_size,epoch_number = 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the GridNet\n",
    "network = gridNet(nInputs = parameters.nFeatureMaps_init,nOutputs = parameters.number_classes, nColumns = parameters.nColumns,\n",
    "                      nFeatMaps = parameters.nFeatMaps, dropFactor = parameters.dropFactor)\n",
    "\n",
    "#network = network.cuda()\n",
    "#x_train = x_train.cuda()\n",
    "#y_train = y_train.cuda()\n",
    "\n",
    "train(network = network, parameters = parameters, x_train=x_train, y_train=y_train, x_validation= x_validation,y_validation = y_validation)\n",
    "plot_training(loss_train,conf_mat_train,IoU_classes_train,IoU_total_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize(x_train,y_train):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0173953c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training(loss_train,conf_mat_train,IoU_classes_train,IoU_total_train):\n",
    "    fig = plt.figure(1)\n",
    "    plt.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    res = ax.imshow(np.array(conf_mat_train[-1]), cmap=plt.cm.jet, \n",
    "                    interpolation='nearest')\n",
    "    cb = fig.colorbar(res)\n",
    "    plt.show()\n",
    "    \n",
    "    ggplot(aes(),data = IoU_classes_train)+ geom_point(x=\"0\",y=\"1\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "plot_training(loss_train,conf_mat_train,IoU_classes_train,IoU_total_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAH8CAYAAAB/4HFKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0173932668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (-9223363309091037822)>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggplot(aes(x=\"\",y=\"20\"),data = IoU_classes_train)+ geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>beef</th>\n",
       "      <th>veal</th>\n",
       "      <th>pork</th>\n",
       "      <th>lamb_and_mutton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1944-01-01</td>\n",
       "      <td>751.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1944-02-01</td>\n",
       "      <td>713.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>741.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1944-04-01</td>\n",
       "      <td>650.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1944-05-01</td>\n",
       "      <td>681.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1944-06-01</td>\n",
       "      <td>658.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1944-07-01</td>\n",
       "      <td>662.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1944-08-01</td>\n",
       "      <td>787.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1944-09-01</td>\n",
       "      <td>774.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1944-10-01</td>\n",
       "      <td>834.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1944-11-01</td>\n",
       "      <td>786.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1944-12-01</td>\n",
       "      <td>764.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1945-01-01</td>\n",
       "      <td>820.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1945-02-01</td>\n",
       "      <td>816.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1945-03-01</td>\n",
       "      <td>836.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1945-04-01</td>\n",
       "      <td>736.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1945-05-01</td>\n",
       "      <td>747.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1945-06-01</td>\n",
       "      <td>739.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1945-07-01</td>\n",
       "      <td>736.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1945-08-01</td>\n",
       "      <td>858.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1945-09-01</td>\n",
       "      <td>910.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1945-10-01</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1945-11-01</td>\n",
       "      <td>933.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1945-12-01</td>\n",
       "      <td>783.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1946-01-01</td>\n",
       "      <td>856.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1946-02-01</td>\n",
       "      <td>827.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1946-03-01</td>\n",
       "      <td>796.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1946-04-01</td>\n",
       "      <td>734.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1946-05-01</td>\n",
       "      <td>605.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1946-06-01</td>\n",
       "      <td>461.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1831.7</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>2229.6</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1702.2</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2286.6</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1815.3</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>2252.2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1883.5</td>\n",
       "      <td>13.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>2234.9</td>\n",
       "      <td>11.4</td>\n",
       "      <td>2002.7</td>\n",
       "      <td>13.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>2235.5</td>\n",
       "      <td>11.3</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>2270.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2055.4</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2122.9</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1896.2</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>2020.4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1768.1</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>2266.2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>2054.4</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2052.5</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1790.7</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>2131.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>1759.7</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>2011-06-01</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>2134.1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1637.1</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>2011-08-01</td>\n",
       "      <td>2386.9</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1892.1</td>\n",
       "      <td>13.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>2215.2</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1954.4</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>2215.1</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2033.2</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>2148.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2086.7</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2126.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2065.6</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2113.8</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1882.9</td>\n",
       "      <td>12.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>2159.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1987.9</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>1990.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1841.7</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1926.8</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>2252.1</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1750.4</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>2200.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1721.8</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>2367.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1997.9</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>2343.7</td>\n",
       "      <td>10.3</td>\n",
       "      <td>2210.4</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>2206.6</td>\n",
       "      <td>10.1</td>\n",
       "      <td>2078.7</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date    beef   veal    pork  lamb_and_mutton\n",
       "0   1944-01-01   751.0   85.0  1280.0             89.0\n",
       "1   1944-02-01   713.0   77.0  1169.0             72.0\n",
       "2   1944-03-01   741.0   90.0  1128.0             75.0\n",
       "3   1944-04-01   650.0   89.0   978.0             66.0\n",
       "4   1944-05-01   681.0  106.0  1029.0             78.0\n",
       "5   1944-06-01   658.0  125.0   962.0             79.0\n",
       "6   1944-07-01   662.0  142.0   796.0             82.0\n",
       "7   1944-08-01   787.0  175.0   748.0             87.0\n",
       "8   1944-09-01   774.0  182.0   678.0             91.0\n",
       "9   1944-10-01   834.0  215.0   777.0            100.0\n",
       "10  1944-11-01   786.0  197.0   944.0             91.0\n",
       "11  1944-12-01   764.0  146.0  1013.0             91.0\n",
       "12  1945-01-01   820.0  119.0  1037.0            100.0\n",
       "13  1945-02-01   816.0   97.0   724.0             81.0\n",
       "14  1945-03-01   836.0  107.0   723.0             87.0\n",
       "15  1945-04-01   736.0   98.0   651.0             78.0\n",
       "16  1945-05-01   747.0  103.0   682.0             87.0\n",
       "17  1945-06-01   739.0  110.0   674.0             87.0\n",
       "18  1945-07-01   736.0  117.0   610.0             82.0\n",
       "19  1945-08-01   858.0  145.0   531.0             77.0\n",
       "20  1945-09-01   910.0  164.0   535.0             80.0\n",
       "21  1945-10-01  1022.0  201.0   628.0             95.0\n",
       "22  1945-11-01   933.0  173.0   945.0             86.0\n",
       "23  1945-12-01   783.0  118.0  1103.0             90.0\n",
       "24  1946-01-01   856.0  101.0  1051.0             74.0\n",
       "25  1946-02-01   827.0   85.0   966.0            109.0\n",
       "26  1946-03-01   796.0   92.0   780.0             97.0\n",
       "27  1946-04-01   734.0   91.0   846.0             83.0\n",
       "28  1946-05-01   605.0   83.0   810.0             65.0\n",
       "29  1946-06-01   461.0   78.0   505.0             75.0\n",
       "..         ...     ...    ...     ...              ...\n",
       "797 2010-06-01  2320.0   10.7  1831.7             14.1\n",
       "798 2010-07-01  2229.6   10.9  1702.2             12.8\n",
       "799 2010-08-01  2286.6   11.2  1815.3             12.8\n",
       "800 2010-09-01  2252.2   11.3  1883.5             13.1\n",
       "801 2010-10-01  2234.9   11.4  2002.7             13.1\n",
       "802 2010-11-01  2235.5   11.3  2068.0             14.4\n",
       "803 2010-12-01  2270.9   11.7  2055.4             14.9\n",
       "804 2011-01-01  2122.9   10.8  1896.2             11.2\n",
       "805 2011-02-01  2020.4   10.5  1768.1             10.9\n",
       "806 2011-03-01  2266.2   12.1  2054.4             14.1\n",
       "807 2011-04-01  2052.5   10.2  1790.7             14.4\n",
       "808 2011-05-01  2131.9   10.3  1759.7             12.9\n",
       "809 2011-06-01  2375.0   11.2  1820.0             12.6\n",
       "810 2011-07-01  2134.1   10.1  1637.1             10.9\n",
       "811 2011-08-01  2386.9   11.4  1892.1             13.1\n",
       "812 2011-09-01  2215.2   10.7  1954.4             11.8\n",
       "813 2011-10-01  2215.1   10.6  2033.2             11.7\n",
       "814 2011-11-01  2148.8   10.8  2086.7             12.6\n",
       "815 2011-12-01  2126.3   10.8  2065.6             12.5\n",
       "816 2012-01-01  2113.8   10.4  1987.0             12.1\n",
       "817 2012-02-01  2009.0    9.8  1882.9             12.3\n",
       "818 2012-03-01  2159.8   10.0  1987.9             14.2\n",
       "819 2012-04-01  1990.6    9.9  1841.7             12.9\n",
       "820 2012-05-01  2232.0   10.4  1926.8             13.6\n",
       "821 2012-06-01  2252.1    8.9  1750.4             12.4\n",
       "822 2012-07-01  2200.8    9.5  1721.8             12.5\n",
       "823 2012-08-01  2367.5   10.1  1997.9             14.2\n",
       "824 2012-09-01  2016.0    8.8  1911.0             12.5\n",
       "825 2012-10-01  2343.7   10.3  2210.4             14.2\n",
       "826 2012-11-01  2206.6   10.1  2078.7             12.4\n",
       "\n",
       "[827 rows x 5 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.005151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.007348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.008368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.010531</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.007898</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.007912</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.008881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.009666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.010898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.008816</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.007298</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.008862</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.009557</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.006082</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.011468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.006639</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.010482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.011585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.002879</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.014395</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.011177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.008918</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.010666</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.011890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.012307</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.008840</td>\n",
       "      <td>0.007454</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.015427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.011675</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.010281</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.010281</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.014811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>0.014794</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.013737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.011205</td>\n",
       "      <td>0.007930</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.013274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>0.008579</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.013941</td>\n",
       "      <td>0.008579</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.013762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>0.011975</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.010066</td>\n",
       "      <td>0.007289</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.017702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.010202</td>\n",
       "      <td>0.010202</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.014453</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.010542</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.011223</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.014793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.013747</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.016919</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>0.007578</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.015686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.005592</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.016595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>0.003856</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>0.016126</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>0.009816</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.008764</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.015776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.019219</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.019219</td>\n",
       "      <td>0.016503</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>0.024650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.009203</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>0.009939</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.015461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.012259</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.011701</td>\n",
       "      <td>0.015230</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>0.017831</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.015788</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.016716</td>\n",
       "      <td>0.017645</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>0.008544</td>\n",
       "      <td>0.021360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.019658</td>\n",
       "      <td>0.013701</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>0.022836</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.017077</td>\n",
       "      <td>0.014098</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>0.013701</td>\n",
       "      <td>0.013503</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.021446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.013856</td>\n",
       "      <td>0.019596</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>0.018804</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>0.013262</td>\n",
       "      <td>0.018804</td>\n",
       "      <td>0.018211</td>\n",
       "      <td>0.015637</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.015044</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.022367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.016015</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>0.014767</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>0.021423</td>\n",
       "      <td>0.026622</td>\n",
       "      <td>0.018927</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.016015</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.027246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>0.009647</td>\n",
       "      <td>0.013469</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>0.015835</td>\n",
       "      <td>0.018566</td>\n",
       "      <td>0.013105</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>0.013651</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>0.008009</td>\n",
       "      <td>0.018930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.010432</td>\n",
       "      <td>0.016727</td>\n",
       "      <td>0.010072</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.017446</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.014029</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.014029</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.023251</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>0.019136</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>0.022016</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>0.020988</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>0.019342</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.011934</td>\n",
       "      <td>0.025309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.023486</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.023486</td>\n",
       "      <td>0.025752</td>\n",
       "      <td>0.021014</td>\n",
       "      <td>0.019984</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.021014</td>\n",
       "      <td>0.023486</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.017099</td>\n",
       "      <td>0.019778</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.025546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>0.020603</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.024214</td>\n",
       "      <td>0.027188</td>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.020603</td>\n",
       "      <td>0.017417</td>\n",
       "      <td>0.021878</td>\n",
       "      <td>0.023577</td>\n",
       "      <td>0.020816</td>\n",
       "      <td>0.015718</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.012319</td>\n",
       "      <td>0.013381</td>\n",
       "      <td>0.026338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>0.020582</td>\n",
       "      <td>0.022363</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>0.018603</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.021373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.016606</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.013323</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.015254</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.015254</td>\n",
       "      <td>0.011585</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.020853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.014747</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.015145</td>\n",
       "      <td>0.018135</td>\n",
       "      <td>0.018334</td>\n",
       "      <td>0.019530</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.018334</td>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.021323</td>\n",
       "      <td>0.016740</td>\n",
       "      <td>0.015145</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.011758</td>\n",
       "      <td>0.024312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.013205</td>\n",
       "      <td>0.018329</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>0.023059</td>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>0.019511</td>\n",
       "      <td>0.020102</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.015767</td>\n",
       "      <td>0.011037</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.021876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.018573</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.018178</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.021340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.014098</td>\n",
       "      <td>0.019189</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.016840</td>\n",
       "      <td>0.018406</td>\n",
       "      <td>0.022518</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>0.012140</td>\n",
       "      <td>0.018994</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.014490</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.021735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>0.015995</td>\n",
       "      <td>0.020565</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.021811</td>\n",
       "      <td>0.018903</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.023681</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.019734</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>0.024096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.016485</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.016485</td>\n",
       "      <td>0.022051</td>\n",
       "      <td>0.022907</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>0.027831</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.019268</td>\n",
       "      <td>0.025262</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>0.019054</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>0.020552</td>\n",
       "      <td>0.016271</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.024834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.016028</td>\n",
       "      <td>0.024908</td>\n",
       "      <td>0.017977</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.028590</td>\n",
       "      <td>0.021659</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.019493</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>0.021876</td>\n",
       "      <td>0.016028</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.027074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.016823</td>\n",
       "      <td>0.025563</td>\n",
       "      <td>0.017916</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>0.026218</td>\n",
       "      <td>0.027747</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.024689</td>\n",
       "      <td>0.024252</td>\n",
       "      <td>0.019882</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.015949</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.026873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.020838</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017537</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.023313</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.016299</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>0.017124</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.022901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>0.017012</td>\n",
       "      <td>0.023337</td>\n",
       "      <td>0.023119</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>0.027263</td>\n",
       "      <td>0.022028</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0.018757</td>\n",
       "      <td>0.023773</td>\n",
       "      <td>0.023991</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.015703</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>0.027481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.016518</td>\n",
       "      <td>0.025212</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.026951</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.021517</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.019778</td>\n",
       "      <td>0.019778</td>\n",
       "      <td>0.016735</td>\n",
       "      <td>0.020865</td>\n",
       "      <td>0.016301</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.028472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.016914</td>\n",
       "      <td>0.022969</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>0.023387</td>\n",
       "      <td>0.024431</td>\n",
       "      <td>0.020046</td>\n",
       "      <td>0.020046</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>0.019420</td>\n",
       "      <td>0.016079</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.027981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.015964</td>\n",
       "      <td>0.025117</td>\n",
       "      <td>0.017454</td>\n",
       "      <td>0.022563</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.025117</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>0.022563</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.024479</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.027884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.014891</td>\n",
       "      <td>0.020973</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.020763</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.015520</td>\n",
       "      <td>0.021812</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.013207</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.017747</td>\n",
       "      <td>0.018778</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.017540</td>\n",
       "      <td>0.014858</td>\n",
       "      <td>0.019604</td>\n",
       "      <td>0.019397</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.014239</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.014445</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.022286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.016861</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.015997</td>\n",
       "      <td>0.020968</td>\n",
       "      <td>0.020968</td>\n",
       "      <td>0.023130</td>\n",
       "      <td>0.025724</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.017294</td>\n",
       "      <td>0.023130</td>\n",
       "      <td>0.021833</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>0.017726</td>\n",
       "      <td>0.020104</td>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>0.026157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.012004</td>\n",
       "      <td>0.016457</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.014714</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.015295</td>\n",
       "      <td>0.017038</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.016263</td>\n",
       "      <td>0.015295</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>0.018393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          t0        t1        t2        t3        t4        t5        t6  \\\n",
       "0   0.000304  0.002582  0.000000  0.002279  0.003342  0.001519  0.004405   \n",
       "1   0.000302  0.004377  0.000302  0.003018  0.004377  0.002113  0.004980   \n",
       "2   0.000468  0.003434  0.001561  0.003434  0.004527  0.003122  0.004059   \n",
       "3   0.000914  0.006246  0.001523  0.005789  0.003809  0.002895  0.005180   \n",
       "4   0.001252  0.006572  0.001878  0.005789  0.004694  0.002973  0.005789   \n",
       "5   0.001116  0.007018  0.002552  0.005901  0.005582  0.003509  0.006699   \n",
       "6   0.000782  0.007661  0.002345  0.005316  0.005629  0.003127  0.006567   \n",
       "7   0.000805  0.009012  0.003058  0.007724  0.006759  0.003701  0.008529   \n",
       "8   0.001170  0.007854  0.002841  0.006684  0.007019  0.004846  0.008021   \n",
       "9   0.000962  0.009140  0.003047  0.008339  0.006895  0.004650  0.010584   \n",
       "10  0.001239  0.008983  0.002943  0.007589  0.006814  0.005266  0.010531   \n",
       "11  0.001776  0.009850  0.002907  0.008074  0.007266  0.005813  0.011465   \n",
       "12  0.002130  0.009830  0.003440  0.007536  0.006225  0.005406  0.011304   \n",
       "13  0.002440  0.010085  0.003090  0.007482  0.007807  0.006994  0.012199   \n",
       "14  0.002495  0.011311  0.003327  0.008317  0.007651  0.008317  0.012974   \n",
       "15  0.003128  0.011295  0.003301  0.008514  0.007124  0.007298  0.012685   \n",
       "16  0.001572  0.010832  0.002795  0.008735  0.007512  0.005590  0.013103   \n",
       "17  0.001580  0.008425  0.002984  0.008425  0.008250  0.006846  0.013516   \n",
       "18  0.002032  0.010669  0.002879  0.008975  0.008298  0.008129  0.014395   \n",
       "19  0.002273  0.010841  0.003147  0.009967  0.007169  0.007694  0.013639   \n",
       "20  0.002947  0.012307  0.003120  0.011787  0.009880  0.009707  0.014907   \n",
       "21  0.002788  0.011675  0.003833  0.012720  0.010281  0.010978  0.014462   \n",
       "22  0.003875  0.011624  0.003522  0.009334  0.009510  0.009158  0.014794   \n",
       "23  0.003275  0.011895  0.004310  0.010171  0.008792  0.009654  0.012756   \n",
       "24  0.003038  0.011439  0.003932  0.008937  0.008579  0.009294  0.013941   \n",
       "25  0.004339  0.014231  0.004512  0.011801  0.010413  0.011801  0.016834   \n",
       "26  0.004251  0.013433  0.004591  0.010202  0.010202  0.011733  0.014453   \n",
       "27  0.005111  0.013747  0.003349  0.010927  0.010575  0.012161  0.016919   \n",
       "28  0.005592  0.015873  0.005592  0.012085  0.011544  0.013348  0.018579   \n",
       "29  0.004032  0.012796  0.003856  0.009991  0.010868  0.012796  0.016126   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "70  0.015876  0.023814  0.014205  0.018383  0.019637  0.020472  0.024859   \n",
       "71  0.009203  0.015461  0.009939  0.011964  0.012516  0.011780  0.016197   \n",
       "72  0.012259  0.020059  0.011701  0.015230  0.015602  0.017831  0.020059   \n",
       "73  0.013900  0.019658  0.013701  0.016084  0.016680  0.018467  0.022836   \n",
       "74  0.013856  0.019596  0.012470  0.016033  0.016429  0.018804  0.021971   \n",
       "75  0.016015  0.022671  0.014767  0.020383  0.019135  0.021423  0.026622   \n",
       "76  0.011649  0.016382  0.009647  0.013469  0.014015  0.015835  0.018566   \n",
       "77  0.010432  0.016727  0.010072  0.013489  0.010971  0.015288  0.017446   \n",
       "78  0.014815  0.023251  0.016049  0.019136  0.018313  0.022016  0.024486   \n",
       "79  0.015657  0.023486  0.015863  0.019365  0.018541  0.023486  0.025752   \n",
       "80  0.016780  0.023789  0.016143  0.020603  0.018904  0.024214  0.027188   \n",
       "81  0.014843  0.019790  0.012270  0.017217  0.017217  0.020582  0.022363   \n",
       "82  0.014288  0.018536  0.011971  0.016606  0.017185  0.019116  0.021626   \n",
       "83  0.014747  0.021921  0.015145  0.018135  0.018334  0.019530  0.022917   \n",
       "84  0.014584  0.018920  0.013205  0.018329  0.018132  0.018526  0.023059   \n",
       "85  0.014819  0.018969  0.013436  0.017783  0.017981  0.018573  0.022723   \n",
       "86  0.014098  0.019189  0.012923  0.017231  0.016840  0.018406  0.022518   \n",
       "87  0.015787  0.023889  0.015995  0.020565  0.021188  0.022019  0.025758   \n",
       "88  0.016485  0.024834  0.016485  0.022051  0.022907  0.023764  0.027831   \n",
       "89  0.016028  0.024908  0.017977  0.023392  0.022959  0.023825  0.028590   \n",
       "90  0.016823  0.025563  0.017916  0.023596  0.023596  0.026218  0.027747   \n",
       "91  0.015061  0.020838  0.014236  0.018981  0.017537  0.019600  0.023313   \n",
       "92  0.016794  0.025082  0.017012  0.023337  0.023119  0.025082  0.027263   \n",
       "93  0.016518  0.025212  0.016953  0.023473  0.023038  0.025429  0.026951   \n",
       "94  0.015870  0.024640  0.016914  0.022969  0.021716  0.024013  0.026101   \n",
       "95  0.015964  0.025117  0.017454  0.022563  0.022137  0.025117  0.027884   \n",
       "96  0.014891  0.020973  0.013423  0.019086  0.019086  0.020763  0.024958   \n",
       "97  0.013207  0.021048  0.015064  0.017747  0.018778  0.017953  0.022286   \n",
       "98  0.016861  0.024859  0.015997  0.020968  0.020968  0.023130  0.025724   \n",
       "99  0.012004  0.016457  0.011036  0.014714  0.014327  0.015295  0.017038   \n",
       "\n",
       "          t7        t8        t9       t10       t11       t12       t13  \\\n",
       "0   0.002431  0.000456  0.000608  0.001823  0.002886  0.003646  0.003190   \n",
       "1   0.002415  0.001358  0.001056  0.002867  0.002867  0.003622  0.004377   \n",
       "2   0.003122  0.001405  0.001405  0.003278  0.003434  0.003590  0.003434   \n",
       "3   0.003504  0.001980  0.001980  0.004418  0.003809  0.004723  0.003961   \n",
       "4   0.004225  0.002660  0.002504  0.004538  0.004068  0.005164  0.004068   \n",
       "5   0.004466  0.003349  0.003030  0.005901  0.004147  0.004944  0.004147   \n",
       "6   0.004690  0.002502  0.003283  0.005941  0.004221  0.004690  0.003283   \n",
       "7   0.004989  0.004345  0.003540  0.006759  0.004828  0.004989  0.004667   \n",
       "8   0.004846  0.002507  0.003509  0.006016  0.005180  0.004846  0.003844   \n",
       "9   0.006414  0.004490  0.003688  0.007858  0.006735  0.005613  0.004330   \n",
       "10  0.006505  0.004336  0.003407  0.007898  0.006814  0.005111  0.004801   \n",
       "11  0.007751  0.004683  0.003391  0.007912  0.007428  0.006620  0.005006   \n",
       "12  0.006553  0.004260  0.003440  0.008847  0.007536  0.005570  0.004751   \n",
       "13  0.008133  0.005530  0.003904  0.009597  0.008295  0.006344  0.007319   \n",
       "14  0.008982  0.006487  0.003826  0.010146  0.008816  0.006487  0.006986   \n",
       "15  0.008862  0.005908  0.003997  0.009557  0.008514  0.006255  0.006082   \n",
       "16  0.008211  0.005590  0.002446  0.009609  0.007338  0.006639  0.005416   \n",
       "17  0.007899  0.005968  0.003511  0.007021  0.008952  0.006846  0.005266   \n",
       "18  0.007959  0.007282  0.003048  0.008975  0.008975  0.008467  0.005758   \n",
       "19  0.008918  0.006994  0.004022  0.010666  0.008568  0.006994  0.006120   \n",
       "20  0.010574  0.009707  0.004854  0.012480  0.010227  0.008840  0.007454   \n",
       "21  0.009932  0.010106  0.004008  0.010281  0.010106  0.007493  0.008887   \n",
       "22  0.007925  0.008101  0.003522  0.009510  0.010743  0.007925  0.007221   \n",
       "23  0.009136  0.008619  0.003792  0.011205  0.007930  0.006551  0.006033   \n",
       "24  0.008579  0.008043  0.003396  0.010724  0.009473  0.007507  0.006077   \n",
       "25  0.011107  0.010760  0.005380  0.011975  0.011628  0.010066  0.007289   \n",
       "26  0.009862  0.010542  0.004761  0.011223  0.011393  0.008672  0.006802   \n",
       "27  0.010575  0.010046  0.004054  0.010751  0.009870  0.008460  0.007578   \n",
       "28  0.011724  0.011905  0.005411  0.012266  0.012446  0.011183  0.008117   \n",
       "29  0.009641  0.009290  0.004557  0.012095  0.012095  0.009816  0.008063   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "70  0.019637  0.019219  0.015876  0.021726  0.021726  0.019219  0.016503   \n",
       "71  0.012332  0.011596  0.008835  0.013068  0.013436  0.011044  0.008283   \n",
       "72  0.015788  0.014859  0.012073  0.016716  0.017645  0.015416  0.013559   \n",
       "73  0.016283  0.017077  0.014098  0.018864  0.018864  0.016084  0.013701   \n",
       "74  0.016627  0.016033  0.013262  0.018804  0.018211  0.015637  0.013658   \n",
       "75  0.018927  0.016847  0.016015  0.022255  0.022463  0.018719  0.017679   \n",
       "76  0.013105  0.014379  0.010739  0.014197  0.014015  0.013651  0.010739   \n",
       "77  0.012950  0.013849  0.011331  0.015827  0.014029  0.012050  0.010252   \n",
       "78  0.020576  0.019753  0.016049  0.020988  0.022634  0.019342  0.016049   \n",
       "79  0.021014  0.019984  0.016069  0.021014  0.023486  0.019571  0.015451   \n",
       "80  0.021453  0.020603  0.017417  0.021878  0.023577  0.020816  0.015718   \n",
       "81  0.018801  0.017217  0.012468  0.018405  0.018603  0.015832  0.014051   \n",
       "82  0.017764  0.017571  0.013323  0.017185  0.018343  0.015254  0.012744   \n",
       "83  0.018533  0.018334  0.014946  0.020725  0.021323  0.016740  0.015145   \n",
       "84  0.018723  0.017343  0.013402  0.019511  0.020102  0.014584  0.013796   \n",
       "85  0.018178  0.017190  0.014622  0.019364  0.019561  0.014424  0.014424   \n",
       "86  0.017623  0.016252  0.012140  0.018994  0.018798  0.014686  0.014490   \n",
       "87  0.021811  0.018903  0.017657  0.023681  0.023889  0.018072  0.017034   \n",
       "88  0.022265  0.019482  0.019268  0.025262  0.025905  0.019696  0.019054   \n",
       "89  0.021659  0.022525  0.019060  0.025341  0.025341  0.020143  0.019493   \n",
       "90  0.021848  0.021848  0.018790  0.024689  0.024252  0.019882  0.018790   \n",
       "91  0.016711  0.016918  0.016299  0.019393  0.019393  0.017124  0.015680   \n",
       "92  0.022028  0.020502  0.018757  0.023773  0.023991  0.019411  0.018539   \n",
       "93  0.021300  0.021517  0.016953  0.024343  0.024343  0.019778  0.019778   \n",
       "94  0.021716  0.021716  0.018167  0.023387  0.024431  0.020046  0.020046   \n",
       "95  0.021499  0.022563  0.018519  0.024479  0.024053  0.020647  0.019583   \n",
       "96  0.018247  0.019086  0.015520  0.021812  0.020554  0.017827  0.015940   \n",
       "97  0.016508  0.017540  0.014858  0.019604  0.019397  0.017334  0.014239   \n",
       "98  0.018807  0.020752  0.017294  0.023130  0.021833  0.018807  0.018807   \n",
       "99  0.013553  0.014134  0.013359  0.016263  0.015295  0.014521  0.014134   \n",
       "\n",
       "         t14       t15       t16       t17       t18  \n",
       "0   0.001519  0.002582  0.000000  0.000000  0.004405  \n",
       "1   0.001509  0.003320  0.000000  0.000000  0.005433  \n",
       "2   0.001249  0.002966  0.000312  0.000156  0.005151  \n",
       "3   0.002438  0.003199  0.000152  0.000000  0.007922  \n",
       "4   0.002504  0.003912  0.000156  0.000000  0.007667  \n",
       "5   0.002552  0.004625  0.000319  0.000000  0.007815  \n",
       "6   0.002189  0.004847  0.000156  0.000156  0.007348  \n",
       "7   0.002414  0.004989  0.000644  0.000161  0.008368  \n",
       "8   0.001671  0.003844  0.000501  0.000000  0.007854  \n",
       "9   0.003368  0.005292  0.000802  0.000000  0.008659  \n",
       "10  0.003252  0.005730  0.000929  0.000000  0.008983  \n",
       "11  0.003714  0.006136  0.001130  0.000161  0.008881  \n",
       "12  0.003932  0.004751  0.000655  0.000164  0.009666  \n",
       "13  0.004066  0.006831  0.001301  0.000163  0.010898  \n",
       "14  0.004158  0.006820  0.001164  0.000000  0.011477  \n",
       "15  0.004170  0.007472  0.001390  0.000174  0.011468  \n",
       "16  0.003669  0.006988  0.001572  0.000175  0.010482  \n",
       "17  0.004213  0.007548  0.002457  0.000351  0.011585  \n",
       "18  0.004572  0.008637  0.001693  0.000339  0.011177  \n",
       "19  0.004896  0.007519  0.002098  0.000175  0.011890  \n",
       "20  0.007107  0.009880  0.002253  0.000347  0.015427  \n",
       "21  0.005924  0.009758  0.002788  0.000523  0.014811  \n",
       "22  0.006516  0.009334  0.002642  0.000528  0.013737  \n",
       "23  0.004827  0.007585  0.002413  0.000517  0.013274  \n",
       "24  0.006077  0.008043  0.002502  0.000179  0.013762  \n",
       "25  0.007810  0.010240  0.003124  0.000694  0.017702  \n",
       "26  0.007482  0.008842  0.002551  0.000510  0.014793  \n",
       "27  0.007050  0.010575  0.003172  0.001410  0.015686  \n",
       "28  0.008838  0.010462  0.003247  0.001623  0.016595  \n",
       "29  0.008764  0.009991  0.003330  0.001227  0.015776  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "70  0.015459  0.018383  0.011072  0.010445  0.024650  \n",
       "71  0.009571  0.011412  0.009019  0.004233  0.015461  \n",
       "72  0.013001  0.014673  0.009844  0.008544  0.021360  \n",
       "73  0.013503  0.014496  0.011120  0.010127  0.021446  \n",
       "74  0.014054  0.015044  0.010689  0.009699  0.022367  \n",
       "75  0.016847  0.019135  0.013311  0.010607  0.027246  \n",
       "76  0.012195  0.013287  0.008555  0.008009  0.018930  \n",
       "77  0.010791  0.014029  0.008273  0.008094  0.016906  \n",
       "78  0.017490  0.019547  0.012346  0.011934  0.025309  \n",
       "79  0.017099  0.019778  0.012155  0.012567  0.025546  \n",
       "80  0.018904  0.020178  0.012319  0.013381  0.026338  \n",
       "81  0.014843  0.016030  0.011874  0.009301  0.021373  \n",
       "82  0.013902  0.015254  0.011585  0.009268  0.020853  \n",
       "83  0.016341  0.018733  0.013352  0.011758  0.024312  \n",
       "84  0.014978  0.015767  0.011037  0.010642  0.021876  \n",
       "85  0.015214  0.015214  0.011658  0.010867  0.021340  \n",
       "86  0.014294  0.015469  0.010378  0.010574  0.021735  \n",
       "87  0.017657  0.019734  0.014749  0.012671  0.024096  \n",
       "88  0.017769  0.020552  0.016271  0.014558  0.024834  \n",
       "89  0.018627  0.021876  0.016028  0.013862  0.027074  \n",
       "90  0.017479  0.021848  0.015949  0.013546  0.026873  \n",
       "91  0.016092  0.016918  0.012379  0.012379  0.022901  \n",
       "92  0.016794  0.021592  0.015703  0.013086  0.027481  \n",
       "93  0.016735  0.020865  0.016301  0.013910  0.028472  \n",
       "94  0.017749  0.019420  0.016079  0.014408  0.027981  \n",
       "95  0.018519  0.020221  0.014900  0.014261  0.027884  \n",
       "96  0.016569  0.018876  0.011955  0.012584  0.023700  \n",
       "97  0.015270  0.016508  0.014445  0.011556  0.022286  \n",
       "98  0.017726  0.020104  0.015348  0.013186  0.026157  \n",
       "99  0.011810  0.015682  0.010842  0.008519  0.018393  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoU_classes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoU_classes_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1944-01-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1944-02-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>713.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1944-04-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1944-05-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1944-06-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1944-07-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1944-08-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>787.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1944-09-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1944-10-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>834.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1944-11-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1944-12-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1945-01-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1945-02-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1945-03-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1945-04-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1945-05-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1945-06-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1945-07-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1945-08-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>858.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1945-09-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1945-10-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>1022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1945-11-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1945-12-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1946-01-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1946-02-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>827.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1946-03-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>796.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1946-04-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>734.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1946-05-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>605.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1946-06-01</td>\n",
       "      <td>beef</td>\n",
       "      <td>461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5759</th>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>489.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>466.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>481.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>467.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>524.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>520.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>461.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>435.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>503.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5769</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>455.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>496.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>2011-06-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>519.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772</th>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>448.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>2011-08-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>501.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5774</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>473.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5775</th>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>524.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>511.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5777</th>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>458.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5778</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>476.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5779</th>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>466.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>502.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5782</th>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>519.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>506.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5784</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>497.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>530.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>453.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>579.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>turkey</td>\n",
       "      <td>515.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5789 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date variable   value\n",
       "0    1944-01-01     beef   751.0\n",
       "1    1944-02-01     beef   713.0\n",
       "2    1944-03-01     beef   741.0\n",
       "3    1944-04-01     beef   650.0\n",
       "4    1944-05-01     beef   681.0\n",
       "5    1944-06-01     beef   658.0\n",
       "6    1944-07-01     beef   662.0\n",
       "7    1944-08-01     beef   787.0\n",
       "8    1944-09-01     beef   774.0\n",
       "9    1944-10-01     beef   834.0\n",
       "10   1944-11-01     beef   786.0\n",
       "11   1944-12-01     beef   764.0\n",
       "12   1945-01-01     beef   820.0\n",
       "13   1945-02-01     beef   816.0\n",
       "14   1945-03-01     beef   836.0\n",
       "15   1945-04-01     beef   736.0\n",
       "16   1945-05-01     beef   747.0\n",
       "17   1945-06-01     beef   739.0\n",
       "18   1945-07-01     beef   736.0\n",
       "19   1945-08-01     beef   858.0\n",
       "20   1945-09-01     beef   910.0\n",
       "21   1945-10-01     beef  1022.0\n",
       "22   1945-11-01     beef   933.0\n",
       "23   1945-12-01     beef   783.0\n",
       "24   1946-01-01     beef   856.0\n",
       "25   1946-02-01     beef   827.0\n",
       "26   1946-03-01     beef   796.0\n",
       "27   1946-04-01     beef   734.0\n",
       "28   1946-05-01     beef   605.0\n",
       "29   1946-06-01     beef   461.0\n",
       "...         ...      ...     ...\n",
       "5759 2010-06-01   turkey   489.5\n",
       "5760 2010-07-01   turkey   466.9\n",
       "5761 2010-08-01   turkey   481.1\n",
       "5762 2010-09-01   turkey   467.1\n",
       "5763 2010-10-01   turkey   524.3\n",
       "5764 2010-11-01   turkey   520.5\n",
       "5765 2010-12-01   turkey   461.4\n",
       "5766 2011-01-01   turkey   463.0\n",
       "5767 2011-02-01   turkey   435.6\n",
       "5768 2011-03-01   turkey   503.1\n",
       "5769 2011-04-01   turkey   455.6\n",
       "5770 2011-05-01   turkey   496.4\n",
       "5771 2011-06-01   turkey   519.3\n",
       "5772 2011-07-01   turkey   448.1\n",
       "5773 2011-08-01   turkey   501.3\n",
       "5774 2011-09-01   turkey   473.2\n",
       "5775 2011-10-01   turkey   524.5\n",
       "5776 2011-11-01   turkey   511.4\n",
       "5777 2011-12-01   turkey   458.9\n",
       "5778 2012-01-01   turkey   476.7\n",
       "5779 2012-02-01   turkey   466.8\n",
       "5780 2012-03-01   turkey   502.1\n",
       "5781 2012-04-01   turkey   478.0\n",
       "5782 2012-05-01   turkey   519.9\n",
       "5783 2012-06-01   turkey   506.6\n",
       "5784 2012-07-01   turkey   497.2\n",
       "5785 2012-08-01   turkey   530.1\n",
       "5786 2012-09-01   turkey   453.1\n",
       "5787 2012-10-01   turkey   579.9\n",
       "5788 2012-11-01   turkey   515.3\n",
       "\n",
       "[5789 rows x 3 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ggplot import meat\n",
    "meat_lng = pd.melt(meat, id_vars=['date'])\n",
    "meat_lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "y needed for <ggplot.geoms.geom_point.geom_point object at 0x7f017153ee10>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    379\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/ggplot.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# this is nice for dev but not the best for \"real\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GGPLOT_DEV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/ggplot.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacetgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/geoms/geom_point.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, data, _aes)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_aes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_aes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_aes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_aes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_aes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/geoms/geom.py\u001b[0m in \u001b[0;36m_get_plot_args\u001b[0;34m(self, data, _aes)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREQUIRED_AES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmpl_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s needed for %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mmpl_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: y needed for <ggplot.geoms.geom_point.geom_point object at 0x7f017153ee10>"
     ]
    }
   ],
   "source": [
    "ggplot(IoU_classes_train, aes()) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAHvCAYAAABT1snrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr76kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f017463e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (-9223363309090736829)>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meat = meat.dropna(thresh=800, axis=1) # drop columns that have fewer than 800 observations\n",
    "ts = meat.set_index(['date'])\n",
    "\n",
    "ggplot(meat, aes('pork', weight='beef')) + geom_bar() + scale_y_continuous(labels='comma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-90848168d7ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIoU_classes_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2970\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "IoU_classes_train.name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Graphical representation of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'subSamplingSequence' object has no attribute 'Seque'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e1c0a669babc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubSamplingSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnInputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnOutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-e1c0a669babc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nInputs, nOutputs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnInputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnOutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubSamplingSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnInputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanguy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 238\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'subSamplingSequence' object has no attribute 'Seque'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable, Function\n",
    "from collections import defaultdict\n",
    "import graphviz\n",
    "\n",
    "\"\"\"\n",
    "This is a rather distorted implementation of graph visualization in PyTorch.\n",
    "This implementation is distorted because PyTorch's autograd is undergoing refactoring right now.\n",
    "- neither func.next_functions nor func.previous_functions can be relied upon\n",
    "- BatchNorm's C backend does not follow the python Function interface\n",
    "- I'm not even sure whether to use var.creator or var.grad_fn (apparently the source tree and wheel builds use different\n",
    "  interface now)\n",
    "As a result, we are forced to manually trace the graph, using 2 redundant mechanisms:\n",
    "- Function.__call__: this allows us to trace all Function creations. Function corresponds to Op in TF\n",
    "- Module.forward_hook: this is needed because the above method doesn't work for BatchNorm, as the current C backend does\n",
    "  not follow the Python Function interface. \n",
    "To do graph visualization, follow these steps:\n",
    "1. register hooks on model: register_vis_hooks(model)\n",
    "2. pass data through model: output = model(input)\n",
    "3. remove hooks           : remove_vis_hooks()\n",
    "4. perform visualization  : save_visualization(name, format='svg') # name is a string without extension\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "old_function__call__ = Function.__call__\n",
    "\n",
    "def register_creator(inputs, creator, output):\n",
    "    \"\"\"\n",
    "    In the forward pass, our Function.__call__ and BatchNorm.forward_hook both call this method to register the creators\n",
    "    inputs: list of input variables\n",
    "    creator: one of\n",
    "        - Function\n",
    "        - BatchNorm module\n",
    "    output: a single output variable\n",
    "    \"\"\"\n",
    "    cid = id(creator)\n",
    "    oid = id(output)\n",
    "    if oid in vars: \n",
    "        return\n",
    "    # connect creator to input\n",
    "    for input in inputs:\n",
    "        iid = id(input)\n",
    "        func_trace[cid][iid] = input\n",
    "        # register input\n",
    "        vars[iid] = input\n",
    "    # connect output to creator\n",
    "    assert type(output) not in [tuple, list, dict]\n",
    "    var_trace[oid][cid] = creator\n",
    "    # register creator and output and all inputs\n",
    "    vars[oid] = output\n",
    "    funcs[cid] = creator\n",
    "\n",
    "hooks = []\n",
    "\n",
    "def register_vis_hooks(model):\n",
    "    global var_trace, func_trace, vars, funcs\n",
    "    remove_vis_hooks()\n",
    "    var_trace  = defaultdict(lambda: {})     # map oid to {cid:creator}\n",
    "    func_trace = defaultdict(lambda: {})     # map cid to {iid:input}\n",
    "    vars  = {}                               # map vid to Variable/Parameter\n",
    "    funcs = {}                               # map cid to Function/BatchNorm module\n",
    "    hooks = []                               # contains the forward hooks, needed for hook removal\n",
    "\n",
    "    def hook_func(module, inputs, output):\n",
    "        assert 'BatchNorm' in mod.__class__.__name__        # batchnorms don't have shared superclass\n",
    "        inputs = list(inputs)\n",
    "        for p in [module.weight, module.bias]:\n",
    "            if p is not None:\n",
    "                inputs.append(p)\n",
    "        register_creator(inputs, module, output)\n",
    "\n",
    "    for mod in model.modules():\n",
    "        if 'BatchNorm' in mod.__class__.__name__:           # batchnorms don't have shared superclass\n",
    "            hook = mod.register_forward_hook(hook_func)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    def new_function__call__(self, *args, **kwargs):\n",
    "        inputs =  [a for a in args            if isinstance(a, Variable)]\n",
    "        inputs += [a for a in kwargs.values() if isinstance(a, Variable)]\n",
    "        output = old_function__call__(self, *args, **kwargs)\n",
    "        register_creator(inputs, self, output)\n",
    "        return output\n",
    "\n",
    "    Function.__call__ = new_function__call__\n",
    "\n",
    "\n",
    "def remove_vis_hooks():\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    Function.__call__ = old_function__call__\n",
    "\n",
    "\n",
    "def save_visualization(name, format='svg'):\n",
    "    g = graphviz.Digraph(format=format)\n",
    "    def sizestr(var):\n",
    "        size = [int(i) for i in list(var.size())]\n",
    "        return str(size)\n",
    "    # add variable nodes\n",
    "    for vid, var in vars.iteritems():\n",
    "        if isinstance(var, nn.Parameter):\n",
    "            g.node(str(vid), label=sizestr(var), shape='ellipse', style='filled', fillcolor='red')\n",
    "        elif isinstance(var, Variable):\n",
    "            g.node(str(vid), label=sizestr(var), shape='ellipse', style='filled', fillcolor='lightblue')\n",
    "        else:\n",
    "            assert False, var.__class__\n",
    "    # add creator nodes\n",
    "    for cid in func_trace:\n",
    "        creator = funcs[cid]\n",
    "        g.node(str(cid), label=str(creator.__class__.__name__), shape='rectangle', style='filled', fillcolor='orange')\n",
    "    # add edges between creator and inputs\n",
    "    for cid in func_trace:\n",
    "        for iid in func_trace[cid]:\n",
    "            g.edge(str(iid), str(cid))\n",
    "    # add edges between outputs and creators\n",
    "    for oid in var_trace:\n",
    "        for cid in var_trace[oid]:\n",
    "            g.edge(str(cid), str(oid))\n",
    "    g.render(name)\n",
    "class subSamplingSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    This class represente a bloc that reduce the resolution of each feature map(factor2)\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(subSamplingSequence, self).__init__()\n",
    "        self.Seque\n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(2,2), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "network = subSamplingSequence(nInputs = 3,nOutputs = 6)\n",
    "print(network)\n",
    "a = torch.randn(2, 3, 7, 7)\n",
    "inputs = Variable(a)\n",
    "out = network(inputs)\n",
    "print(out.size())\n",
    "\n",
    "def visualize(a,network):\n",
    "    global recon\n",
    "    inputs = Variable(a)\n",
    "    register_vis_hooks(network)\n",
    "    recon = network(inputs)\n",
    "    remove_vis_hooks()\n",
    "    save_visualization('pytorch_model', 'png')\n",
    "    \n",
    "visualize(a,network)\n",
    "#resnet18 = models.resnet18()\n",
    "#y = resnet18(inputs)\n",
    "# print(y)\n",
    "\n",
    "g = make_dot(out)\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD8CAYAAADAKumpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f018eb6ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_arr = [[33,2,0,0,0,0,0,0,0,1,3], \n",
    "            [3,31,0,0,0,0,0,0,0,0,0], \n",
    "            [0,4,41,0,0,0,0,0,0,0,1], \n",
    "            [0,1,0,30,0,6,0,0,0,0,1], \n",
    "            [0,0,0,0,38,10,0,0,0,0,0], \n",
    "            [0,0,0,3,1,39,0,0,0,0,4], \n",
    "            [0,2,2,0,4,1,31,0,0,0,2],\n",
    "            [0,1,0,0,0,0,0,36,0,2,0], \n",
    "            [0,0,0,0,0,0,1,5,37,5,1], \n",
    "            [3,0,0,0,0,0,0,0,0,39,0], \n",
    "            [0,0,0,0,0,0,0,0,0,0,38]]\n",
    "\n",
    "norm_conf = []\n",
    "for i in conf_arr:\n",
    "    a = 0\n",
    "    tmp_arr = []\n",
    "    a = sum(i, 0)\n",
    "    for j in i:\n",
    "        tmp_arr.append(float(j)/float(a))\n",
    "    norm_conf.append(tmp_arr)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect(1)\n",
    "res = ax.imshow(np.array(conf_arr), cmap=plt.cm.jet, \n",
    "                interpolation='nearest')\n",
    "#width, height = parameters.number_classes,parameters.number_classes\n",
    "\n",
    "#for x in range(width):\n",
    "#    for y in range(height):\n",
    "#        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
    "#                    horizontalalignment='center',\n",
    "#                    verticalalignment='center')\n",
    "\n",
    "cb = fig.colorbar(res)\n",
    "#alphabet = '0123'\n",
    "#plt.xticks(range(width), alphabet[:width])\n",
    "#plt.yticks(range(height), alphabet[:height])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import re\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "def make_dot(var):\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if isinstance(var, Variable):\n",
    "                value = '('+(', ').join(['%d'% v for v in var.size()])+')'\n",
    "                dot.node(str(id(var)), str(value), fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'previous_functions'):\n",
    "                for u in var.previous_functions:\n",
    "                    dot.edge(str(id(u[0])), str(id(var)))\n",
    "                    add_nodes(u[0])\n",
    "    add_nodes(var.creator)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test function : IoU\n",
    "    return a matrix of confusion\n",
    "\"\"\"\n",
    "def IoU(y_train_estimated, y_train):\n",
    "    \n",
    "    #We keep only the higest value, which is the prediction\n",
    "    pred = torch.max(y_train_estimated, dim=1)[1]\n",
    "\n",
    "    confusion_matrix = [[0] * parameters.number_classes for i in range(parameters.number_classes)]\n",
    "    \n",
    "    #For each classes : [0] = TP [1] = FP [2] = FN\n",
    "    IoU_each_classes = np.zeros(shape = (3,parameters.number_classes))\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    target = y_train.view(-1)\n",
    "\n",
    "    # Double loop over the number of classes at each iteration we add the intersection\n",
    "    for cls1 in range(parameters.number_classes):\n",
    "        pred_inds = pred == cls1\n",
    "        for cls2 in range(parameters.number_classes):\n",
    "            target_inds = target == cls2\n",
    "            intersection = (pred_inds*target_inds).long().sum().data.cpu()[0]\n",
    "            confusion_matrix[cls1][cls2] = intersection\n",
    "\n",
    "            # TP here for each classes\n",
    "            if(cls1 == cls2):\n",
    "                IoU_each_classes[0,cls1] = intersection\n",
    "            # FN and FP here for each classes\n",
    "            else:\n",
    "                IoU_each_classes[1,cls1] = IoU_each_classes[1,cls1] + intersection\n",
    "                IoU_each_classes[2,cls2] = IoU_each_classes[2,cls1] + intersection\n",
    "    \n",
    "    # compute the IoU value\n",
    "    IoU_each_classes_total = IoU_each_classes[0,:]/np.sum(IoU_each_classes)\n",
    "\n",
    "    return(confusion_matrix,IoU_each_classes_total, np.mean((np.float32(IoU_each_classes_total))))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
