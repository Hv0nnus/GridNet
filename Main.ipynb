{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Torch related package\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torchviz\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6021\n"
     ]
    }
   ],
   "source": [
    "#cuda related package\n",
    "import torch.cuda\n",
    "import torch.backends.cudnn as cudnn\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# Other package\n",
    "import time\n",
    "import random\n",
    "from ggplot import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import seaborn as sns; sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run GridNet_structure.ipynb #Good trick to launch another notebook\n",
    "%run Plot.ipynb \n",
    "%run Save_import.ipynb\n",
    "%run Loss_Error.ipynb\n",
    "%run Label.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire pour la suite (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut enlever le biais dans les convolutions avant la normalisation\n",
    "\n",
    "Le diminution de la taille des images, pas clair cette division par deux....\n",
    "\n",
    "Change la fonction de cout, lien entre fonction de cout et de perte ?\n",
    "\n",
    "\n",
    "Il faut enlever les biais quand il y a une batch normalisation apres\n",
    "\n",
    "Vérifier ce qu'est la mesure de test IoU\n",
    "\n",
    "Mettre des commentaire\n",
    "\n",
    "Ne pas sauvgarder n'importe quand le réseau. Peut ere garder un truc qui a en mémoire le meilleur IuO sur \n",
    "la validation\n",
    "\n",
    "Quel modification sont a faire sur les données ? genre pour les augmenter\n",
    "\n",
    "Parallelisation, comment ca marche ?\n",
    "\n",
    "COMMENT VERIFIER QUE TOUT FONCTIONNE ??? Regarder le graph peut être.\n",
    "Regarder le temps de calcul quand \n",
    "\n",
    "Idée: reregarder comment on fait de la segmentation avec les champs de Markov, ca peut donner des idées\n",
    "\n",
    "Afficher le nom de la courbe avec la courbe et pas juste a coter\n",
    "\n",
    "\n",
    "Indice qui indique si une fonction est dérivable informatiquement, et prendre uen fonction qui se rapproche de l'IoU\n",
    "\n",
    "Corrélation entre IoU et le logsoftmax\n",
    "\n",
    "distance entre deux fonctions ?\n",
    "\n",
    "Faire les tests sur la base de tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self,\n",
    "                 # Number of columns of the grid\n",
    "                 nColumns = 2,\n",
    "                 # Number of features map at each rows\n",
    "                 nFeatMaps = [3,6],\n",
    "                 # Number of feature map of the input image\n",
    "                 nFeatureMaps_init = 3,\n",
    "                 #Number of classes (19(usefull classes) + 1(all other classes together))\n",
    "                 number_classes = 20,\n",
    "                 # DataFrame with the name of each label associated with their number\n",
    "                 label_DF = None,\n",
    "\n",
    "                 # Size of initial image\n",
    "                 width_image_initial = 2048, height_image_initial = 1024,\n",
    "                 # Size after the crop\n",
    "                 width_image_crop = 19, height_image_crop = 19,\n",
    "\n",
    "                 # Probability of a Blockwise dropout\n",
    "                 dropFactor = 0.1,\n",
    "                 learning_rate=0.01,\n",
    "                 weight_decay = 5*10**(-6),\n",
    "                 #Parameter of the Adam Optimizer (beta1 beta2 and epsilon)\n",
    "                 beta1 = 0.9,\n",
    "                 beta2 = 0.999,\n",
    "                 epsilon = 1*10**(-8),\n",
    "                 # Size of the mini batch\n",
    "                 batch_size = 2,\n",
    "                 # Size of the mini batch to compute error (if the entire validation set cannot be in loaded)\n",
    "                 batch_size_val = 10,\n",
    "                 # Maximum value of epoch iteration\n",
    "                 epoch_total = 10,\n",
    "                 # The actual epoch is not null if we train the network which has already been train\n",
    "                 actual_epoch = 0,\n",
    "\n",
    "                 # File where all the parameter model can be store\n",
    "                 path_save_net = \"Model/\",\n",
    "                 #Name of the network, used for store (name_network and train_number)\n",
    "                 name_network = \"test\",\n",
    "                 train_number = 0,\n",
    "                 # File where the error will be stored\n",
    "                 path_CSV = \"CSV/\",\n",
    "                 # Path of the Data\n",
    "                 path_data = \"Cityscapes_Copy/\",\n",
    "                 # Number of process that will load the Data\n",
    "                 num_workers = 0):\n",
    "        \n",
    "        super(Parameters, self).__init__()\n",
    "        # Image\n",
    "        self.number_classes = number_classes\n",
    "        self.label_DF = label_DF\n",
    "        self.width_image_initial = width_image_initial\n",
    "        self.height_image_initial = height_image_initial\n",
    "        self.width_image_crop = width_image_crop\n",
    "        self.height_image_crop = height_image_crop\n",
    "        # Number of feature map at the begining, if RGB image it would be 3\n",
    "        self.nFeatureMaps_init = nFeatureMaps_init\n",
    "        self.path_data = path_data\n",
    "        \n",
    "        # GridNet\n",
    "        self.nColumns = nColumns\n",
    "        self.nFeatMaps = nFeatMaps\n",
    "        self.name_network = name_network\n",
    "        self.train_number = train_number\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        #Save\n",
    "        self.path_CSV = path_CSV\n",
    "        self.path_save_net = path_save_net\n",
    "        \n",
    "        \n",
    "        # Learning\n",
    "        self.dropFactor = dropFactor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_val = batch_size_val\n",
    "        self.epoch_total = epoch_total\n",
    "        self.actual_epoch = actual_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"train return nothing but modify the weight of the network and call save_error to store the error.\n",
    "    (0) = parameters : list of parameters of the network\n",
    "    (1) = network : network that will be learned\n",
    "    (2) = train_loader : Dataloader which contains input and target of the train dataset\n",
    "    (3) = val_loader : Dataloader which contains input and target of the validation dataset\n",
    "\"\"\"\n",
    "def train(parameters,network,train_loader,val_loader):\n",
    "    #Store the time at the begining of the training\n",
    "    timer_init = time.time()\n",
    "    \n",
    "    # create your optimizer\n",
    "    optimizer = optim.Adam(params = network.parameters(), lr = parameters.learning_rate,\n",
    "                           betas = (parameters.beta1, parameters.beta2),\n",
    "                           eps = parameters.epsilon, weight_decay = parameters.weight_decay)\n",
    "    \n",
    "    #High value just to initialize this variable\n",
    "    # validation error min will store the lowest validation result\n",
    "    validation_error_min = 9999\n",
    "    \n",
    "    # Store the indice of the next checkpoint. This value is 0 or 1. We always keep one checkpoint untouched\n",
    "    # while the other one is changed.\n",
    "    index_save_regular = 0\n",
    "    index_save_best = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(parameters.actual_epoch,parameters.epoch_total):\n",
    "        #Store the time at the begining of each epoch\n",
    "        timer_epoch = time.time()\n",
    "        \n",
    "        for i,(x_batch, y_batch) in enumerate(train_loader):\n",
    "\n",
    "            # zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Transform into Variable\n",
    "            x_batch, y_batch = Variable(x_batch), Variable(y_batch)\n",
    "            \n",
    "            # Compute the forward function\n",
    "            y_batch_estimated = network(x_batch)\n",
    "            \n",
    "            #Get the error\n",
    "            loss = criterion(y_batch_estimated, y_batch)\n",
    "            \n",
    "            #Compute the backward function\n",
    "            loss.backward()\n",
    "            \n",
    "            # Does the update according to the optimizer define above\n",
    "            optimizer.step()\n",
    "            #Save error of the training Dataset\n",
    "            save_error(x = x_batch,y = y_batch, network = network,epoch = epoch,name_network = parameters.name_network,\n",
    "                       train_number = parameters.train_number,path_CSV = parameters.path_CSV,set_type = \"train\")\n",
    "\n",
    "        # Validation_error contains the error on the validation set\n",
    "        validation_error = 0\n",
    "        \n",
    "        # Save the error of the validation Dataset\n",
    "        for i,(x_val_batch, y_val_batch) in enumerate(val_loader):\n",
    "            x_val_batch, y_val_batch = Variable(x_val_batch), Variable(y_val_batch)\n",
    "            validation_error += save_error(x = x_val_batch,y = y_val_batch,network = network,epoch = epoch,\n",
    "                                          name_network = parameters.name_network, train_number = parameters.train_number,\n",
    "                                          path_CSV = parameters.path_CSV, set_type = \"validation\")\n",
    "        \n",
    "        # Divise by the the number of element in the entire batch\n",
    "        validation_error = validation_error/((i+1)*parameters.batch_size_val)\n",
    "        \n",
    "        # checkpoint will save the network if needed\n",
    "        validation_error_min,index_save_best,index_save_regular = checkpoint(validation_error,validation_error_min,\n",
    "                                                                               index_save_best,index_save_regular,\n",
    "                                                                               epoch,network,parameters)\n",
    "                    \n",
    "        \n",
    "        print(\"Epoch :\",epoch,\"/\",parameters.epoch_total - 1,\". Last loss : \",loss.data[0],\n",
    "              \". Time Epoch :\", time.time() - timer_epoch,\". Time total\", time.time() - timer_init)\n",
    "\n",
    "    print(\"Finish. Total mean time : \", (time.time() - timer_init)/(parameters.epoch_total- parameters.actual_epoch))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_name = {'Real_name' : [\"road\",\"sidewalk\",\"building\",\"wall\",\"fence\",\"pole\",\"traffic light\",\"traffic sign\",\n",
    "                             \"vegetation\",\"terrain\",\"sky\",\"person\",\"rider\",\"car\",\"truck\",\"bus\",\"train\",\"motorcycle\",\n",
    "                             \"bicycle\",\"autre\"],\n",
    "              'Class_name' : [\"class\"+str(i) for i in range(19)] + [\"class19\"]}\n",
    "\n",
    "label_DF = pd.DataFrame(data=label_name)\n",
    " \n",
    "%run GridNet_structure.ipynb #Good trick to launch another notebook\n",
    "%run Plot.ipynb \n",
    "%run Save_import.ipynb\n",
    "%run Loss_Error.ipynb\n",
    "%run Label.ipynb\n",
    "parameters = Parameters(nColumns = 2,\n",
    "                            nFeatMaps = [3,6],\n",
    "                            nFeatureMaps_init = 3,\n",
    "                            number_classes = 20-1,\n",
    "                            label_DF = label_DF,\n",
    "\n",
    "                            width_image_initial = 2048, height_image_initial = 1024,\n",
    "                            width_image_crop = 3, height_image_crop = 19,\n",
    "\n",
    "                            dropFactor = 0.1,\n",
    "                            learning_rate=0.01,\n",
    "                            weight_decay = 5*10**(-6),\n",
    "                            beta1 = 0.9,\n",
    "                            beta2 = 0.999,\n",
    "                            epsilon = 1*10**(-8),\n",
    "                            batch_size = 1,\n",
    "                            batch_size_val = 10,\n",
    "                            epoch_total = 8,\n",
    "                            actual_epoch = 0,\n",
    "\n",
    "                            path_save_net = \"Model/\",\n",
    "                            name_network = \"test\",\n",
    "                            train_number = 0,\n",
    "                            path_CSV = \"CSV/\",\n",
    "                            path_data = \"Cityscapes_Copy/\",\n",
    "                            num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO ici il faudra faire arreter le programme ! On ecrase les fichiers !\n",
      "The network as been saved at the epoch 0(best score)0\n",
      "Epoch : 0 / 0 . Last loss :  2.907681703567505 . Time Epoch : 2.870622396469116 . Time total 2.870816707611084\n",
      "Finish. Total mean time :  2.871652841567993\n"
     ]
    }
   ],
   "source": [
    "def main_new_learning():\n",
    "    \n",
    "    #Transformation that will be apply on the data just after the import\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(parameters.width_image_crop),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(parameters.width_image_crop),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    #Import both dataset with the transformation\n",
    "    train_dataset = CityScapes_final('fine', 'train',transform = transform, target_transform = target_transform)\n",
    "    val_dataset = CityScapes_final('fine', 'val',transform = transform, target_transform = transform)\n",
    "\n",
    "    # Creat the DataSet for pytorch used\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=parameters.batch_size, shuffle=True,\n",
    "                                               num_workers=parameters.num_workers,drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=parameters.batch_size_val, shuffle = True,\n",
    "                                             num_workers=parameters.num_workers,drop_last=True)\n",
    "    \n",
    "    # Define the GridNet\n",
    "    network = gridNet(nInputs = parameters.nFeatureMaps_init,nOutputs = parameters.number_classes,\n",
    "                      nColumns = parameters.nColumns,nFeatMaps = parameters.nFeatMaps, dropFactor = parameters.dropFactor)\n",
    "\n",
    "    #Init the csv file that will store the error\n",
    "    init_csv(name_network = parameters.name_network, train_number = parameters.train_number,\n",
    "             path_CSV = parameters.path_CSV)\n",
    "\n",
    "    #Train the network\n",
    "    train(network = network, parameters = parameters, train_loader = train_loader, val_loader = val_loader)\n",
    "\n",
    "    \n",
    "main_new_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main_continue_learning():\n",
    "    parameters = Parameters(nColumns = 2,\n",
    "                            nFeatMaps = [3,6],\n",
    "                            nFeatureMaps_init = 3,\n",
    "                            number_classes = 20-1,\n",
    "                            label_DF = label_DF,\n",
    "\n",
    "                            width_image_initial = 2048, height_image_initial = 1024,\n",
    "                            width_image_crop = 3, height_image_crop = 19,\n",
    "\n",
    "                            dropFactor = 0.1,\n",
    "                            learning_rate=0.01,\n",
    "                            weight_decay = 5*10**(-6),\n",
    "                            beta1 = 0.9,\n",
    "                            beta2 = 0.999,\n",
    "                            epsilon = 1*10**(-8),\n",
    "                            batch_size = 1,\n",
    "                            batch_size_val = 10,\n",
    "                            epoch_total = 3,\n",
    "                            actual_epoch = 0,\n",
    "\n",
    "                            path_save_net = \"Model/\",\n",
    "                            name_network = \"test\",\n",
    "                            train_number = 0,\n",
    "                            path_CSV = \"CSV/\",\n",
    "                            path_data = \"Cityscapes_Copy/\",\n",
    "                            num_workers = 0)\n",
    "\n",
    "    #Transformation that will be apply on the data just after the import\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(parameters.width_image_crop),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(parameters.width_image_crop),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    #Import both dataset with the transformation\n",
    "    train_dataset = CityScapes_final('fine', 'train',transform = transform, target_transform = target_transform)\n",
    "    val_dataset = CityScapes_final('fine', 'val',transform = transform, target_transform = target_transform)\n",
    "\n",
    "    # Creat the DataSet for pytorch used\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=parameters.batch_size, shuffle=True,\n",
    "                                               num_workers=parameters.num_workers,drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=parameters.batch_size_val, shuffle = True,\n",
    "                                             num_workers=parameters.num_workers,drop_last=True)\n",
    "    \n",
    "    # Define the GridNet\n",
    "    network = gridNet(nInputs = parameters.nFeatureMaps_init,nOutputs = parameters.number_classes,\n",
    "                      nColumns = parameters.nColumns,nFeatMaps = parameters.nFeatMaps, dropFactor = parameters.dropFactor)\n",
    "    \n",
    "    # Load the trained Network\n",
    "    parameters = load_from_checkpoint(path_checkpoint = parameters.path_save_net + \"best1test0checkpoint.pth.tar\",\n",
    "                                      network = network)\n",
    "    \n",
    "    # Here we can change some parameters\n",
    "    parameters.epoch_total = 15\n",
    "    \n",
    "    # Train the network\n",
    "    train(network = network, parameters = parameters, train_loader = train_loader, val_loader = val_loader)\n",
    "\n",
    "    \n",
    "#main_continue_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def all_plot():\n",
    "    #Because we work with mini batch, there is an error for each mini_batch. Organise CSV take the mean of those error.\n",
    "    organise_CSV(name_network = parameters.name_network,train_number = parameters.train_number)\n",
    "\n",
    "    plot_loss(name_network = parameters.name_network,train_number = parameters.train_number)\n",
    "\n",
    "    plot_IuO(name_network = parameters.name_network,train_number = parameters.train_number)\n",
    "\n",
    "    plot_mat_confusion(name_network = parameters.name_network, train_number = parameters.train_number,\n",
    "                       epoch = parameters.epoch_total - 1, data_set = \"train\")\n",
    "\n",
    "    plot_mat_confusion(name_network = parameters.name_network, train_number = parameters.train_number,\n",
    "                       epoch = parameters.epoch_total - 1,data_set = \"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
