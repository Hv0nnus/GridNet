{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import ggplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import torch\n",
    "import matplotlib.pylab as pylab\n",
    "from ipywidgets import interact\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Parameters\n",
    "import Label\n",
    "import Save_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"organise_CSV import two CSV files and delete all duplicate row. Because the algorithme work with\n",
    "    mini_batch there is many value for the loss for one epoch and one data set. We compute here the mean\n",
    "    of all this loss that have the same epoch and data set. We did the same with the confusion matrix\n",
    "    (0) = name_network : name of the network associated with the CSV file\n",
    "    (1) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def organise_CSV(path_CSV,name_network,train_number, both=True):\n",
    "    # Import the CSV file into pandas DataFrame\n",
    "    loss_DF = pd.read_csv(path_CSV + \"CSV_loss_\" + name_network + str(train_number) + \".csv\")\n",
    "    # This Groupby will regroupe all line that have the same \"Set\" and \"Epoch\" and compute the mean over the \"Values\"\n",
    "    loss_DF = loss_DF.groupby(['Set','Epoch'])['Value'].mean().reset_index()\n",
    "    # Recreate the CSV file\n",
    "    loss_DF.to_csv(path_CSV + \"CSV_loss_\" + name_network + str(train_number) + \".csv\",index = False)\n",
    "    \n",
    "    if both:\n",
    "        # Import the CSV file into pandas DataFrame\n",
    "        conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\")\n",
    "        # This Groupby will regroupe all line that have the same 'Target','Prediction','Epoch','Set'\n",
    "        # and compute the mean over the \"Values\"\n",
    "        conf_DF = conf_DF.groupby(['Target','Prediction','Epoch','Set'])['Value'].mean().reset_index()\n",
    "        # Recreate the CSV file\n",
    "        conf_DF.to_csv(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_loss will plot the loss against the epoch\n",
    "    (0) = name_network : name of the network associated with the CSV file\n",
    "    (1) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def plot_loss(path_CSV,name_network,train_number):\n",
    "    # Import the CSV file into pandas DataFrame\n",
    "    loss_DF = pd.read_csv(path_CSV + \"CSV_loss_\" + name_network + str(train_number) + \".csv\")\n",
    "    \n",
    "    # Reset sns parameter to default\n",
    "    sns.set()\n",
    "    # Set some parameter to have better plot\n",
    "    params = {'legend.fontsize': 'x-large',\n",
    "              'figure.figsize': (16, 10),\n",
    "             'axes.labelsize': 'x-large',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'xtick.labelsize':'x-large',\n",
    "             'ytick.labelsize':'x-large'}\n",
    "    pylab.rcParams.update(params)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    #fig.set_size_inches(15, 10)\n",
    "    sns.tsplot(time=\"Epoch\", value=\"Value\", data=loss_DF, unit=\"Set\",condition=\"Set\")\n",
    "    plt.xlabel(r'Value')\n",
    "    plt.ylabel(r'Loss')\n",
    "    plt.title(\"Loss for each epoch\")\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,prop={'size': 17})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_IuO will plot the loss against the epoch\n",
    "    (other) = labels : All the labels. In 19 different classes to have an user-friendly interact interface.\n",
    "    If one of this label is True then the line associated is plot.\n",
    "    (-3) = path_CSV : path of the CSV file\n",
    "    (-2) = name_network : name of the network associated with the CSV file\n",
    "    (-1) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def plot_IuO_modif(road, sidewalk, building, wall, fence, pole, traffic_light, traffic_sign, vegetation, terrain,\n",
    "                   sky, person, rider, car, truck, bus, train, motorcycle, bicycle,                  \n",
    "                   path_CSV,\n",
    "                   name_network,\n",
    "                   train_number):\n",
    "    \n",
    "    # List of all the label that will be used \n",
    "    list_label = [road, sidewalk, building, wall, fence, pole, traffic_light, traffic_sign, vegetation, terrain,\n",
    "                  sky, person, rider, car, truck, bus, train, motorcycle, bicycle]\n",
    "    \n",
    "    # Import the CSV of the conv matrix\n",
    "    conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\")\n",
    "    \n",
    "    # In the column \"Value\" there is the TP information\n",
    "    conf_DF_TP = conf_DF[conf_DF[\"Prediction\"] == conf_DF[\"Target\"]]\n",
    "\n",
    "    # In the column \"Value\" there is the FN information\n",
    "    conf_DF_FN = conf_DF.groupby([\"Set\",\"Epoch\",\"Target\"])[\"Value\"].sum().reset_index()\n",
    "    # In the column \"Value\" there is the FP information\n",
    "    conf_DF_FP = conf_DF.groupby([\"Set\",\"Epoch\",\"Prediction\"])[\"Value\"].sum().reset_index()\n",
    "\n",
    "    # Change the name\n",
    "    conf_DF_FN.rename(columns={\"Value\": \"FN\"}, inplace=True)\n",
    "    conf_DF_FP.rename(columns={\"Value\": \"FP\"}, inplace=True)\n",
    "\n",
    "    # Merge the dataset together according to certain column\n",
    "    conf_DF_TP_FN = conf_DF_TP.merge(conf_DF_FN, on=[\"Epoch\",\"Set\",\"Target\"])\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN.merge(conf_DF_FP, on=[\"Epoch\",\"Set\",\"Prediction\"])\n",
    "    \n",
    "    # We compute the realFP and FN value, we have to substract the TP values\n",
    "    conf_DF_TP_FN_FP[\"FP\"] = conf_DF_TP_FN_FP[\"FP\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    conf_DF_TP_FN_FP[\"FN\"] = conf_DF_TP_FN_FP[\"FN\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    \n",
    "    # Compute the IoU each class\n",
    "    conf_DF_TP_FN_FP[\"IoU\"] = conf_DF_TP_FN_FP[\"Value\"]/(conf_DF_TP_FN_FP[\"Value\"] + \n",
    "                                                         conf_DF_TP_FN_FP[\"FP\"] + conf_DF_TP_FN_FP[\"FN\"])\n",
    "    \n",
    "    #Change change the name of class for the plot\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN_FP.merge(parameters.label_DF, left_on='Target', right_on='Class_name')\n",
    "    \n",
    "    # Round the Epoch value to have a value  every 10 Epoch. This line could be delete if needed\n",
    "    conf_DF_TP_FN_FP[\"Epoch\"] = round(conf_DF_TP_FN_FP[\"Epoch\"],-1)\n",
    "    \n",
    "    # Regroupe each line that have the same \"Set\",\"Epoch\",\"Real_name\" and compute the mean fot the IoU\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN_FP.groupby([\"Set\",\"Epoch\",\"Real_name\"])[\"IoU\"].mean().reset_index()\n",
    "    \n",
    "    # A list of color associated with the labels\n",
    "    flatui = [\"#804080\", \"#F423E8\", \"#464646\", \"#66669C\", \"#BE9999\", \"#999999\",\n",
    "             \"#FAAA1E\", \"#DCDC00\", \"#6B8E23\", \"#98FB98\", \"#4682B4\", \"#DC143C\",\n",
    "             \"#FF0000\", \"#00008E\", \"#000046\", \"#003C64\", \"#005064\", \"#0000E6\",\n",
    "             \"#770B20\"]\n",
    "    \n",
    "    # List of labels\n",
    "    label_name = [\"road\", \"sidewalk\", \"building\", \"wall\", \"fence\",\"pole\",\n",
    "                  \"traffic light\", \"traffic sign\",\"vegetation\", \"terrain\", \"sky\", \"person\",\n",
    "                  \"rider\", \"car\", \"truck\", \"bus\", \"train\", \"motorcycle\",\n",
    "                  \"bicycle\"]\n",
    "    \n",
    "    # frames is a list a dataframe that will be concatenate at the end of the loop\n",
    "    frames= []\n",
    "    # Only keep the color needed\n",
    "    flatui_needed = []\n",
    "    # The loop over the label look if eachlabel is set to True or False and add the information if Label si True\n",
    "    for i,label in enumerate(list_label):\n",
    "        if label:\n",
    "            frames.append(conf_DF_TP_FN_FP[conf_DF_TP_FN_FP[\"Real_name\"]==label_name[i]])\n",
    "            flatui_needed.append(flatui[i])\n",
    "    \n",
    "    # Concat all the needed DataFrames\n",
    "    conf_DF_TP_FN_FP = pd.concat(frames)\n",
    "\n",
    "    # creat 2 areas where the plot will be display, one for each \"Set\" (most of the time Training and Validation)\n",
    "    g = sns.FacetGrid(conf_DF_TP_FN_FP, row=\"Set\", size=9)\n",
    "    \n",
    "    # IoU against Epoch and a different line for each Real_name. We also add the color and the titles\n",
    "    g = (g.map_dataframe(sns.tsplot,\n",
    "                         data=conf_DF_TP_FN_FP,time=\"Epoch\",\n",
    "                         value=\"IoU\",\n",
    "                         unit=\"Real_name\", condition=\"Real_name\",\n",
    "                         color = sns.color_palette(flatui_needed))).set_titles(\"IoU per classes for both training\")\n",
    "    \n",
    "    # Define the x axis name and fontsize\n",
    "    plt.xlabel(r'Epoch',fontsize = 20)\n",
    "    # Define the y axis name and fontsize\n",
    "    plt.ylabel(r'IoU',fontsize = 20)\n",
    "    # Add a legend \n",
    "    plt.legend(bbox_to_anchor=(1.2, 1.75), loc=2, borderaxespad=0.,prop={'size': 20})\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(path_CSV, name_network, train_number):\n",
    "    \n",
    "    # Import the CSV as DataFrame\n",
    "    conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\")\n",
    "    # Select only the usefull epoch\n",
    "    #conf_DF = conf_DF[conf_DF[\"Epoch\"]==epoch]\n",
    "    # Keep only the DataSet to display\n",
    "    #conf_DF = conf_DF[conf_DF[\"Set\"]==data_set]\n",
    "\n",
    "    # Keep diag values\n",
    "    conf_diag_DF = conf_DF[conf_DF[\"Prediction\"] == conf_DF[\"Target\"]]\n",
    "\n",
    "    # Group by Epoch and Set, we get the diagonal\n",
    "    conf_diag_DF = conf_diag_DF.groupby(['Epoch', 'Set'])[\"Value\"].sum().reset_index()\n",
    "    # The number of pixel for each Epoch and Set\n",
    "    conf_DF = conf_DF.groupby(['Epoch', 'Set'])[\"Value\"].sum().reset_index()\n",
    "\n",
    "    # Divide by the number of pixel for each Epoch and Set\n",
    "    conf_diag_DF[\"Value\"] = conf_diag_DF[\"Value\"].div(conf_DF[\"Value\"])\n",
    "    # Reset sns parameter to default\n",
    "    sns.set()\n",
    "    # Set some parameter to have better plot\n",
    "    params = {'legend.fontsize': 'x-large',\n",
    "              'figure.figsize': (16, 10),\n",
    "             'axes.labelsize': 'x-large',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'xtick.labelsize':'x-large',\n",
    "             'ytick.labelsize':'x-large'}\n",
    "    pylab.rcParams.update(params)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    #fig.set_size_inches(15, 10)\n",
    "    sns.tsplot(time=\"Epoch\", value=\"Value\", data=conf_diag_DF, unit=\"Set\",condition=\"Set\")\n",
    "    plt.xlabel(r'Value')\n",
    "    plt.ylabel(r'Pixel_accuracy')\n",
    "    plt.title(\"Pixel accuracy for each epoch\")\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,prop={'size': 17})\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_mat_confusion_modif will plot confusion matrix\n",
    "    (0) = path_CSV : path to the CSV file where the data are stored\n",
    "    (0) = name_network : name of the network associated with the CSV file\n",
    "    (1) = train_number : number of the network associated with the CSV file\n",
    "    (2) = epoch : Value of the epoch were we want to display the confusion matrix\n",
    "    (3) = data_set : \n",
    "\"\"\"\n",
    "def plot_mat_confusion_modif(path_CSV,name_network,train_number,epoch,data_set,log_scale):\n",
    "    \n",
    "    # Import the CSV as DataFrame\n",
    "    conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\")\n",
    "    # Select only the usefull epoch\n",
    "    conf_DF = conf_DF[conf_DF[\"Epoch\"]==epoch]\n",
    "    # Keep only the DataSet to display\n",
    "    conf_DF = conf_DF[conf_DF[\"Set\"]==data_set]\n",
    "    # Define param of union (FalsePositiv, True Negativ, True Positiv)\n",
    "    FP = [0 for i in range(parameters.number_classes)]\n",
    "    TN = [0 for i in range(parameters.number_classes)]\n",
    "    TP = [0 for i in range(parameters.number_classes)]\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_mat_for_plot = np.zeros((parameters.number_classes,parameters.number_classes))\n",
    "    #Double loop over the confusion matrix the ad each value\n",
    "    for i in range(parameters.number_classes):\n",
    "        for j in range(parameters.number_classes):\n",
    "            conf_mat_for_plot[i,j] = conf_DF.loc[(conf_DF[\"Prediction\"]==(\"class\"+str(i))) &\n",
    "                                                 (conf_DF[\"Target\"]==(\"class\"+str(j))),\"Value\"].values\n",
    "\n",
    "    # Normalize the matrix\n",
    "    if not log_scale:\n",
    "        #for i in range(parameters.number_classes):\n",
    "            #conf_mat_for_plot[i,:] = conf_mat_for_plot[i,:]/sum(conf_mat_for_plot[i,:])\n",
    "        \n",
    "        # Round the value to have less number display in the heatmap\n",
    "        conf_mat_for_plot = np.round(conf_mat_for_plot,0)\n",
    "        \n",
    "    else:\n",
    "        row = 0\n",
    "        col = 0\n",
    "        for i in range(parameters.number_classes):\n",
    "            row += conf_mat_for_plot[i,:]\n",
    "            col += conf_mat_for_plot[:,i]\n",
    "        #conf_mat_for_plot = (conf_mat_for_plot/a)*100\n",
    "        \n",
    "        # Round the value to have less number display in the heatmap\n",
    "        conf_mat_for_plot = np.round(conf_mat_for_plot,0)\n",
    "\n",
    "    conf_mat_for_plot = conf_mat_for_plot.transpose()\n",
    "    \n",
    "    # Just change column and row name to make good plot and make the confusion matrix a dataframe\n",
    "    conf_mat_for_plot = pd.DataFrame(conf_mat_for_plot, columns = [\"road\",\"sidewalk\",\"building\",\"wall\",\"fence\",\n",
    "                                                                        \"pole\",\"traffic light\",\"traffic sign\",\n",
    "                                                                        \"vegetation\",\"terrain\",\"sky\",\"person\",\"rider\",\n",
    "                                                                        \"car\",\"truck\",\"bus\",\"train\",\"motorcycle\",\n",
    "                                                                        \"bicycle\"])\n",
    "    \n",
    "    # Rename the rows\n",
    "    conf_mat_for_plot = conf_mat_for_plot.rename({0 : \"road\", 1 : \"sidewalk\", 2 : \"building\", 3 : \"wall\",4 : \"fence\",\n",
    "                                                  5 : \"pole\",6 : \"traffic light\",7 : \"traffic sign\", 8 : \"vegetation\",\n",
    "                                                  9 : \"terrain\",10 : \"sky\" ,11 : \"person\" ,12 : \"rider\", 13 :\"car\",\n",
    "                                                  14 : \"truck\",15 : \"bus\",16 : \"train\",17 : \"motorcycle\",\n",
    "                                                  18 : \"bicycle\"},axis='index')\n",
    "\n",
    "    # Set the size\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(13, 13)\n",
    "    conf_mat_for_plot = conf_mat_for_plot.astype(int)\n",
    "    conf_mat_for_plot[conf_mat_for_plot==0] = float('nan')\n",
    "    if log_scale:\n",
    "        #conf_mat_for_plot += 0.1\n",
    "        sns.heatmap(conf_mat_for_plot,\n",
    "                    annot=True,\n",
    "                    ax=ax,\n",
    "                    square = True,\n",
    "                    cmap = \"Reds\",\n",
    "                    fmt='g',\n",
    "                    norm=LogNorm(vmin=conf_mat_for_plot.min().min() + 1, vmax=conf_mat_for_plot.max().max()),\n",
    "                    cbar=False)\n",
    "    else:\n",
    "        sns.heatmap(conf_mat_for_plot,\n",
    "                    annot=True,\n",
    "                    ax=ax,\n",
    "                    square = True,\n",
    "                    cmap = \"Reds\",\n",
    "                    fmt='g')\n",
    "    if False:\n",
    "        # PLot line to separate True Positif from the error for an example\n",
    "        plt.plot([0, 17], [2, 2], linewidth=2, color=\"blue\")\n",
    "        plt.plot([0, 17], [1, 1], linewidth=2, color=\"blue\")\n",
    "        plt.plot([0, 0], [1, 2], linewidth=2, color=\"blue\")\n",
    "        plt.plot([17, 17], [1, 2], linewidth=2, color=\"green\")\n",
    "\n",
    "        plt.plot([18, 19], [2, 2], linewidth=2, color=\"blue\")\n",
    "        plt.plot([18, 19], [1, 1], linewidth=2, color=\"blue\")\n",
    "        plt.plot([19, 19], [1, 2], linewidth=2, color=\"blue\")\n",
    "        plt.plot([18, 18], [1, 2], linewidth=2, color=\"green\")\n",
    "\n",
    "        plt.plot([17, 17], [2, 19], linewidth=2, color=\"purple\")\n",
    "        plt.plot([18, 18], [2, 19], linewidth=2, color=\"purple\")\n",
    "        plt.plot([17, 18], [19, 19], linewidth=2, color=\"purple\")\n",
    "        plt.plot([17, 18], [2, 2], linewidth=2, color=\"green\")\n",
    "\n",
    "        plt.plot([18, 18], [0, 1], linewidth=2, color=\"purple\")\n",
    "        plt.plot([17, 17], [0, 1], linewidth=2, color=\"purple\")\n",
    "        plt.plot([17, 18], [0, 0], linewidth=2, color=\"purple\")\n",
    "        plt.plot([17, 18], [1, 1], linewidth=2, color=\"green\")\n",
    "\n",
    "    #ax.annotate('True Positive', xy=(17.25, 1.25), xytext=(13.75, 0.30), color=\"green\", fontsize=15,\n",
    "    #        arrowprops=dict(facecolor='green', shrink=0.05))\n",
    "    #ax.text(16, 8.30, \"False Positive\", fontsize=15, color = \"purple\")\n",
    "    #ax.text(13.75, 1.30, \"False Negative\", fontsize=15, color = \"blue\")\n",
    "\n",
    "    plt.xlabel(r'Prediction',fontsize = 20)\n",
    "    plt.ylabel(r'Real class',fontsize = 20)\n",
    "    #plt.title(\"Confusion matrix of \" + data_set + \"set for epoch \" + str(epoch), fontsize = 20)\n",
    "    plt.title(\"Matrice de confusion\", fontsize = 20)\n",
    "    plt.savefig(filename=\"Matrice_confusion.eps\", format='eps',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_IuO will plot the loss against the epoch\n",
    "    (0) = path_CSV : path of the CSV file\n",
    "    (1) = name_network : name of the network associated with the CSV file\n",
    "    (2) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def plot_IuO(path_CSV,name_network,train_number):\n",
    "    \n",
    "    # Import the CSV of the conv matrix\n",
    "    conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\")\n",
    "    \n",
    "    # In the column \"Value\" there is the TP information\n",
    "    conf_DF_TP = conf_DF[conf_DF[\"Prediction\"] == conf_DF[\"Target\"]]\n",
    "\n",
    "    # In the column \"Value\" there is the FN information\n",
    "    conf_DF_FN = conf_DF.groupby([\"Set\",\"Epoch\",\"Target\"])[\"Value\"].sum().reset_index()\n",
    "    # In the column \"Value\" there is the FP information\n",
    "    conf_DF_FP = conf_DF.groupby([\"Set\",\"Epoch\",\"Prediction\"])[\"Value\"].sum().reset_index()\n",
    "\n",
    "    # Change the name\n",
    "    conf_DF_FN.rename(columns={\"Value\": \"FN\"}, inplace=True)\n",
    "    conf_DF_FP.rename(columns={\"Value\": \"FP\"}, inplace=True)\n",
    "\n",
    "    # Merge the dataset together according to certain column\n",
    "    conf_DF_TP_FN = conf_DF_TP.merge(conf_DF_FN, on=[\"Epoch\",\"Set\",\"Target\"])\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN.merge(conf_DF_FP, on=[\"Epoch\",\"Set\",\"Prediction\"])\n",
    "    \n",
    "    # We compute the realFP and FN value, we have to substract the TP values\n",
    "    conf_DF_TP_FN_FP[\"FP\"] = conf_DF_TP_FN_FP[\"FP\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    conf_DF_TP_FN_FP[\"FN\"] = conf_DF_TP_FN_FP[\"FN\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    \n",
    "    # Compute the IoU each class\n",
    "    conf_DF_TP_FN_FP[\"IoU\"] = conf_DF_TP_FN_FP[\"Value\"]/(conf_DF_TP_FN_FP[\"Value\"] + \n",
    "                                                         conf_DF_TP_FN_FP[\"FP\"] + conf_DF_TP_FN_FP[\"FN\"])\n",
    "\n",
    "    # Round the Epoch value to have a value  every 10 Epoch. This line could be delete if needed\n",
    "    #conf_DF_TP_FN_FP[\"Epoch\"] = round(conf_DF_TP_FN_FP[\"Epoch\"],-1)\n",
    "    \n",
    "    # Regroupe each line that have the same \"Set\",\"Epoch\",\"Real_name\" and compute the mean fot the IoU\n",
    "    plot_DF = conf_DF_TP_FN_FP.groupby([\"Set\",\"Epoch\"])[\"IoU\"].mean().reset_index()\n",
    "    \n",
    "    # Start the plot part\n",
    "    \n",
    "    # Reset sns parameter to default\n",
    "    sns.set()\n",
    "    # Set some parameter to have better plot\n",
    "    params = {'legend.fontsize': 'x-large',\n",
    "              'figure.figsize': (16, 10),\n",
    "             'axes.labelsize': 'x-large',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'xtick.labelsize':'x-large',\n",
    "             'ytick.labelsize':'x-large'}\n",
    "    pylab.rcParams.update(params)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    # Epoch against IoU. A line for each Set\n",
    "    sns.tsplot(time=\"Epoch\", value=\"IoU\", data=plot_DF, unit=\"Set\",condition=\"Set\")\n",
    "    # Define the x axis name and fontsize\n",
    "    plt.xlabel(r'Epoch')\n",
    "    # Define the y axis name and fontsize\n",
    "    plt.ylabel(r'IoU mean')\n",
    "    plt.title(\"IoU mean for each epoch\")\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters=Parameters.Parameters(# Number of columns of the grid\n",
    "                                 nColumns = 6,\n",
    "                                 # Number of features map at each rows\n",
    "                                 nFeatMaps = [8,16,32,64,128],\n",
    "                                 # Number of feature map of the input image\n",
    "                                 nFeatureMaps_init = 3,\n",
    "                                 #Number of classes (19(usefull classes) + 1(all other classes together))\n",
    "                                 number_classes = 20 - 1,\n",
    "                                 # DataFrame with the name of each label associated with their number\n",
    "                                 label_DF = Label.create_label(),\n",
    "\n",
    "                                 # File where all the parameter model can be store\n",
    "                                 path_save_net = \"Model/\",\n",
    "                                 #Name of the network, used for store (name_network and train_number)\n",
    "                                 #name_network = \"architecture_gpu\",\n",
    "                                 name_network = \"cross_to_IoU_scratch\",\n",
    "                                 train_number = 0,\n",
    "                                 # File where the error will be stored\n",
    "                                 path_CSV = \"CSV/\")\n",
    "\n",
    "name_network = [\"focal_loss2\",\"focal_loss\", \"focal_loss_small\",\n",
    "                \"resnet18\",\"resnet18_low_lr\",\n",
    "                \"Lovasz\",\"cross_to_IoU_Lovasz\", \"pretrain_test\",\n",
    "                \"better_IoU_approximation\",\"one_image\",\"cross_to_IoU_scratch\",\n",
    "                \"hinge_from_cross\",\"IoU_from_cross\",\"IoU_lr_decay\",\n",
    "                \"dam_weight_decay_each_batch\" ,\"IoU_decay_each_batch\", \"decay_each_epoch\",\n",
    "                \"Decay_final\", \"New_decay\", \"hinge\", \"decay6_each_batch\",\n",
    "                \"LongTest\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    organise_CSV(path_CSV = parameters.path_CSV, name_network = name_network[i],train_number = parameters.train_number,\n",
    "             both=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802a91c3dd634b30b7151a79249158e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_loss>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_loss,\n",
    "         path_CSV = parameters.path_CSV,\n",
    "         name_network = name_network,\n",
    "         train_number = parameters.train_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47b14ccebe34db6a2dd16fb7b1ffa43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.pixel_accuracy>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(pixel_accuracy,\n",
    "         path_CSV = parameters.path_CSV,\n",
    "         name_network = name_network,\n",
    "         train_number = parameters.train_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f33e754a6f649adb3d1e1179671c8f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_IuO>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_IuO,\n",
    "         path_CSV = parameters.path_CSV,\n",
    "         name_network = name_network,\n",
    "         train_number = parameters.train_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18e0d0b025243968e9ff067af74d9be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_mat_confusion_modif>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = [\"train\", \"validation\"]\n",
    "epoch_total= [i for i in range(400)]\n",
    "log_scale = [True,False]\n",
    "\n",
    "interact(plot_mat_confusion_modif, path_CSV = parameters.path_CSV,\n",
    "         name_network = name_network,train_number = parameters.train_number,\n",
    "         epoch = 399,data_set = data_set, log_scale = log_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56fe02e8f284db1b0041d8932ed59f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_IuO_modif>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed = [True,False]\n",
    "interact(plot_IuO_modif,\n",
    "         road = needed, sidewalk = needed, building = needed, wall = needed, fence = needed,pole = needed,\n",
    "         traffic_light = needed, traffic_sign = needed, vegetation = needed, terrain = needed, sky = needed,\n",
    "         person = needed, rider = needed, car = needed, truck = needed, bus = needed, train = needed,\n",
    "         motorcycle = needed, bicycle = needed,\n",
    "         \n",
    "         title = \"Enter a plot title!\",\n",
    "         path_CSV = parameters.path_CSV,\n",
    "         name_network = name_network,\n",
    "         train_number = parameters.train_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program that count the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.08594745e-01   4.33242058e-02   2.36556657e-01   3.03907861e-03\n",
      "   5.85874431e-03   1.12567472e-02   1.98426151e-03   7.51448570e-03\n",
      "   1.73769207e-01   1.71847910e-02   3.09820367e-02   3.12143336e-03\n",
      "   7.21845376e-04   5.32527900e-02   0.00000000e+00   2.59391780e-04\n",
      "   0.00000000e+00   6.35338646e-04   1.94423984e-03]\n",
      "[  2.44741278e+00   2.30817849e+01   4.22731709e+00   3.29047099e+02\n",
      "   1.70685039e+02   8.88356098e+01   5.03965830e+02   1.33076306e+02\n",
      "   5.75475952e+00   5.81909898e+01   3.22767676e+01   3.20365641e+02\n",
      "   1.38533824e+03   1.87783588e+01              inf   3.85517228e+03\n",
      "              inf   1.57396375e+03   5.14339835e+02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanguy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:80: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import GridNet_structure\n",
    "import Parameters\n",
    "import Label\n",
    "import Save_import\n",
    "\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os.path\n",
    "import torch\n",
    "\n",
    "def count_classes(y_batch, parameters):\n",
    "    count = np.array([0 for i in range(parameters.number_classes)])\n",
    "    for i in range(parameters.number_classes):\n",
    "        count[i] = torch.sum(y_batch == i)\n",
    "    return(count)\n",
    "\n",
    "def f():\n",
    "    # Define all the parameters\n",
    "    parameters = Parameters.Parameters(nColumns=2,\n",
    "                                       nFeatMaps=[8, 16],\n",
    "                                       nFeatureMaps_init=3,\n",
    "                                       number_classes=20 - 1,\n",
    "                                       label_DF=Label.create_label(),\n",
    "\n",
    "                                       width_image_initial=2048, height_image_initial=1024,\n",
    "                                       size_image_crop=50,\n",
    "\n",
    "                                       dropFactor=0.1,\n",
    "                                       learning_rate=0.01,\n",
    "                                       weight_decay=5 * 10 ** (-6),\n",
    "                                       beta1=0.9,\n",
    "                                       beta2=0.999,\n",
    "                                       epsilon=1 * 10 ** (-8),\n",
    "                                       batch_size=1,\n",
    "                                       batch_size_val=8,\n",
    "                                       epoch_total=200,\n",
    "                                       actual_epoch=0,\n",
    "                                       scale=(0.39, 0.5),\n",
    "                                       ratio=(1, 1),\n",
    "\n",
    "                                       path_save_net=\"./Model/\",\n",
    "                                       name_network=\"IoU_loss8\",\n",
    "                                       train_number=0,\n",
    "                                       path_CSV=\"./CSV/\",\n",
    "                                       path_data=\"/home_expes/collections/Cityscapes/\",\n",
    "                                       #path_data=\"../Cityscapes_Copy/\",\n",
    "                                       path_print=\"./Python_print8.txt\",\n",
    "                                       path_result=\"./Result\",\n",
    "                                       num_workers=2)\n",
    "    \n",
    "    parameters.transforms_output = transforms.Compose([transforms.ToTensor()])\n",
    "    parameters.transforms_input = transforms.Compose([transforms.ToTensor()])\n",
    "                                      \n",
    "    \n",
    "    # Define the GridNet\n",
    "    network = GridNet_structure.gridNet(nInputs=parameters.nFeatureMaps_init,\n",
    "                                        nOutputs=parameters.number_classes,\n",
    "                                        nColumns=parameters.nColumns,\n",
    "                                        nFeatMaps=parameters.nFeatMaps,\n",
    "                                        dropFactor=parameters.dropFactor)\n",
    "\n",
    "\n",
    "    # Import both DataSets with the transformation\n",
    "    train_dataset = Save_import.CityScapes_final('fine', 'train',\n",
    "                                                 transform=parameters.transforms_input,\n",
    "                                                 transform_target=parameters.transforms_output,\n",
    "                                                 parameters=parameters)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=parameters.batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=parameters.num_workers,\n",
    "                                               drop_last=False)\n",
    "    \n",
    "    count = np.array([0 for i in range(parameters.number_classes)])\n",
    "    \n",
    "    for i, (x_batch, y_batch, _) in enumerate(train_loader):\n",
    "        count += count_classes(y_batch,parameters)\n",
    "    count = count/sum(count)\n",
    "    print(count)\n",
    "    return(1/count)\n",
    "    \n",
    "a = f()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differente approximation de l'IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "import scipy.special as special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_tanguy(x,y,i=0):\n",
    "    inter = sum(x * y)\n",
    "    union = sum(x + y) - inter\n",
    "    return(inter/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*(x>0.5)\n",
    "\n",
    "def F(x_0,x_1):\n",
    "    #print(x_0)\n",
    "    #print(x_1)\n",
    "    #print(integrate.quad(f, x_1, x_0)[0])\n",
    "    return integrate.quad(f, x_1, x_0)[0]\n",
    "\n",
    "def IoU_article_distrib(x,y,j):\n",
    "    #print(x)\n",
    "    liste_1 = [1] * len(x)\n",
    "    liste_0 = [0] * len(x)\n",
    "    total = (F(1, x[0])) * (IoU_tanguy(np.array(liste_0),y,j))\n",
    "    #print((IoU_tanguy(np.array(liste_0),y,j)))\n",
    "\n",
    "    for i in range(0, len(x) - 1):\n",
    "        #multi_proba = x[i] #* (1 - y[i]) + (1 - x[i]) * y[i]\n",
    "        total += (IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)) * (F(x[i], x[i+1]))\n",
    "        #print(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]))\n",
    "        #print(total)\n",
    "        if j == 20 and False:\n",
    "            print(\"new distribution\")\n",
    "            print(i)\n",
    "            print(\"total : \",total)\n",
    "            print(\"(IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)) : \",(IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)))\n",
    "            print(\"F(x[i] - x[i+1])\", F(x[i], x[i+1]))\n",
    "    total += (IoU_tanguy(np.array(liste_1),y,j)) * (F(x[-1], 0))\n",
    "    #print(np.array(liste_1))\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_article(x,y,j):\n",
    "    #print(x)\n",
    "    liste_1 = [1] * len(x)\n",
    "    liste_0 = [0] * len(x)\n",
    "    total = (1 - x[0]) * (IoU_tanguy(np.array(liste_0),y,j))\n",
    "    #print((IoU_tanguy(np.array(liste_0),y,j)))\n",
    "    #if j == 20:\n",
    "        #print(x)\n",
    "    for i in range(0, len(x) - 1):\n",
    "        #multi_proba = x[i] #* (1 - y[i]) + (1 - x[i]) * y[i]\n",
    "        total += (IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)) * (x[i] - x[i+1])\n",
    "        #print(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]))\n",
    "        #print(total)\n",
    "        if j == 40 and False:\n",
    "            print(\"basic article\")\n",
    "            print(i)\n",
    "            print(\"total : \",total)\n",
    "            print(\"(IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)) : \",\n",
    "                  (IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)))\n",
    "            print(\"(x[i] - x[i+1])\", (x[i] - x[i+1]))\n",
    "    total += (IoU_tanguy(np.array(liste_1),y,j)) * (x[-1] - 0)\n",
    "    #print(np.array(liste_1))\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_lovasz_product(x,y,j):\n",
    "    #print(x)\n",
    "    liste_1 = [1] * len(x)\n",
    "    liste_0 = [0] * len(x)\n",
    "    total = (IoU_tanguy(np.array(liste_0),y,j)) ** (1 - x[0]) \n",
    "    #print((IoU_tanguy(np.array(liste_0),y,j)))\n",
    "    for i in range(0, len(x) - 1):\n",
    "        #multi_proba = x[i] #* (1 - y[i]) + (1 - x[i]) * y[i]\n",
    "        total *= (IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j)) ** (x[i] - x[i+1])\n",
    "        #print(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]))\n",
    "        #print(total)\n",
    "        #if total > 0.5:\n",
    "            #print(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]))\n",
    "            #print(y)\n",
    "    total *= (IoU_tanguy(np.array(liste_1),y,j)) ** (x[-1] - 0)\n",
    "    #print(np.array(liste_1))\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "from random import gauss\n",
    "import random\n",
    "import math\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "def f_temp(x):\n",
    "    return (1/(math.sqrt(2*math.pi)*sigma))*math.exp(-(1/2)*((x-0.9)**2)/(sigma**2))\n",
    "    #return((0.5**0.5)-(x-0.5)**0.5)\n",
    "constante_norm = (integrate.quad(f_temp, 0, 1)[0])\n",
    "\n",
    "def f(x):\n",
    "    #return ((0.5**0.5)-(x-0.5)**0.5)/constante_norm\n",
    "    return (1/(math.sqrt(2*math.pi)*sigma))*math.exp(-(1/2)*((x-(0.9))**2)/(sigma**2))/constante_norm\n",
    "    #return 5*(x<0.6)*(x>0.4)\n",
    "print(F(1,0))\n",
    "\n",
    "def differentIoU(nombre_positif, max_iter, nb_neutre, ecart_type):\n",
    "    x = np.array([0] * 100)\n",
    "    liste_1 = [1] * len(x)\n",
    "    liste_0 = [0] * len(x)\n",
    "    tanguy = []\n",
    "    article = []\n",
    "    part_entiere = []\n",
    "    Lovasz_product = []\n",
    "    article_distrib = []\n",
    "    for i in range(-1, max_iter + 1):\n",
    "        y = np.array(liste_1[0:nombre_positif] + liste_0[nombre_positif:len(x)])\n",
    "        a = (0.5*max_iter + i*0.5)/max_iter\n",
    "\n",
    "        j=0\n",
    "        indice = a\n",
    "        x_not_order = []\n",
    "        while j < len(x):\n",
    "            g = gauss(indice,ecart_type)\n",
    "            #g = random.random()*ecart_type - 0.5*ecart_type + indice\n",
    "            #print(g)\n",
    "            if 0 <= g <= 1:\n",
    "                x_not_order.append(g)\n",
    "                j += 1\n",
    "                #if j >= nombre_positif - nb_neutre and j <= nombre_positif + nb_neutre:\n",
    "                #if j == (len(x)-2):\n",
    "                #    indice = 0.5\n",
    "                if j >= nombre_positif + nb_neutre:\n",
    "                    indice = 1 - a\n",
    "        if i == (max_iter):\n",
    "            x_not_order = liste_1[0:nombre_positif] + liste_0[nombre_positif:len(x)]\n",
    "        if i == -1:\n",
    "            x_not_order = liste_0\n",
    "\n",
    "        x = np.array(x_not_order)\n",
    "        #if i ==40:\n",
    "        #    print(x)\n",
    "        x_special = x #* (1-y) + (1 - x) * y\n",
    "        list_indice_sorted = np.argsort(x)\n",
    "        list_indice_sorted = list_indice_sorted[::-1]\n",
    "        x = x[list_indice_sorted]\n",
    "        if i ==40:\n",
    "            print(x)\n",
    "        y = y[list_indice_sorted]\n",
    "        y[1] = 0\n",
    "        y[-3] = 1\n",
    "        #list_indice_sorted_special = np.argsort(x_special)\n",
    "        #list_indice_sorted_special = list_indice_sorted_special[::-1]\n",
    "        #x_special = x[list_indice_sorted_special]\n",
    "        #y_special = y[list_indice_sorted_special]\n",
    "        tanguy.append(IoU_tanguy(x,y,i))\n",
    "        #Lovasz_product.append(IoU_lovasz_product(x,y,i))\n",
    "        article.append(IoU_article(x,y,i))\n",
    "        article_distrib.append(IoU_article_distrib(x,y,i))\n",
    "        x_part_entiere = [round(j) for j in x]\n",
    "        part_entiere.append(IoU_tanguy(x_part_entiere,y,i))\n",
    "\n",
    "\n",
    "    plt.plot(tanguy,label = \"tanguy\")\n",
    "    plt.plot(article,label = \"article\")\n",
    "    plt.plot(part_entiere,label = \"part_entiere\")\n",
    "    #plt.plot(Lovasz_product, label = \"Lovasz_product\")\n",
    "    plt.plot(article_distrib, label = \"article_distrib\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442abbae82cb4fa39c6e310d2373bb44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.differentIoU>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_ecart_type = [0.001,0.01,0.05,0.1]\n",
    "interact(differentIoU,\n",
    "         nombre_positif = 50,\n",
    "         max_iter = 50,\n",
    "         nb_neutre = 0,\n",
    "         ecart_type = l_ecart_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-61a28bb072e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-61a28bb072e6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([f(x/100) for x in range(101)])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_article(x,y,j):\n",
    "    liste_1 = [1] * len(x)\n",
    "    liste_0 = [0] * len(x)\n",
    "    total = [(IoU_tanguy(np.array(liste_1[0:1] + liste_0[1:len(x)]),y,j) - 0)]\n",
    "    for i in range(1, len(x)):\n",
    "        total.append((IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j) -\n",
    "                  IoU_tanguy(np.array(liste_1[0:i] + liste_0[i:len(x)]),y,j)))\n",
    "    return(total)\n",
    "\n",
    "\n",
    "def gradient_tanguy(x,y,tanguy,i=0):\n",
    "    inter = sum(x * y)\n",
    "    union = sum(x + y) - inter\n",
    "    for k in range(len(x)):\n",
    "        if y[k] == 0:\n",
    "            tanguy.append(-1/(union))\n",
    "        else:\n",
    "            tanguy.append(-inter/(union))\n",
    "            \n",
    "def gradient_tanguy(x,y,tanguy,i=0):\n",
    "    liste_1 = [1] * len(x)\n",
    "    liste_0 = [0] * len(x)\n",
    "    tanguy.append((IoU_tanguy(np.array(liste_1[0:1] + liste_0[1:len(x)]),y,j) - 0)*math.exp(x[0]))\n",
    "    for i in range(1, len(x)):\n",
    "        tanguy.append((IoU_tanguy(np.array(liste_1[0:i+1] + liste_0[i+1:len(x)]),y,j) -\n",
    "                  IoU_tanguy(np.array(liste_1[0:i] + liste_0[i:len(x)]),y,j))*math.exp(x[i]))\n",
    "    return(tanguy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gradient_tanguy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e3ec2e0d1ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_indice_sorted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_indice_sorted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mgradient_tanguy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtanguy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0marticle\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mgradient_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mk_total\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gradient_tanguy' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import gauss\n",
    "import random\n",
    "\n",
    "x = np.array([0] * 100)\n",
    "nombre_positif = 10\n",
    "liste_1 = [1] * len(x)\n",
    "liste_0 = [0] * len(x)\n",
    "tanguy = []\n",
    "article = []\n",
    "part_entiere = []\n",
    "max_iter = 50\n",
    "max_iter_total = []\n",
    "k_total = []\n",
    "for i in range(max_iter):\n",
    "    y = np.array(liste_1[0:nombre_positif] + liste_0[nombre_positif:len(x)])\n",
    "    a = (0.5*max_iter + i*0.5)/max_iter\n",
    "    ecart_type = 0.5\n",
    "    \n",
    "    j=0\n",
    "    indice = a\n",
    "    x_not_order = []\n",
    "    while j < len(x):\n",
    "        #g = gauss(indice,ecart_type)\n",
    "        g = random.random()*ecart_type - 0.5*ecart_type + indice\n",
    "        #print(g)\n",
    "        if 0 <= g <= 1:\n",
    "            x_not_order.append(g)\n",
    "            j += 1\n",
    "            if j >= nombre_positif - 2:\n",
    "                indice = 0.5\n",
    "            if j >= nombre_positif + 5:\n",
    "                indice = 1 - a\n",
    "    \n",
    "    x = np.array(x_not_order)\n",
    "    list_indice_sorted = np.argsort(x)\n",
    "    list_indice_sorted = list_indice_sorted[::-1]\n",
    "    x = x[list_indice_sorted]\n",
    "    y = y[list_indice_sorted]\n",
    "    gradient_tanguy(x,y,tanguy,i)\n",
    "    article+=gradient_article(x,y,i)\n",
    "    k_total+=([a for a in range(len(x))])\n",
    "    max_iter_total+=[i for j in range(len(x))]\n",
    "    x_part_entiere = [round(j) for j in x]\n",
    "    #part_entiere.append(IoU_tanguy(x_part_entiere,y,i))\n",
    "    \n",
    "#plt.plot(tanguy,label = \"tanguy\")\n",
    "#plt.plot(article,label = \"article\")\n",
    "#plt.plot(part_entiere,label = \"part_entiere\")\n",
    "#plt.legend()\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure(1)\n",
    "ax = plt.axes(projection='3d')  \n",
    "    \n",
    "print(len(tanguy))\n",
    "print(len(max_iter_total))\n",
    "print(len(k_total))\n",
    "#ax.plot3D(max_iter_total,k_total, tanguy, 'gray')\n",
    "\n",
    "#ax.scatter(max_iter_total,k_total, article)\n",
    "ax.scatter(max_iter_total,k_total, tanguy)\n",
    "\n",
    "plt.show()\n",
    "plt.figure(2)\n",
    "ax = plt.axes(projection='3d')  \n",
    "ax.scatter(max_iter_total,k_total, article)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work on momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1b1de5b7b8>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_1 = 0.01\n",
    "a = 0.07\n",
    "Ireel = [lambda_1]\n",
    "E_I_t = [0]\n",
    "for n in range(1000):\n",
    "    bernoulli = np.random.binomial(1,0.5)\n",
    "    Ireel.append(Ireel[n] + lambda_1 )#* bernoulli - lambda_1 * (1 - bernoulli))\n",
    "    E_I_t.append(a*Ireel[n]  + (1-a)*E_I_t[n])\n",
    "for n in range(1001):\n",
    "    E_I_t[n] = E_I_t[n] - Ireel[n]\n",
    "plt.plot([n for n in range(1001)],E_I_t)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5e4b3cb47ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'variance' is not defined"
     ]
    }
   ],
   "source": [
    "sigma = 1\n",
    "a = 0.07\n",
    "t = [sigma**2]\n",
    "for n in range(1000):\n",
    "    t.append(sigma**2 * (a)**2 + ((1-(a))**2)*(t[n]))\n",
    "plt.plot([n for n in range(1001)],np.array(t))\n",
    "print(variance(a,sigma))\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance(a,sigma):\n",
    "    return((a*(sigma**2))/(2-a))\n",
    "\n",
    "def EQM(a,sigma,lambda_1):\n",
    "    return(variance(a,sigma) + (((1-a)*lambda_1)/a)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a*sigma**2/(-a + 2)**2 + sigma**2/(-a + 2) + lambd**2*(2*a - 2)/a**2 - 2*lambd**2*(-a + 1)**2/a**3\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "a = sympy.symbols('a')\n",
    "sigma = sympy.symbols('sigma')\n",
    "lambd = sympy.symbols('lambd')\n",
    "EQM = ((1-a)*lambd/a)**2 + (sigma**2)*a/(2-a)\n",
    "print(sympy.diff(EQM,a))\n",
    "solution = sympy.solve(EQM,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4*lambd**2/(3*(lambd**2 - sigma**2)) - (16*lambd**4/(lambd**2 - sigma**2)**2 - 15*lambd**2/(lambd**2 - sigma**2))/(3*(-64*lambd**6/(lambd**2 - sigma**2)**3 + 90*lambd**4/(lambd**2 - sigma**2)**2 - 27*lambd**2/(lambd**2 - sigma**2) + sqrt(-4*(16*lambd**4/(lambd**2 - sigma**2)**2 - 15*lambd**2/(lambd**2 - sigma**2))**3 + (-128*lambd**6/(lambd**2 - sigma**2)**3 + 180*lambd**4/(lambd**2 - sigma**2)**2 - 54*lambd**2/(lambd**2 - sigma**2))**2)/2)**(1/3)) - (-64*lambd**6/(lambd**2 - sigma**2)**3 + 90*lambd**4/(lambd**2 - sigma**2)**2 - 27*lambd**2/(lambd**2 - sigma**2) + sqrt(-4*(16*lambd**4/(lambd**2 - sigma**2)**2 - 15*lambd**2/(lambd**2 - sigma**2))**3 + (-128*lambd**6/(lambd**2 - sigma**2)**3 + 180*lambd**4/(lambd**2 - sigma**2)**2 - 54*lambd**2/(lambd**2 - sigma**2))**2)/2)**(1/3)/3"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EQM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ac11868fb46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlambda_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumber_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtoplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEQM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-ac11868fb46b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlambda_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumber_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtoplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEQM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EQM' is not defined"
     ]
    }
   ],
   "source": [
    "sigma = 0.1\n",
    "lambda_1 = 0.05\n",
    "number_iteration = 500\n",
    "toplot = np.array([EQM(a/number_iteration,sigma,lambda_1) for a in range(1,number_iteration)])\n",
    "print(np.argmin(toplot)/number_iteration)\n",
    "plt.plot([(a/number_iteration) for a in range(1,number_iteration)],toplot)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1b1de4d240>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[1]\n",
    "l2=[0.1]\n",
    "momentum1 = 0.99\n",
    "for i in range(1,1000):\n",
    "    l.append(l[i-1]*momentum1 + (1-momentum1))\n",
    "    l2.append(l2[i-1]*0.9 + 0.1)\n",
    "plt.plot(l)\n",
    "plt.plot(l2)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "x = Variable(torch.FloatTensor([[1,1],[1,1]]), requires_grad=True)\n",
    "\n",
    "result_global = Variable(torch.zeros(2,2), requires_grad=False)\n",
    "result_global2 =  Variable(torch.zeros(2,2), requires_grad=False)\n",
    "\n",
    "for i in range(1000):\n",
    "    if i > 0:\n",
    "        x.grad.data.zero_()\n",
    "    \n",
    "    y = x\n",
    "    IoU = 0\n",
    "    #result_global2 = y * 0.1# + result_global * 0.9\n",
    "    out_global = Variable(torch.zeros(2), requires_grad=False)\n",
    "    for k in range(2):\n",
    "        out_global[k] = torch.sum(y[:,k])\n",
    "\n",
    "    for i in range(2):\n",
    "        \n",
    "        IoU += out_global[i]/(out_global[0])\n",
    "    IoU.backward(retain_graph=False)\n",
    "\n",
    "    x.data = x.data - x.grad.data * 0.1\n",
    "    result_global.data = result_global2.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1b1de5a278>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_init = 0.01\n",
    "lr_decay = 1*10**-1\n",
    "lr_epoch = []\n",
    "a = -0.000045\n",
    "a = - 4.5 * 10**(-5)\n",
    "b = 0\n",
    "for epoch in range(400,800):\n",
    "    if epoch > 700:\n",
    "        lr_epoch.append(lr_init/10)\n",
    "    elif epoch > 500:\n",
    "        lr_epoch.append(a*(epoch-500) + lr_init)\n",
    "\n",
    "    else:\n",
    "        lr_epoch.append(lr_init)\n",
    "\n",
    "plt.plot(lr_epoch)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of parameter of GridNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre activ 113643408\n",
      "Go param 1.6934187412261963\n"
     ]
    }
   ],
   "source": [
    "print(\"nombre activ\",6*353*353*8*(4*3+3*3-2))\n",
    "print(\"Go param\",6*353*353*8*(4*3+3*3-2)*16/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre activ 401359296\n",
      "Go activ 10.466259241104126\n"
     ]
    }
   ],
   "source": [
    "size_image = 401\n",
    "Ncu = 4\n",
    "Ncs = 4\n",
    "Ns = 5\n",
    "F0 = 16\n",
    "nombre_image = 7\n",
    "print(\"nombre activ\",6*size_image*size_image*F0*(4*Ncu+3*Ncs-2))\n",
    "print(\"Go activ\",6*nombre_image*size_image*size_image*F0*(4*Ncu+3*Ncs-2)*4/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre param 30670848\n",
      "Go param 0.1142578125\n"
     ]
    }
   ],
   "source": [
    "print(\"nombre param\",18*2**(2*(Ns-1))*(F0**2)*(4*Ncu + 3*Ncs - 2))\n",
    "print(\"Go param\",18*2**(2*(Ns-1))*(F0**2)*(4*Ncu + 3*Ncs - 2)*4/(1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight of Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proportion of classes in the cityscape dataset\n",
    "weight_grad=torch.FloatTensor([1., 6.05912619, 1.61538805, 56.25538031,\n",
    "                                                                          42.02768709, 30.04029159, 177.43320666,\n",
    "                                                                          66.881359, 2.31467787, 31.84411732,\n",
    "                                                                          9.17388852, 30.24691356, 272.86243109,\n",
    "                                                                          5.27120742, 137.85345999, 156.76451618,\n",
    "                                                                          158.30407732, 373.71255691, 89.07728323]),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
