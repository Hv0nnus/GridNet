{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torchviz\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import matplotlib.pylab as pylab\n",
    "from ipywidgets import interact\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadCsv(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            data.append(np.array(row))\n",
    "    data = np.array(data)\n",
    "    (n, d) = data.shape\n",
    "    return data, n, d\n",
    "\n",
    "\n",
    "def loadSatimage():\n",
    "    data, n, d = loadCsv('satimage/satimage.data')\n",
    "    rawX = data[:, 0:d-1].astype(float)\n",
    "    rawY = data[:, d-1].astype(int)\n",
    "    #rawY[rawY != 7] = 0\n",
    "    rawY[rawY == 7] = 0\n",
    "    #rawY[rawY == 7] = 1\n",
    "    return rawX, rawY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1508\n",
      "1: 1533\n",
      "2: 703\n",
      "3: 1358\n",
      "4: 626\n",
      "5: 707\n",
      "6: 0\n"
     ]
    }
   ],
   "source": [
    "X,Y = loadSatimage()\n",
    "for i in range(7):\n",
    "    print(str(i) + \":\",sum(Y==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n",
      "Network_2n_1(\n",
      "  (batch1): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (first_linear): Linear(in_features=36, out_features=73, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (linear0): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid0): Sigmoid()\n",
      "  (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (linear3): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (linear4): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (linear5): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid5): Sigmoid()\n",
      "  (linear6): Linear(in_features=73, out_features=73, bias=True)\n",
      "  (sigmoid6): Sigmoid()\n",
      "  (last_linear): Linear(in_features=73, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network_2n_1(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features for the input\n",
    "    (2) = nOutput : number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs, nb_neurones_total, nb_neurones_layer):\n",
    "        super(Network_2n_1, self).__init__()\n",
    "\n",
    "        self.nb_layer = math.ceil(nb_neurones_total/nb_neurones_layer)\n",
    "        print(self.nb_layer * nb_neurones_layer)\n",
    "        self.nb_neurones_total = nb_neurones_total\n",
    "        self.nb_neurones_layer = nb_neurones_layer\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm1d(nInputs)#, eps=1e-05, momentum=0.1, affine=False)\n",
    "        \n",
    "            \n",
    "        self.first_linear = torch.nn.Linear(nInputs, self.nb_neurones_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for layer in range(self.nb_layer):\n",
    "            setattr(self, \"linear\" + str(layer), torch.nn.Linear(self.nb_neurones_layer, self.nb_neurones_layer))\n",
    "            #torch.nn.Dropout(p=0.5, inplace=False))\n",
    "            setattr(self, \"sigmoid\" + str(layer), nn.Sigmoid())\n",
    "        \n",
    "        self.last_linear = torch.nn.Linear(self.nb_neurones_layer, nOutputs)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.first_linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        for layer in range(self.nb_layer):\n",
    "            x = getattr(self, \"linear\" + str(layer))(x)\n",
    "            x = getattr(self, \"sigmoid\" + str(layer))(x)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "network = Network_2n_1(nInputs = 36,nOutputs = 6, nb_neurones_total=511, nb_neurones_layer=72+1)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (batch1): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fully_Connected1): Linear(in_features=36, out_features=50, bias=True)\n",
      "  (dropout1): Dropout(p=0.5)\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (fully_Connected2): Linear(in_features=50, out_features=30, bias=True)\n",
      "  (dropout2): Dropout(p=0.5)\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (fully_Connected3): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (dropout3): Dropout(p=0.5)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fully_Connected4): Linear(in_features=10, out_features=7, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features for the input\n",
    "    (2) = nOutput : number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.batch1 = nn.BatchNorm1d(nInputs)#, eps=1e-05, momentum=0.1, affine=False)\n",
    "\n",
    "        self.fully_Connected1 = torch.nn.Linear(nInputs, 50)        \n",
    "        self.dropout1 = torch.nn.Dropout(p=0.5)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        \n",
    "        self.fully_Connected2 = torch.nn.Linear(50, 30)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.5)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        \n",
    "        self.fully_Connected3 = torch.nn.Linear(30, 10)\n",
    "        self.dropout3 = torch.nn.Dropout(p=0.5)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        self.fully_Connected4 = torch.nn.Linear(10, nOutputs)\n",
    "        #self.dropout4 = torch.nn.Dropout(p=0.5)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.fully_Connected1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        \n",
    "        x = self.fully_Connected2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        \n",
    "        x = self.fully_Connected3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fully_Connected4(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "network = Network(nInputs = 36,nOutputs = 7)\n",
    "print(network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(t):\n",
    "    return(t**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    4,  1156])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.tensor([2,34])\n",
    "F(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    4,  1156])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def method2(methodToRun,t):\n",
    "    result = methodToRun(t)\n",
    "    return result\n",
    "\n",
    "method2(F,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_Lovasz_extend(y_estimated, y, dimension, F_delta=None, f_delta=None, softmax=False):\n",
    "    \"\"\"\n",
    "    :param y_estimated: result of train(x) which is the forward action\n",
    "    :param y: Label associated with x\n",
    "    :param dimension: number of classes to predict\n",
    "    :return: difference between y_estimated and y, according to the approximation of IoU : Lovasz function\n",
    "    \"\"\"\n",
    "    # Apply softmax the log on the result\n",
    "    y_estimated = F.softmax(input=y_estimated, dim=1)\n",
    "\n",
    "    IoU_loss = 0\n",
    "\n",
    "    # We will divide by the the number of classes present in the dataset.\n",
    "    number_class_used = 0\n",
    "    # Compute the IoU for each class\n",
    "    for k in range(dimension):\n",
    "\n",
    "        # Keep only the classes k.\n",
    "        y_only_k = (y == k).float()\n",
    "\n",
    "        # If this class is not present we don t use it (it avoid division by 0 and speed up the computation)\n",
    "        if not (torch.sum(y_only_k).data.numpy == 0):\n",
    "            number_class_used += 1\n",
    "\n",
    "            # Sort y_estimated so that the highest probability is first\n",
    "            y_estimated_k, y_order = torch.sort(y_estimated[:, k],\n",
    "                                                dim=0,\n",
    "                                                descending=True)\n",
    "\n",
    "            y_only_k = y_only_k[y_order, ]\n",
    "\n",
    "            # Compute the intersection and union according to the algorithm :\n",
    "            # The Lovasz-Softmax loss: A tractable surrogate for the optimization of the\n",
    "            # intersection-over-union measure in neural networks\n",
    "            inter = torch.cumsum(y_only_k, dim=0)\n",
    "            union = torch.sum(y_only_k) + torch.cumsum(1 - y_only_k, dim=0)\n",
    "            grad = - (inter / union)\n",
    "\n",
    "            # We fixe the IoU of the empty set = 0\n",
    "            IoU_loss += y_estimated_k[0] * (grad[0] - 0)\n",
    "            # Again follow the algorithm given in the paper to understand this line.\n",
    "            if F is not None:\n",
    "                IoU_loss += torch.sum(F(y_estimated_k[1:]) * (grad[1:] - grad[:-1]))\n",
    "            \n",
    "        else:\n",
    "            print(\"There is no such class : \", k, \" in this image\")\n",
    "\n",
    "    # Divide by the number of class used to normalize\n",
    "    return 1 + (IoU_loss / number_class_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_Lovasz(y_estimated, y, dimension, global_IoU_modif=False):\n",
    "    \"\"\"\n",
    "    :param y_estimated: result of train(x) which is the forward action\n",
    "    :param y: Label associated with x\n",
    "    :param parameters: List of parameters of the network\n",
    "    :param mask: Image with 1 were there is a class to predict, and 0 if not.\n",
    "    :return: difference between y_estimated and y, according to the approximation of IoU : Lovasz function\n",
    "    \"\"\"\n",
    "    # Apply softmax the log on the result\n",
    "    y_estimated = F.softmax(input=y_estimated, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    IoU_loss = 0\n",
    "\n",
    "    if global_IoU_modif:\n",
    "\n",
    "        # Ugly hack that check if we are at the first epoch and first iteration of batch\n",
    "        if torch.sum(parameters.inter_union.cpu().data != torch.zeros((2, parameters.number_classes))) == 0:\n",
    "            momentum = 0\n",
    "        else:\n",
    "            momentum = parameters.momentum_IoU\n",
    "\n",
    "    else:\n",
    "        # Permute the prediction and the ground truth so that there is only 1 dimension for the groudn truth\n",
    "        # and parameters.number_class for the estimated\n",
    "        #y_estimated = y_estimated.permute(0, 2, 3, 1).contiguous()\n",
    "        #y_estimated = y_estimated.view(-1, parameters.number_classes)\n",
    "        #y = y.contiguous().view(-1)\n",
    "\n",
    "        # We will divide by the the number of classes present in the dataset.\n",
    "        number_class_used = 0\n",
    "        # Compute the IoU for each class\n",
    "        for k in range(dimension):\n",
    "\n",
    "            # Keep only the classes k.\n",
    "            y_only_k = (y == k).float()\n",
    "\n",
    "            # If this class is not present we don t use it (it avoid division by 0 and speed up the computation)\n",
    "            if not (torch.sum(y_only_k).data.numpy == 0):\n",
    "                number_class_used += 1\n",
    "\n",
    "                # Sort y_estimated so that the highest probability is first\n",
    "                y_estimated_k, y_order = torch.sort(y_estimated[:, k],\n",
    "                                                    dim=0,\n",
    "                                                    descending=True)\n",
    "\n",
    "                y_only_k = y_only_k[y_order, ]\n",
    "                #print(\"y_only_k\", y_only_k)\n",
    "                #print(\"y_estimated_k\", y_estimated_k)\n",
    "\n",
    "                # Compute the intersection and union according to the algorithm :\n",
    "                # The Lovasz-Softmax loss: A tractable surrogate for the optimization of the\n",
    "                # intersection-over-union measure in neural networks\n",
    "                inter = torch.cumsum(y_only_k, dim=0)\n",
    "                union = torch.sum(y_only_k) + torch.cumsum(1 - y_only_k, dim=0)\n",
    "                grad = - (inter / union)\n",
    "                #print(inter)\n",
    "                #print(union)\n",
    "                #print(grad)\n",
    "\n",
    "                # We fixe the IoU of the empty set = 0\n",
    "                IoU_loss += y_estimated_k[0] * (grad[0] + 0)\n",
    "                # Again follow the algorithm given in the paper to understand this line.\n",
    "                #print(\"y_estimated_k[1:]\",y_estimated_k[1:])\n",
    "                #print(\"grad[1:]\", grad[1:])\n",
    "                #print(\"grad[:-1]\", grad[:-1])\n",
    "                IoU_loss += torch.sum(y_estimated_k[1:] * (grad[1:] - grad[:-1]))\n",
    "                #return\n",
    "            else:\n",
    "                print(\"There is no such class : \", k, \" in this image\")\n",
    "\n",
    "    # Divide by the number of class used to normalize\n",
    "    return 1 + (IoU_loss / number_class_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We don t want to delete the files that already exist \n",
      "\n",
      "y_only_k tensor([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.])\n",
      "y_estimated_k tensor([ 0.1583,  0.1576,  0.1573,  0.1569,  0.1568,  0.1567,  0.1565,\n",
      "         0.1558,  0.1555,  0.1553,  0.1552,  0.1550,  0.1548,  0.1545,\n",
      "         0.1545,  0.1541,  0.1533,  0.1530,  0.1530,  0.1507])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.])\n",
      "tensor([  3.,   4.,   5.,   6.,   7.,   7.,   8.,   9.,  10.,  11.,\n",
      "         12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  20.])\n",
      "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.1429, -0.1250,\n",
      "        -0.1111, -0.1000, -0.0909, -0.0833, -0.0769, -0.0714, -0.0667,\n",
      "        -0.0625, -0.0588, -0.0556, -0.0526, -0.0500, -0.1000])\n",
      "y_estimated_k[1:] tensor([ 0.1576,  0.1573,  0.1569,  0.1568,  0.1567,  0.1565,  0.1558,\n",
      "         0.1555,  0.1553,  0.1552,  0.1550,  0.1548,  0.1545,  0.1545,\n",
      "         0.1541,  0.1533,  0.1530,  0.1530,  0.1507])\n",
      "grad[1:] tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.1429, -0.1250, -0.1111,\n",
      "        -0.1000, -0.0909, -0.0833, -0.0769, -0.0714, -0.0667, -0.0625,\n",
      "        -0.0588, -0.0556, -0.0526, -0.0500, -0.1000])\n",
      "grad[:-1] tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.1429, -0.1250,\n",
      "        -0.1111, -0.1000, -0.0909, -0.0833, -0.0769, -0.0714, -0.0667,\n",
      "        -0.0625, -0.0588, -0.0556, -0.0526, -0.0500])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-77c1b4fcaf7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0mname_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"IoU_Lovasz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m      name_network=\"test\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-8eaac35c9e67>\u001b[0m in \u001b[0;36mMain\u001b[0;34m(batch_size, nb, nb_epoch, dimension, name_loss, name_network, nb_neurones_layer, nb_neurones_total)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m#Compute the backward function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Does the update according to the optimizer define above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "nb_neurones_layer=73\n",
    "nb_neurones_total=219\n",
    "Main(batch_size = 20,\n",
    "     nb_epoch = 204,\n",
    "     dimension=6,\n",
    "     name_loss=\"IoU_Lovasz\",\n",
    "     name_network=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the loss function between the y_train_estimated and y_train\n",
    "    (0) = y_estimated : result of train(x) which is the forward action\n",
    "    (1) = y : Label associated with x\n",
    "\"\"\"\n",
    "def cross_entropy_loss(y_estimated, y, dimension):\n",
    "\n",
    "    # http://pytorch.org/docs/master/nn.html : torch.nn.NLLLoss\n",
    "    nllcrit = nn.NLLLoss(reduce=True, weight=torch.tensor([1. for i in range(dimension)]))\n",
    "\n",
    "    # Apply softmax then the log on the result\n",
    "    y_estimated = F.log_softmax(y_estimated, dim = 1)\n",
    "\n",
    "    # Apply the criterion define in the first line\n",
    "    return nllcrit(y_estimated, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_loss(y_estimated, y, dimension, global_IoU_modif=False):\n",
    "    \"\"\"\n",
    "    :param y_estimated: result of train(x) which is the forward action\n",
    "    :param y: Label associated with x\n",
    "    :param parameters: List of parameters of the network\n",
    "    :param mask: Image with 1 were there is a class to predict, and 0 if not.\n",
    "    :return: difference between y_estimated and y, according to the continuous IoU loss\n",
    "    \"\"\"\n",
    "    # Apply softmax the log on the result\n",
    "    y_estimated = F.softmax(input=y_estimated, dim=1)\n",
    "\n",
    "    IoU = 0\n",
    "\n",
    "    if global_IoU_modif:\n",
    "\n",
    "        # Ugly hack that check if we are at the first epoch and first iteration of batch\n",
    "        if torch.sum(parameters.inter_union.cpu().data != torch.zeros((2, parameters.number_classes))) == 0:\n",
    "            momentum = 0\n",
    "        else:\n",
    "            momentum = parameters.momentum_IoU\n",
    "\n",
    "        # Compute the IoU per classes\n",
    "        for k in range(parameters.number_classes):\n",
    "            # Keep only the classes k.\n",
    "            y_only_k = (y == k).float()\n",
    "\n",
    "            # Definition of intersection and union\n",
    "            inter = momentum * parameters.inter_union[0, k] + \\\n",
    "                    (1 - momentum) * torch.sum(y_estimated[:, k] * y_only_k)\n",
    "\n",
    "            union = momentum * parameters.inter_union[1, k] + \\\n",
    "                    (1 - momentum) * torch.sum(\n",
    "                        y_only_k + y_estimated[:, k] - y_estimated[:, k] * y_only_k)\n",
    "\n",
    "            IoU += parameters.weight_grad[k] * inter / union\n",
    "\n",
    "            parameters.inter_union[0, k] = inter.data\n",
    "            parameters.inter_union[1, k] = union.data\n",
    "    else:\n",
    "        # Compute the IoU per classes\n",
    "        for k in range(dimension):\n",
    "            # Keep only the classes k.\n",
    "            y_only_k = (y == k).float()\n",
    "\n",
    "            # Definition of intersection and union\n",
    "            inter = torch.sum(y_estimated[:, k] * y_only_k)\n",
    "\n",
    "            union = torch.sum(y_only_k + y_estimated[:, k] - y_estimated[:, k] * y_only_k)\n",
    "\n",
    "            IoU += inter / union\n",
    "\n",
    "    # Divide by the number of class to have IoU between 0 and 1. we add \"1 -\" to have a loss to minimize and\n",
    "    # to stay between 0 and 1.\n",
    "    return 1 - (IoU / dimension)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def criterion(y_estimated, y, name_loss, global_IoU_modif=False, dimension=2):\n",
    "    \"\"\"\n",
    "    :param y_estimated: result of train(x) which is the forward action\n",
    "    :param y: Label associated with x\n",
    "    :param inter_union: List of the sum of Intersection and Union for each classes\n",
    "    :param parameters: List of parameters of the network\n",
    "    :return: difference between y_estimated and y, according to some function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if name_loss == \"focal_loss\":\n",
    "        return focal_loss(y_estimated=y_estimated,\n",
    "                                  y=y,\n",
    "                                  parameters=parameters,\n",
    "                                  mask=mask,\n",
    "                                  number_of_used_pixel=number_of_used_pixel)\n",
    "\n",
    "\n",
    "    if name_loss == \"cross_entropy\":\n",
    "        return cross_entropy_loss(y_estimated=y_estimated,\n",
    "                                  y=y,\n",
    "                                  dimension=dimension)\n",
    "\n",
    "    if name_loss == \"IoU\":\n",
    "        return IoU_loss(y_estimated=y_estimated,\n",
    "                        y=y,\n",
    "                        dimension=dimension)\n",
    "\n",
    "    if name_loss == \"hinge\":\n",
    "        return hinge_multidimensional_loss(y_estimated=y_estimated,\n",
    "                                           y=y,\n",
    "                                           parameters=parameters,\n",
    "                                           mask=mask)\n",
    "\n",
    "    if name_loss == \"IoU_Lovasz\":\n",
    "        return IoU_Lovasz(y_estimated=y_estimated,\n",
    "                          y=y,\n",
    "                          dimension=dimension)\n",
    "\n",
    "    if name_loss == \"cross_entropy_to_IoU\":\n",
    "        if parameters.actual_epoch < 470:\n",
    "            return cross_entropy_loss(y_estimated=y_estimated,\n",
    "                                      y=y,\n",
    "                                      parameters=parameters,\n",
    "                                      mask=mask,\n",
    "                                      number_of_used_pixel=number_of_used_pixel)\n",
    "        elif parameters.actual_epoch > 670:\n",
    "            return IoU_loss(y_estimated=y_estimated,\n",
    "                            y=y,\n",
    "                            parameters=parameters,\n",
    "                            mask=mask)\n",
    "        else:\n",
    "            balance_between_loss = sigmoid(\n",
    "                (parameters.actual_epoch - 570) * (15 / 200))\n",
    "\n",
    "            return (1 - balance_between_loss) * cross_entropy_loss(y_estimated=y_estimated,\n",
    "                                                                   y=y,\n",
    "                                                                   parameters=parameters,\n",
    "                                                                   mask=mask,\n",
    "                                                                   number_of_used_pixel=number_of_used_pixel) + \\\n",
    "                   balance_between_loss * IoU_loss(y_estimated=y_estimated,\n",
    "                                                   y=y,\n",
    "                                                   parameters=parameters,\n",
    "                                                   mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def criterion_pd_format(y_estimated, y, name_loss, epoch, set_type, dimension=2):\n",
    "    \"\"\"\n",
    "    :param y_estimated: result of train(x) which is the forward action\n",
    "    :param y: Label associated with x\n",
    "    :param epoch: The actual epoch of the learning\n",
    "    :param set_type: String which is train of validation\n",
    "    :param parameters: List of parameters of the network\n",
    "    :return: Define the loss function between the y_estimated and y and return a vector with the epoch the value\n",
    "    and if the criterion is used on training or validation set.\n",
    "    \"\"\"\n",
    "    loss = criterion(y_estimated=y_estimated,\n",
    "                     y=y,\n",
    "                     name_loss=name_loss,\n",
    "                     dimension=dimension)\n",
    "    # Return a vector  usefull to copy to CSV \n",
    "    return [set_type, epoch, loss.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test function : IoU_pd_format\n",
    "    Return a matrix of confusion in a good format to creat a pandas DataFrame\n",
    "    (0) = y_estimated : result of train(x) which is the forward action\n",
    "    (1) = y : Label associated with x\n",
    "    (2) = epoch : the actual epoch of the learning\n",
    "    (3) = set_type : string which is train of validation\n",
    "\"\"\"\n",
    "def IoU_pd_format(y_estimated, y, set_type, epoch, dimension):\n",
    "    \n",
    "    #We keep only the higest value, which is the prediction\n",
    "    pred = torch.max(y_estimated, dim=1)[1]\n",
    "\n",
    "    #Creat the confusion matrix, only the second column will have the value\n",
    "    confusion_matrix = [[0] * 5 for i in range(dimension*dimension)]\n",
    "    pred = pred.view(-1)\n",
    "    target = y.view(-1)\n",
    "\n",
    "    # We will normalise the value in the matrix. We don t want to be influence by the size of the batch\n",
    "    normalisation_value = target.size(0)\n",
    "    #print(normalisation_value)\n",
    "    #print(normalisation_value.item)\n",
    "\n",
    "\n",
    "    # Double loop over the number of classes at each iteration we add the intersection\n",
    "    # i will be the number of iteration\n",
    "    i = 0\n",
    "    for cls1 in range(dimension):\n",
    "        pred_inds = pred == cls1\n",
    "        \n",
    "        for cls2 in range(dimension):\n",
    "            \n",
    "            i = cls1*dimension + cls2\n",
    "            target_inds = target == cls2\n",
    "            # Intersection is the value predicted of class cls2 and are in reality class cls1\n",
    "            intersection = (pred_inds*target_inds).long().sum().data.cpu()\n",
    "            # Associated with this value we keep the two classes\n",
    "            confusion_matrix[i][0] = \"class\" + str(cls1)\n",
    "            confusion_matrix[i][1] = \"class\" + str(cls2)\n",
    "            # The epoch associated\n",
    "            confusion_matrix[i][2] = epoch\n",
    "            # Traning or validation set\n",
    "            confusion_matrix[i][3] = set_type\n",
    "            # The value normalised\n",
    "            confusion_matrix[i][4] = intersection.item()/normalisation_value\n",
    "\n",
    "    #print(confusion_matrix)\n",
    "    return(confusion_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_csv(path_CSV, name_network):\n",
    "    \"\"\"\n",
    "    :param path_CSV: path to store the CSV files\n",
    "    :param name_network: name of the network associated with the CSV files\n",
    "    :return: Nothing but create two CSV files in which the data of the training/validation will be store.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stop the program if the CSV file already exist\n",
    "    if exists(path_CSV + \"CSV_confMat_\" + name_network  + \".csv\"):\n",
    "        print(\"We don t want to delete the files that already exist \\n\")\n",
    "            # raise RuntimeError('We don t want to delete the files that already exist')\n",
    "            # TODO ici il faudra faire arreter le programme !\n",
    "\n",
    "    # header of the futur Pandas DataFrames\n",
    "    header_confMat = ['Target', 'Prediction', 'Epoch', 'Set', 'Value']\n",
    "    header_loss = [\"Set\", \"Epoch\", \"Value\"]\n",
    "\n",
    "    # Try to open the file and write the header\n",
    "    with open(path_CSV + \"CSV_confMat_\" + name_network + \".csv\", 'w') as csvfile:\n",
    "        cwriter = csv.writer(csvfile,\n",
    "                             delimiter=',',\n",
    "                             quoting=csv.QUOTE_MINIMAL)\n",
    "        cwriter.writerow(header_confMat)\n",
    "\n",
    "    # Try to open the file and write the header\n",
    "    with open(path_CSV + \"CSV_loss_\" + name_network + \".csv\", 'w') as csvfile:\n",
    "        cwriter = csv.writer(csvfile,\n",
    "                             delimiter=',',\n",
    "                             quoting=csv.QUOTE_MINIMAL)\n",
    "        cwriter.writerow(header_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_error(x, y, network, epoch, set_type, dimension, name_loss, name_network, loss=None, y_estimated=None):\n",
    "    \"\"\"\n",
    "    :param x: Input data of validation or training set\n",
    "    :param y: Output data expected of validation or training set\n",
    "    :param network: network that will be used to compute the y_estimated\n",
    "    :param epoch: Actual epoch of the training\n",
    "    :param set_type: Validation or train DataSet\n",
    "    :param dimension: Number of classes of the Dataset\n",
    "    :param loss: Value of loss if we are on the train set\n",
    "    :param y_estimated: Value of y_predicted by the network\n",
    "    :return: the loss and save the error in CSV : Confusion matrix and loss for the set_type\n",
    "    \"\"\"\n",
    "    if y_estimated is None:\n",
    "        # Result of the network\n",
    "        y_estimated = network(x)\n",
    "\n",
    "    # Compare the real result and the one of the network\n",
    "    conf_mat = IoU_pd_format(y_estimated=y_estimated,\n",
    "                             y=y,\n",
    "                             set_type=set_type,\n",
    "                             epoch=epoch,\n",
    "                             dimension=dimension)\n",
    "    # Store the IoU confusion matrix into a CSV file\n",
    "    with open(\"./satimage/CSV_confMat_\" + name_network +\".csv\", 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerows(conf_mat)\n",
    "\n",
    "    # If we don't compute the loss before (we are on validation set). We compute it.\n",
    "    if loss is None:\n",
    "        # Same with the loss\n",
    "        loss = criterion_pd_format(y_estimated=y_estimated,\n",
    "                                   y=y,\n",
    "                                   epoch=epoch,\n",
    "                                   set_type=set_type,\n",
    "                                   name_loss=name_loss,\n",
    "                                   dimension=dimension)\n",
    "        \n",
    "    # If the loss was already compute, we transform it to a better format\n",
    "    else:\n",
    "        loss = [set_type, epoch, loss.item()]\n",
    "    # Save the loss\n",
    "    with open(\"./satimage/CSV_loss_\" + name_network + \".csv\", 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerows([loss])\n",
    "\n",
    "    return loss[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Main(batch_size=200, nb=4, nb_epoch=1, dimension=2, name_loss=\"cross_entropy\", name_network=\"test\",\n",
    "         nb_neurones_layer=None, nb_neurones_total=None):\n",
    "    \n",
    "    torch.manual_seed(5718) \n",
    "    np.random.seed(114685)\n",
    "    \n",
    "    init_csv(path_CSV=\"./satimage/\", name_network=name_network)\n",
    "    x, y = loadSatimage()\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "    \n",
    "    permutation = np.random.permutation(np.arange(x.size(0)))\n",
    "    percentage = 0.7\n",
    "    x_train, x_val = x[permutation[:int(percentage*x.size(0))],:], x[permutation[int(percentage*x.size(0)):],:]\n",
    "    y_train, y_val = y[permutation[:int(percentage*x.size(0))]], y[permutation[int(percentage*x.size(0)):]]\n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    \n",
    "    if nb_neurones_layer is None:\n",
    "        network = Network(nInputs = 36, nOutputs = dimension)\n",
    "    else:\n",
    "        network = Network_2n_1(nInputs = 36,\n",
    "                               nOutputs = dimension,\n",
    "                               nb_neurones_layer=nb_neurones_layer,\n",
    "                               nb_neurones_total=nb_neurones_total)\n",
    "\n",
    "    optimizer = optim.Adam(params = network.parameters(), lr = 0.005,\n",
    "                           betas = (0.9, 0.999),\n",
    "                           eps = 1*10**(-8), weight_decay = 0)\n",
    "\n",
    "    for epoch in range(nb_epoch):\n",
    "        mean_loss_val = 0\n",
    "        mean_loss_train = 0\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            # zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            #Transform into Variable\n",
    "            #x_batch = Variable(x_batch).float()\n",
    "            #y_batch = y_batch.type(torch.LongTensor)\n",
    "\n",
    "            # Compute the forward function\n",
    "            y_batch_estimated = network(x_batch)\n",
    "            #Get the error\n",
    "            loss = criterion(y_batch_estimated, y_batch, name_loss=name_loss, dimension=dimension)\n",
    "            \n",
    "\n",
    "            #Compute the backward function\n",
    "            loss.backward()\n",
    "            \n",
    "            # Does the update according to the optimizer define above\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save the error in a CSV files\n",
    "            \"\"\"mean_loss_train += save_error(x=x_batch,\n",
    "                                          y=y_batch,\n",
    "                                          network=network,\n",
    "                                          epoch=epoch,\n",
    "                                          set_type=\"train\",\n",
    "                                          dimension=dimension,\n",
    "                                          loss=loss,\n",
    "                                          y_estimated=y_batch_estimated,\n",
    "                                          name_loss=name_loss)\"\"\"\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(val_loader):\n",
    "            x_batch = Variable(x_batch).float()\n",
    "            y_batch = y_batch.type(torch.LongTensor)\n",
    "            mean_loss_val += save_error(x=x_batch,\n",
    "                                        y=y_batch,\n",
    "                                        network=network,\n",
    "                                        epoch=epoch,\n",
    "                                        set_type=\"val\",\n",
    "                                        dimension=dimension,\n",
    "                                        name_loss=name_loss,\n",
    "                                        name_network=name_network)\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = Variable(x_batch).float()\n",
    "            y_batch = y_batch.type(torch.LongTensor)\n",
    "            mean_loss_train += save_error(x=x_batch,\n",
    "                                        y=y_batch,\n",
    "                                        network=network,\n",
    "                                        epoch=epoch,\n",
    "                                        set_type=\"train\",\n",
    "                                        dimension=dimension,\n",
    "                                        name_loss=name_loss,\n",
    "                                        name_network=name_network)\n",
    "\n",
    "        if epoch%100==0:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / 2\n",
    "            print(\"epoch=\",epoch)\n",
    "            print(\"mean_loss_train :\", mean_loss_train/(i+1))\n",
    "            print(\"mean_loss_val :\", mean_loss_val/(j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"organise_CSV import two CSV files and delete all duplicate row. Because the algorithme work with\n",
    "    mini_batch there is many value for the loss for one epoch and one data set. We compute here the mean\n",
    "    of all this loss that have the same epoch and data set. We did the same with the confusion matrix\n",
    "    (0) = name_network : name of the network associated with the CSV file\n",
    "    (1) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def organise_CSV(path_CSV, name_network, both=True):\n",
    "    # Import the CSV file into pandas DataFrame\n",
    "    loss_DF = pd.read_csv(path_CSV + \"CSV_loss_\" + name_network + \".csv\")\n",
    "    # This Groupby will regroupe all line that have the same \"Set\" and \"Epoch\" and compute the mean over the \"Values\"\n",
    "    loss_DF = loss_DF.groupby(['Set','Epoch'])['Value'].mean().reset_index()\n",
    "    # Recreate the CSV file\n",
    "    loss_DF.to_csv(path_CSV + \"CSV_loss_\" + name_network + \".csv\",index = False)\n",
    "    \n",
    "    if both:\n",
    "        # Import the CSV file into pandas DataFrame\n",
    "        conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + \".csv\")\n",
    "        # This Groupby will regroupe all line that have the same 'Target','Prediction','Epoch','Set'\n",
    "        # and compute the mean over the \"Values\"\n",
    "        conf_DF = conf_DF.groupby(['Set','Target','Prediction','Epoch'])['Value'].mean().reset_index()\n",
    "        # Recreate the CSV file\n",
    "        conf_DF.to_csv(path_CSV + \"CSV_confMat_\" + name_network + \".csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_loss will plot the loss against the epoch\n",
    "    (0) = name_network : name of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def plot_loss(path_CSV, name_network):\n",
    "    # Import the CSV file into pandas DataFrame\n",
    "    loss_DF = pd.read_csv(path_CSV + \"CSV_loss_\" + name_network + \".csv\")\n",
    "    \n",
    "    # Reset sns parameter to default\n",
    "    sns.set()\n",
    "    # Set some parameter to have better plot\n",
    "    params = {'legend.fontsize': 'x-large',\n",
    "              'figure.figsize': (16, 10),\n",
    "             'axes.labelsize': 'x-large',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'xtick.labelsize':'x-large',\n",
    "             'ytick.labelsize':'x-large'}\n",
    "    pylab.rcParams.update(params)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.tsplot(time=\"Epoch\", value=\"Value\", data=loss_DF, unit=\"Set\",condition=\"Set\")\n",
    "    plt.xlabel(r'Value')\n",
    "    plt.ylabel(r'Loss')\n",
    "    plt.title(\"Loss for each epoch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_IuO will plot the loss against the epoch\n",
    "    (0) = path_CSV : path of the CSV file\n",
    "    (1) = name_network : name of the network associated with the CSV file\n",
    "    (2) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def plot_IuO(path_CSV, name_network):\n",
    "    \n",
    "    # Import the CSV of the conv matrix\n",
    "    conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + \".csv\")\n",
    "    \n",
    "    # In the column \"Value\" there is the TP information\n",
    "    conf_DF_TP = conf_DF[conf_DF[\"Prediction\"] == conf_DF[\"Target\"]]\n",
    "\n",
    "    # In the column \"Value\" there is the FN information\n",
    "    conf_DF_FN = conf_DF.groupby([\"Set\",\"Epoch\",\"Target\"])[\"Value\"].sum().reset_index()\n",
    "    # In the column \"Value\" there is the FP information\n",
    "    conf_DF_FP = conf_DF.groupby([\"Set\",\"Epoch\",\"Prediction\"])[\"Value\"].sum().reset_index()\n",
    "\n",
    "    # Change the name\n",
    "    conf_DF_FN.rename(columns={\"Value\": \"FN\"}, inplace=True)\n",
    "    conf_DF_FP.rename(columns={\"Value\": \"FP\"}, inplace=True)\n",
    "\n",
    "    # Merge the dataset together according to certain column\n",
    "    conf_DF_TP_FN = conf_DF_TP.merge(conf_DF_FN, on=[\"Epoch\",\"Set\",\"Target\"])\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN.merge(conf_DF_FP, on=[\"Epoch\",\"Set\",\"Prediction\"])\n",
    "    \n",
    "    # We compute the realFP and FN value, we have to substract the TP values\n",
    "    conf_DF_TP_FN_FP[\"FP\"] = conf_DF_TP_FN_FP[\"FP\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    conf_DF_TP_FN_FP[\"FN\"] = conf_DF_TP_FN_FP[\"FN\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    #print(conf_DF_TP_FN_FP)    \n",
    "    # Compute the IoU each class\n",
    "    conf_DF_TP_FN_FP[\"IoU\"] = conf_DF_TP_FN_FP[\"Value\"]/(conf_DF_TP_FN_FP[\"Value\"] + \n",
    "                                                         conf_DF_TP_FN_FP[\"FP\"] + conf_DF_TP_FN_FP[\"FN\"])\n",
    "\n",
    "    # Round the Epoch value to have a value  every 10 Epoch. This line could be delete if needed\n",
    "    #conf_DF_TP_FN_FP[\"Epoch\"] = round(conf_DF_TP_FN_FP[\"Epoch\"],-1)\n",
    "    \n",
    "    # Regroupe each line that have the same \"Set\",\"Epoch\",\"Real_name\" and compute the mean fot the IoU\n",
    "    #print(conf_DF_TP_FN_FP)\n",
    "    plot_DF = conf_DF_TP_FN_FP.groupby([\"Set\",\"Epoch\"])[\"IoU\"].mean().reset_index()\n",
    "    #print(plot_DF)\n",
    "    # Start the plot part\n",
    "    \n",
    "    # Reset sns parameter to default\n",
    "    sns.set()\n",
    "    # Set some parameter to have better plot\n",
    "    params = {'legend.fontsize': 'x-large',\n",
    "              'figure.figsize': (16, 10),\n",
    "             'axes.labelsize': 'x-large',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'xtick.labelsize':'x-large',\n",
    "             'ytick.labelsize':'x-large'}\n",
    "    pylab.rcParams.update(params)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    # Epoch against IoU. A line for each Set\n",
    "    sns.tsplot(time=\"Epoch\", value=\"IoU\", data=plot_DF, unit=\"Set\",condition=\"Set\")\n",
    "    # Define the x axis name and fontsize\n",
    "    plt.xlabel(r'Epoch')\n",
    "    # Define the y axis name and fontsize\n",
    "    plt.ylabel(r'IoU mean')\n",
    "    plt.title(\"IoU mean for each epoch\")\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plot_IuO will plot the loss against the epoch\n",
    "    (other) = labels : All the labels. In 19 different classes to have an user-friendly interact interface.\n",
    "    If one of this label is True then the line associated is plot.\n",
    "    (-3) = path_CSV : path of the CSV file\n",
    "    (-2) = name_network : name of the network associated with the CSV file\n",
    "    (-1) = train_number : number of the network associated with the CSV file\n",
    "\"\"\"\n",
    "def plot_IuO_modif(path_CSV,\n",
    "                   name_network,\n",
    "                   dimension):\n",
    "    \n",
    "    # List of all the label that will be used \n",
    "    list_label = [i for i in range(dimension)]\n",
    "    \n",
    "    # Import the CSV of the conv matrix\n",
    "    conf_DF = pd.read_csv(path_CSV + \"CSV_confMat_\" + name_network + \".csv\")\n",
    "    \n",
    "    # In the column \"Value\" there is the TP information\n",
    "    conf_DF_TP = conf_DF[conf_DF[\"Prediction\"] == conf_DF[\"Target\"]]\n",
    "\n",
    "    # In the column \"Value\" there is the FN information\n",
    "    conf_DF_FN = conf_DF.groupby([\"Set\",\"Epoch\",\"Target\"])[\"Value\"].sum().reset_index()\n",
    "    # In the column \"Value\" there is the FP information\n",
    "    conf_DF_FP = conf_DF.groupby([\"Set\",\"Epoch\",\"Prediction\"])[\"Value\"].sum().reset_index()\n",
    "\n",
    "    # Change the name\n",
    "    conf_DF_FN.rename(columns={\"Value\": \"FN\"}, inplace=True)\n",
    "    conf_DF_FP.rename(columns={\"Value\": \"FP\"}, inplace=True)\n",
    "\n",
    "    # Merge the dataset together according to certain column\n",
    "    conf_DF_TP_FN = conf_DF_TP.merge(conf_DF_FN, on=[\"Epoch\",\"Set\",\"Target\"])\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN.merge(conf_DF_FP, on=[\"Epoch\",\"Set\",\"Prediction\"])\n",
    "    \n",
    "    # We compute the realFP and FN value, we have to substract the TP values\n",
    "    conf_DF_TP_FN_FP[\"FP\"] = conf_DF_TP_FN_FP[\"FP\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    conf_DF_TP_FN_FP[\"FN\"] = conf_DF_TP_FN_FP[\"FN\"] - conf_DF_TP_FN_FP[\"Value\"]\n",
    "    \n",
    "    # Compute the IoU each class\n",
    "    conf_DF_TP_FN_FP[\"IoU\"] = conf_DF_TP_FN_FP[\"Value\"]/(conf_DF_TP_FN_FP[\"Value\"] + \n",
    "                                                         conf_DF_TP_FN_FP[\"FP\"] + conf_DF_TP_FN_FP[\"FN\"])\n",
    "    \n",
    "    #Change change the name of class for the plot\n",
    "    label_DF = pd.DataFrame(data={'Real_name': [str(i) for i in range(dimension)],\n",
    "                                  'Class_name': [\"class\" + str(i) for i in range(dimension)]})\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN_FP.merge(label_DF, left_on='Target', right_on='Class_name')\n",
    "    \n",
    "    # Round the Epoch value to have a value  every 10 Epoch. This line could be delete if needed\n",
    "    conf_DF_TP_FN_FP[\"Epoch\"] = round(conf_DF_TP_FN_FP[\"Epoch\"],-1)\n",
    "    \n",
    "    # Regroupe each line that have the same \"Set\",\"Epoch\",\"Real_name\" and compute the mean fot the IoU\n",
    "    #print(conf_DF_TP_FN_FP)\n",
    "    conf_DF_TP_FN_FP = conf_DF_TP_FN_FP.groupby([\"Set\",\"Epoch\",\"Real_name\"])[\"IoU\"].mean().reset_index()\n",
    "    \n",
    "    # A list of color associated with the labels\n",
    "    flatui = [\"#804080\", \"#F423E8\", \"#464646\", \"#66669C\", \"#BE9999\", \"#999999\",\n",
    "             \"#FAAA1E\", \"#DCDC00\", \"#6B8E23\", \"#98FB98\", \"#4682B4\", \"#DC143C\",\n",
    "             \"#FF0000\", \"#00008E\", \"#000046\", \"#003C64\", \"#005064\", \"#0000E6\",\n",
    "             \"#770B20\"][0:dimension]\n",
    "    \n",
    "    # List of labels\n",
    "    label_name = [str(i) for i in range(dimension)]\n",
    "    \n",
    "    # frames is a list a dataframe that will be concatenate at the end of the loop\n",
    "    frames= []\n",
    "    # Only keep the color needed\n",
    "    flatui_needed = []\n",
    "    # The loop over the label look if eachlabel is set to True or False and add the information if Label si True\n",
    "    #print(conf_DF_TP_FN_FP)\n",
    "    #print(label_name)\n",
    "    for i,label in enumerate(list_label):\n",
    "        frames.append(conf_DF_TP_FN_FP[conf_DF_TP_FN_FP[\"Real_name\"]==label_name[i]])\n",
    "        flatui_needed.append(flatui[i])\n",
    "    \n",
    "    # Concat all the needed DataFrames\n",
    "    conf_DF_TP_FN_FP = pd.concat(frames)\n",
    "\n",
    "    # creat 2 areas where the plot will be display, one for each \"Set\" (most of the time Training and Validation)\n",
    "    g = sns.FacetGrid(conf_DF_TP_FN_FP, row=\"Set\", size=9)\n",
    "    \n",
    "    # IoU against Epoch and a different line for each Real_name. We also add the color and the titles\n",
    "    g = (g.map_dataframe(sns.tsplot,\n",
    "                         data=conf_DF_TP_FN_FP,time=\"Epoch\",\n",
    "                         value=\"IoU\",\n",
    "                         unit=\"Real_name\", condition=\"Real_name\",\n",
    "                         color = sns.color_palette(flatui_needed))).set_titles(\"IoU per classes for both training\")\n",
    "    \n",
    "    # Define the x axis name and fontsize\n",
    "    plt.xlabel(r'Epoch',fontsize = 20)\n",
    "    # Define the y axis name and fontsize\n",
    "    plt.ylabel(r'IoU',fontsize = 20)\n",
    "    # Add a legend \n",
    "    plt.legend(bbox_to_anchor=(1.2, 1.75), loc=2, borderaxespad=0.,prop={'size': 20})\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_CSV = \"./satimage/\"\n",
    "#[\"IoU_Lovasz300_1000_6\", \"test\",\"cross_entropy300_1000_6\",\"IoU300_1000_6\",\n",
    "name_network = [\"cross_entropy300_204_2\",\"IoU300_204_2\",\"IoU_Lovasz300_204_2\",\n",
    "                \"cross_entropy_2n_1109_218\", \"cross_entropy_2n73_219\", \"cross_entropy_2n_150_219\", \"cross_entropy_2n_1\",\n",
    "                \"IoU_Lovasz300_3004_6\",\"cross_entropy300_1004_6\",\"IoU300_3004_6\"]\n",
    "dimension=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_entropy300_204_2\n",
      "IoU300_204_2\n",
      "IoU_Lovasz300_204_2\n",
      "cross_entropy_2n_1109_218\n",
      "cross_entropy_2n73_219\n",
      "cross_entropy_2n_150_219\n",
      "cross_entropy_2n_1\n",
      "IoU_Lovasz300_3004_6\n",
      "cross_entropy300_1004_6\n",
      "IoU300_3004_6\n"
     ]
    }
   ],
   "source": [
    "for i in name_network:\n",
    "    print(i)\n",
    "    organise_CSV(path_CSV=path_CSV,name_network=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47b970a8d9f4166a7d769c37275b261"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_loss>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_loss,\n",
    "         path_CSV = path_CSV,\n",
    "         name_network = name_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08074372c6f54b36a573e94172bd56a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_IuO>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_IuO,\n",
    "         path_CSV = path_CSV,\n",
    "         name_network = name_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5da85102f3a400990185a9dc17a293a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_IuO_modif>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed = [True,False]\n",
    "interact(plot_IuO_modif,\n",
    "         TP = needed, TN = needed,\n",
    "         path_CSV = path_CSV,\n",
    "         name_network = name_network,\n",
    "         dimension=dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
