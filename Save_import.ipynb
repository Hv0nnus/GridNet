{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Loss and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Creat two CSV files in wich the data of the training/validation will be store.\n",
    "    (0) = path : path to store the CSV files\n",
    "    (1) = name_network : name of the network associated with the CSV files\n",
    "    (2) = train_number : number of train associated with the CSV files\n",
    "\"\"\"\n",
    "def init_csv(path_CSV,name_network,train_number):\n",
    "    \n",
    "    # Stop the program if the CSV file already exist !\n",
    "    if(exists(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\")):\n",
    "        print(\"TODO ici il faudra faire arreter le programme ! On ecrase les fichiers !\")\n",
    "        #TODO ici il faudra faire arreter le programme !\n",
    "    \n",
    "    # header of the futur Panda\"TODOs DataFrames\n",
    "    header_confMat = [\"Set\",\"Value\",\"Target\",\"Prediction\",\"Epoch\"]\n",
    "    header_loss = [\"Value\",\"Set\",\"Epoch\"]\n",
    "    \n",
    "    # Try to open the file and write the header\n",
    "    with open(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\", 'w') as csvfile:\n",
    "        cwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        cwriter.writerow(header_confMat)\n",
    "\n",
    "    # Try to open the file and write the header\n",
    "    with open(path_CSV + \"CSV_loss_\" + name_network + str(train_number) + \".csv\", 'w') as csvfile:\n",
    "        cwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        cwriter.writerow(header_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Save the error in CSV : Confusion matrix and loss for training and validation\n",
    "    (0) = x : Input data of validation or training set\n",
    "    (1) = y : Output data expected of validation or training set\n",
    "    (2) = network : network that will be used to compute the y_estimated\n",
    "    (3) = epoch : Actual epoch of the training\n",
    "    (4) = name_network : name of the network associated with the CSV files\n",
    "    (5) = train_number : number of train associated with the CSV files\n",
    "    (6) = path_CSV : path to store the CSV files\n",
    "    (7) = set_type : validation or train dataset\n",
    "\"\"\"\n",
    "def save_error(x,y,network,epoch,name_network,train_number,path_CSV,set_type):\n",
    "    # Result of the network\n",
    "    y_estimated = network(x)\n",
    "    \n",
    "    # Compare the real result and the one of the network\n",
    "    conf_mat = IoU_pd_format(y_estimated, y, set_type = set_type, epoch = epoch)\n",
    "    # Store the IoU confusion matrix into a CSV file\n",
    "    with open(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\", 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerows(conf_mat)\n",
    "\n",
    "    # Same with the loss\n",
    "    loss = criterion_pd_format(y_estimated = y_estimated,y = y,epoch=epoch,set_type = set_type)\n",
    "    with open(path_CSV + \"CSV_loss_\" + name_network + str(train_number) + \".csv\", 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerows([loss])\n",
    "    return(loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_error_copy_useless_for_the_moment(x_train,y_train,x_validation,y_validation,network,epoch,name_network,train_number,path_CSV):\n",
    "    \n",
    "    #Compute the forward function for the train and the validation\n",
    "    y_train_estimated = network(x_train)\n",
    "    y_validation_estimated = network(x_validation)\n",
    "    \n",
    "    # compute the matrice of confusion between classes\n",
    "    conf_mat_train = IoU_pd_format(y_train_estimated, y_train, set_type = \"train\", epoch = epoch)\n",
    "    conf_mat_validation = IoU_pd_format(y_validation_estimated, y_validation, set_type = \"validation\", epoch = epoch)\n",
    "    \n",
    "    with open(path_CSV + \"CSV_confMat_\" + name_network + str(train_number) + \".csv\", 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerows(conf_mat_train)\n",
    "        writer.writerows(conf_mat_validation)\n",
    "\n",
    "    loss_train = criterion_pd_format(y_estimated = y_train_estimated,y = y_train,epoch=epoch, set_type = \"train\")\n",
    "    loss_validation = criterion_pd_format(y_estimated = y_validation_estimated,y = y_validation,\n",
    "                                          epoch=epoch, set_type = \"validation\")\n",
    "\n",
    "    with open(path_CSV + \"CSV_loss_\" + name_network + str(train_number) + \".csv\", 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerows([loss_train])\n",
    "        writer.writerows([loss_validation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and import entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Save_checkpoint save the network with weight and parameters.\n",
    "    (0) = state : state contains the network, the epoch and the parameter\n",
    "    (1) = filename : filname is the name of the file where the checkpoint should be saved\n",
    "\"\"\"\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    \n",
    "\"\"\" \n",
    "\n",
    "\"\"\"\n",
    "def load_from_checkpoint(path_checkpoint,network):\n",
    "    if (os.path.isfile(path_checkpoint)):\n",
    "        print(\"=> loading checkpoint '{}'\".format(path_checkpoint))\n",
    "        \n",
    "        checkpoint = torch.load(path_checkpoint)\n",
    "        parameters = checkpoint['parameters']\n",
    "        parameters.actual_epoch = checkpoint['epoch']\n",
    "        network.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(path_checkpoint, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(path_checkpoint))\n",
    "    return(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n",
    "           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n",
    "           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n",
    "zero_pad = 256 * 3 - len(palette)\n",
    "for i in range(zero_pad):\n",
    "    palette.append(0)\n",
    "\n",
    "\n",
    "def colorize_mask(mask):\n",
    "    # mask: numpy array of the mask\n",
    "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
    "    new_mask.putpalette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-12550b6296d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCityScapes_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msliding_crop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def make_dataset(quality, mode):\n",
    "    assert (quality == 'fine' and mode in ['train', 'val']) or \\\n",
    "           (quality == 'coarse' and mode in ['train', 'train_extra', 'val'])\n",
    "\n",
    "    if quality == 'coarse':\n",
    "        img_dir_name = path + '/leftImg8bit'\n",
    "        mask_path = os.path.join(parameters.path_data, 'gtCoarse', mode)\n",
    "        mask_postfix = '_gtCoarse_labelIds.png'\n",
    "    else:\n",
    "        img_dir_name = 'leftImg8bit'\n",
    "        mask_path = os.path.join(parameters.path_data, 'gtFine', mode)\n",
    "        mask_postfix = '_gtFine_labelIds.png'\n",
    "\n",
    "    img_path = os.path.join(parameters.path_data, img_dir_name, mode)\n",
    "    assert os.listdir(img_path) == os.listdir(mask_path)\n",
    "    items = []\n",
    "    categories = os.listdir(img_path)\n",
    "    for c in categories:\n",
    "        c_items = [name.split('_leftImg8bit.png')[0] for name in os.listdir(os.path.join(img_path, c))]\n",
    "        for it in c_items:\n",
    "            item = (os.path.join(img_path, c, it + '_leftImg8bit.png'), os.path.join(mask_path, c, it + mask_postfix))\n",
    "            items.append(item)\n",
    "    return items\n",
    "\n",
    "\n",
    "class CityScapes_final(data.Dataset):\n",
    "    def __init__(self, quality, mode, joint_transform=None, sliding_crop=None, transform=None, target_transform=None):\n",
    "        self.imgs = make_dataset(quality, mode)\n",
    "        if len(self.imgs) == 0:\n",
    "            raise RuntimeError('Found 0 images, please check the data set')\n",
    "        self.quality = quality\n",
    "        self.mode = mode\n",
    "        self.joint_transform = joint_transform\n",
    "        self.sliding_crop = sliding_crop\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        ignore_label = parameters.number_classes - 1\n",
    "        self.id_to_trainid = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "                              3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "                              7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "                              14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "                              18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "                              28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, mask_path = self.imgs[index]\n",
    "        img, mask = Image.open(img_path).convert('RGB'), Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        \n",
    "        mask_copy = mask.copy()\n",
    "        \n",
    "        for k, v in self.id_to_trainid.items():\n",
    "            mask_copy[mask == k] = v\n",
    "            \n",
    "        mask = Image.fromarray(mask_copy.astype(np.uint8))\n",
    "\n",
    "\n",
    "        if self.joint_transform is not None:\n",
    "            img, mask = self.joint_transform(img, mask)\n",
    "                 \n",
    "        if self.sliding_crop is not None:\n",
    "            img_slices, mask_slices, slices_info = self.sliding_crop(img, mask)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                img_slices = [self.transform(e) for e in img_slices]\n",
    "            if self.target_transform is not None:\n",
    "                mask_slices = [self.target_transform(e) for e in mask_slices]\n",
    "            img, mask = torch.stack(img_slices, 0), torch.stack(mask_slices, 0)\n",
    "            mask = torch.squeeze(mask)\n",
    "            return img, mask.long(), torch.LongTensor(slices_info)\n",
    "        else:\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            if self.target_transform is not None:\n",
    "\n",
    "                mask = self.target_transform(mask)*255\n",
    "\n",
    "            mask = torch.squeeze(mask)\n",
    "            #mask_copy = torch.rand(19,19,19)            \n",
    "            #for k in range(19):\n",
    "            #    mask_copy[k,:,:] = (mask == k)*1\n",
    "\n",
    "            return img, mask.long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" checkpoint save the network if this network is better than the previous one or just save it regulary\n",
    "    (0) = validation_error : The error on the validation DataSet \n",
    "    (1) = validation_error_min : The last best validation error\n",
    "    (2) = index_save_best : Index is 0 or 1, there is always a checkpoint untouched (best0 or best1)\n",
    "    (3) = index_save_regular : Index is 0 or 1, there is always a checkpoint untouched(regular0 or regular1)\n",
    "    (4) = epoch :\n",
    "    (5) = network : \n",
    "    (6) = parameters : \n",
    "\"\"\"\n",
    "def checkpoint(validation_error,validation_error_min,index_save_best,index_save_regular,epoch,network,parameters):\n",
    "    \n",
    "    if(validation_error < validation_error_min):    \n",
    "        #Save the entire model with parameter, network and optimizer\n",
    "        save_checkpoint({'epoch': epoch + 1, # +1 because we start to count at 0\n",
    "                         'parameters': parameters,\n",
    "                         'state_dict': network.state_dict(),\n",
    "                        },\n",
    "                        filename = parameters.path_save_net + \"best\" +str(index_save_best)+ parameters.name_network +\n",
    "                        str(parameters.train_number) + \"checkpoint.pth.tar\")\n",
    "        \n",
    "        validation_error_min = validation_error\n",
    "        \n",
    "        print(\"The network as been saved at the epoch \" + str(epoch) + \"(best score)\" + str(index_save_best))\n",
    "        \n",
    "        index_save_best = (index_save_best+1)%2\n",
    "        \n",
    "    else:\n",
    "        #No condition for the moment, but maybe it is not always necessary to save the network\n",
    "        if(True):\n",
    "            \n",
    "            #Save the entire model with parameter, network and optimizer\n",
    "            save_checkpoint({'epoch': epoch + 1, # +1 because we start to count at 0\n",
    "                             'parameters': parameters,\n",
    "                             'state_dict': network.state_dict(),\n",
    "                            },\n",
    "                            filename = parameters.path_save_net + \"save\" + str(index_save_regular) +\n",
    "                            parameters.name_network + str(parameters.train_number) + \"checkpoint.pth.tar\")\n",
    "            validation_error_min = validation_error\n",
    "            \n",
    "\n",
    "            print(\"The network as been saved at the epoch \" + str(epoch) + \"(regular save)\" + str(index_save_regular))\n",
    "            \n",
    "            index_save_regular = (index_save_regular+1)%2\n",
    "        \n",
    "    return(validation_error_min,index_save_best,index_save_regular)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
