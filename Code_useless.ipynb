{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "905\n"
     ]
    }
   ],
   "source": [
    "scale = (0.2,0.37)\n",
    "ratio = (0.75,4./3.)\n",
    "aspect_ratio = random.uniform(*ratio)\n",
    "target_area = random.uniform(*scale) * area\n",
    "\n",
    "w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "print(h)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0762939453125"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400*400/(2048*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37509377344336087"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*1024/(2048*1024*1.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8812003892856966"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(*scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1872448.479571874"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_area = random.uniform(*scale) * area\n",
    "target_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154968.3\n",
      "1809690.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9558105.0"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100a + b = 17306520\n",
    "#20a + b = 4909056\n",
    "#50a + b = 8854292\n",
    "a = (17306520 - 4909056)/80\n",
    "print(a)\n",
    "b = (-20*a) + 4909056\n",
    "print(b)\n",
    "a*50 +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class color():\n",
    "    convolution        = \"darkgoldenrod1\"\n",
    "    subSampling        = \"darkgoldenrod\" \n",
    "    fullConvolution    = \"firebrick1\"\n",
    "    upSampling         = \"firebrick\"\n",
    "    batchNormalization = \"deepskyblue3\"\n",
    "    relu               = \"darkolivegreen3\"\n",
    "    add                = \"bisque3\"\n",
    "    dropout            = \"darkviolet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class Label():\n",
    "    def __init__(self, name, total_id, train_id, category, cat_id, has_instances, ignore_in_eval, color):\n",
    "        self.name = name\n",
    "        self.total_id = total_id\n",
    "        self.train_id = train_id\n",
    "        self.category = category\n",
    "        self.cat_id = cat_id\n",
    "        self.has_instances = has_instances\n",
    "        self.ignore_in_eval = ignore_in_eval\n",
    "        self.color = color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Definitions\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# a label and all meta information\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# A list of all labels\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Please adapt the train IDs as appropriate for your approach.\n",
    "# Note that you might want to ignore labels with ID 255 during training.\n",
    "# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n",
    "# Make sure to provide your results using the original IDs and not the training IDs.\n",
    "# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Create dictionaries for a fast lookup\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Please refer to the main method below for example usages!\n",
    "\n",
    "# name to label object\n",
    "name2label      = { label.name    : label for label in labels           }\n",
    "# id to label object\n",
    "id2label        = { label.id      : label for label in labels           }\n",
    "# trainId to label object\n",
    "trainId2label   = { label.trainId : label for label in reversed(labels) }\n",
    "# category to list of label objects\n",
    "category2labels = {}\n",
    "for label in labels:\n",
    "    category = label.category\n",
    "    if category in category2labels:\n",
    "        category2labels[category].append(label)\n",
    "    else:\n",
    "        category2labels[category] = [label]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Assure single instance name\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# returns the label name that describes a single instance (if possible)\n",
    "# e.g.     input     |   output\n",
    "#        ----------------------\n",
    "#          car       |   car\n",
    "#          cargroup  |   car\n",
    "#          foo       |   None\n",
    "#          foogroup  |   None\n",
    "#          skygroup  |   None\n",
    "def assureSingleInstanceName( name ):\n",
    "    # if the name is known, it is not a group\n",
    "    if name in name2label:\n",
    "        return name\n",
    "    # test if the name actually denotes a group\n",
    "    if not name.endswith(\"group\"):\n",
    "        return None\n",
    "    # remove group\n",
    "    name = name[:-len(\"group\")]\n",
    "    # test if the new name exists\n",
    "    if not name in name2label:\n",
    "        return None\n",
    "    # test if the new name denotes a label that actually has instances\n",
    "    if not name2label[name].hasInstances:\n",
    "        return None\n",
    "    # all good then\n",
    "    return name\n",
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------\n",
    "# Main for testing\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# just a dummy main\n",
    "if __name__ == \"__main__\":\n",
    "    # Print all the labels\n",
    "    print(\"List of cityscapes labels:\")\n",
    "    print(\"\")\n",
    "    print(\"    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}\".format( 'name', 'id', 'trainId', 'category', 'categoryId', 'hasInstances', 'ignoreInEval' ))\n",
    "    print(\"    \" + ('-' * 98))\n",
    "    for label in labels:\n",
    "        print(\"    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}\".format( label.name, label.id, label.trainId, label.category, label.categoryId, label.hasInstances, label.ignoreInEval ))\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Example usages:\")\n",
    "\n",
    "    # Map from name to label\n",
    "    name = 'car'\n",
    "    id   = name2label[name].id\n",
    "    print(\"ID of label '{name}': {id}\".format( name=name, id=id ))\n",
    "\n",
    "    # Map from ID to label\n",
    "    category = id2label[id].category\n",
    "    print(\"Category of label with ID '{id}': {category}\".format( id=id, category=category ))\n",
    "\n",
    "    # Map from trainID to label\n",
    "    trainId = 0\n",
    "    name = trainId2label[trainId].name\n",
    "    print(\"Name of label with trainID '{id}': {name}\".format( id=trainId, name=name ))\n",
    "\"\"\"\n",
    "a=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "\n",
    "#from ..utils.cityscapes import labels as cityscapes_labels\n",
    "cityscapes_labels = labels\n",
    "\n",
    "class Cityscapes(data.Dataset):\n",
    "    \n",
    "    # Images name subfolder in the root folder of the dataset\n",
    "    images_subfolder = 'leftImg8bit'\n",
    "    \n",
    "    # Annotation name subfolder in the root folder of the dataset\n",
    "    annotations_subfolder = 'gtFine'\n",
    "        \n",
    "    # Names of the folders containing train/val/test splits\n",
    "    dataset_types = ['train', 'val', 'test']\n",
    "    \n",
    "    # Train labels which are used to map the labels\n",
    "    # in the annotation images into train labels\n",
    "    # Some variables in the annotatio are ignore for example\n",
    "    # See utils.cityscapes for more details\n",
    "    ordered_train_labels = np.asarray( list(map(lambda x: x.trainId,cityscapes_labels) ))\n",
    "    print(ordered_train_labels)\n",
    "    number_of_classes = 19\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dataset_root,\n",
    "                 dataset_type=0,\n",
    "                 train=True,\n",
    "                 joint_transform=None):\n",
    "        \n",
    "        # dataset_root should point to a folder\n",
    "        # with gtFine and leftImg8bit folders containing\n",
    "        # annotations and images respectively.\n",
    "        \n",
    "        # dataset_type:\n",
    "        # 0 - train\n",
    "        # 1 - val\n",
    "        # 2 - test\n",
    "        \n",
    "        self.dataset_root = dataset_root\n",
    "        self.joint_transform = joint_transform\n",
    "\n",
    "        dataset_type_name = self.dataset_types[dataset_type]\n",
    "\n",
    "        images_folder_path = os.path.join(dataset_root, self.images_subfolder, dataset_type_name)\n",
    "        annotations_folder_path = os.path.join(dataset_root, self.annotations_subfolder, dataset_type_name)\n",
    "\n",
    "        self.images_filenames = []\n",
    "        self.annotations_filenames = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(images_folder_path):\n",
    "\n",
    "            for filename in filenames:\n",
    "\n",
    "                image_filename = os.path.join(dirpath, filename)\n",
    "\n",
    "                annotation_filename = os.path.join( dirpath.replace(images_folder_path, annotations_folder_path),\n",
    "                                                    filename.replace('leftImg8bit', 'gtFine_labelTrainIds') )\n",
    "\n",
    "                self.images_filenames.append( image_filename )\n",
    "                self.annotations_filenames.append( annotation_filename )\n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_path = self.images_filenames[index]\n",
    "        annotation_path = self.annotations_filenames[index]\n",
    "\n",
    "        _img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # TODO: maybe can be done in a better way\n",
    "        _target = Image.open(annotation_path)\n",
    "        \n",
    "        _target_np = np.asarray(_target)\n",
    "        \n",
    "        # https://stackoverflow.com/questions/8188726/how-do-i-do-this-array-lookup-replace-with-numpy\n",
    "        _target_np = self.ordered_train_labels[_target_np].astype(np.uint8)\n",
    "        \n",
    "        _target = Image.fromarray(_target_np)\n",
    "\n",
    "        if self.joint_transform is not None:\n",
    "            _img, _target = self.joint_transform([_img, _target])\n",
    "        return _img, _target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Labels: -1 license plate, 0 unlabeled, 1 ego vehicle, 2 rectification border, 3 out of roi, 4 static, 5 dynamic, 6 ground, 7 road, 8 sidewalk, 9 parking, 10 rail track, 11 building, 12 wall, 13 fence, 14 guard rail, 15 bridge, 16 tunnel, 17 pole, 18 polegroup, 19 traffic light, 20 traffic sign, 21 vegetation, 22 terrain, 23 sky, 24 person, 25 rider, 26 car, 27 truck, 28 bus, 29 caravan, 30 trailer, 31 train, 32 motorcycle, 33 bicycle\n",
    "num_classes = 20\n",
    "full_to_train = {-1: 19, 0: 19, 1: 19, 2: 19, 3: 19, 4: 19, 5: 19, 6: 19, 7: 0, 8: 1, 9: 19, 10: 19, 11: 2, 12: 3, 13: 4, 14: 19, 15: 19, 16: 19, 17: 5, 18: 19, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14, 28: 15, 29: 19, 30: 19, 31: 16, 32: 17, 33: 18}\n",
    "train_to_full = {0: 7, 1: 8, 2: 11, 3: 12, 4: 13, 5: 17, 6: 19, 7: 20, 8: 21, 9: 22, 10: 23, 11: 24, 12: 25, 13: 26, 14: 27, 15: 28, 16: 31, 17: 32, 18: 33, 19: 0}\n",
    "full_to_colour = {0: (0, 0, 0), 7: (128, 64, 128), 8: (244, 35, 232), 11: (70, 70, 70), 12: (102, 102, 156), 13: (190, 153, 153), 17: (153, 153, 153), 19: (250, 170, 30), 20: (220, 220, 0), 21: (107, 142, 35), 22: (152, 251, 152), 23: (70, 130, 180), 24: (220, 20, 60), 25: (255, 0, 0), 26: (0, 0, 142), 27: (0, 0, 70), 28: (0, 60,100), 31: (0, 80, 100), 32: (0, 0, 230), 33: (119, 11, 32)}\n",
    "\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, split, crop=None, flip=False):\n",
    "        super().__init__()\n",
    "        self.crop = crop\n",
    "        self.flip = flip\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        for root, _, filenames in os.walk(path+\"/leftImg8bit/\"+split):\n",
    "            for filename in filenames:\n",
    "                if os.path.splitext(filename)[1] == '.png':\n",
    "                    filename_base = '_'.join(filename.split('_')[:-1])\n",
    "                    target_root = os.path.join(path+\"/gtFine\", split, os.path.basename(root))\n",
    "                    self.inputs.append(os.path.join(root, filename_base + '_leftImg8bit.png'))\n",
    "                    self.targets.append(os.path.join(target_root, filename_base + '_gtFine_labelIds.png'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Load images and perform augmentations with PIL\n",
    "        img, target = Image.open(self.inputs[i]), Image.open(self.targets[i])\n",
    "        # Random uniform crop\n",
    "        if self.crop is not None:\n",
    "            w, h = img.size\n",
    "            x1, y1 = random.randint(0, w - self.crop), random.randint(0, h - self.crop)\n",
    "            img, target = img.crop((x1, y1, x1 + self.crop, y1 + self.crop)), target.crop((x1, y1, x1 + self.crop, y1 + self.crop))\n",
    "        # Random horizontal flip\n",
    "        if self.flip:\n",
    "            if random.random() < 0.5:\n",
    "                img, target = img.transpose(Image.FLIP_LEFT_RIGHT), target.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        w, h = img.size\n",
    "        img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(h, w, 3).permute(2, 0, 1).float().div(255)\n",
    "        target = torch.ByteTensor(torch.ByteStorage.from_buffer(target.tobytes())).view(h, w).long()\n",
    "        \n",
    "        # Normalise input\n",
    "        img[0].add_(-0.485).div_(0.229)\n",
    "        img[1].add_(-0.456).div_(0.224)\n",
    "        img[2].add_(-0.406).div_(0.225)\n",
    "        # Convert to training labels\n",
    "        remapped_target = target.clone()\n",
    "        for k, v in full_to_train.items():\n",
    "            remapped_target[target == k] = v\n",
    "        # Create one-hot encoding\n",
    "        target = torch.zeros(num_classes, h, w)\n",
    "        for c in range(num_classes):\n",
    "            target[c][remapped_target == c] = 1\n",
    "        return img, target, remapped_target # Return x, y (one-hot), y (index)\n",
    "            #return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test function : IoU\n",
    "    return a matrix of confusion\n",
    "\"\"\"\n",
    "def IoU(y_train_estimated, y_train):\n",
    "    \n",
    "    #We keep only the higest value, which is the prediction\n",
    "    pred = torch.max(y_train_estimated, dim=1)[1]\n",
    "\n",
    "    confusion_matrix = [[0] * parameters.number_classes for i in range(parameters.number_classes)]\n",
    "    \n",
    "    #For each classes : [0] = TP [1] = FP [2] = FN\n",
    "    IoU_each_classes = np.zeros(shape = (3,parameters.number_classes))\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    target = y_train.view(-1)\n",
    "\n",
    "    # Double loop over the number of classes at each iteration we add the intersection\n",
    "    for cls1 in range(parameters.number_classes):\n",
    "        pred_inds = pred == cls1\n",
    "        for cls2 in range(parameters.number_classes):\n",
    "            target_inds = target == cls2\n",
    "            intersection = (pred_inds*target_inds).long().sum().data.cpu()[0]\n",
    "            confusion_matrix[cls1][cls2] = intersection\n",
    "\n",
    "            # TP here for each classes\n",
    "            if(cls1 == cls2):\n",
    "                IoU_each_classes[0,cls1] = intersection\n",
    "            # FN and FP here for each classes\n",
    "            else:\n",
    "                IoU_each_classes[1,cls1] = IoU_each_classes[1,cls1] + intersection\n",
    "                IoU_each_classes[2,cls2] = IoU_each_classes[2,cls1] + intersection\n",
    "    \n",
    "    # compute the IoU value\n",
    "    IoU_each_classes_total = IoU_each_classes[0,:]/np.sum(IoU_each_classes)\n",
    "\n",
    "    return(confusion_matrix,IoU_each_classes_total, np.mean((np.float32(IoU_each_classes_total))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import re\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "def make_dot(var):\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if isinstance(var, Variable):\n",
    "                value = '('+(', ').join(['%d'% v for v in var.size()])+')'\n",
    "                dot.node(str(id(var)), str(value), fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'previous_functions'):\n",
    "                for u in var.previous_functions:\n",
    "                    dot.edge(str(id(u[0])), str(id(var)))\n",
    "                    add_nodes(u[0])\n",
    "    add_nodes(var.creator)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test function : IoU\n",
    "    return a matrix of confusion\n",
    "\"\"\"\n",
    "def IoU(y_train_estimated, y_train):\n",
    "    \n",
    "    #We keep only the higest value, which is the prediction\n",
    "    pred = torch.max(y_train_estimated, dim=1)[1]\n",
    "\n",
    "    confusion_matrix = [[0] * parameters.number_classes for i in range(parameters.number_classes)]\n",
    "    \n",
    "    pred = pred.view(-1)\n",
    "    target = y_train.view(-1)\n",
    "\n",
    "    # Double loop over the number of classes at each iteration we add the intersection\n",
    "    for cls1 in range(parameters.number_classes):\n",
    "        pred_inds = pred == cls1\n",
    "        for cls2 in range(parameters.number_classes):\n",
    "            target_inds = target == cls2\n",
    "            intersection = (pred_inds*target_inds).long().sum().data.cpu()[0]\n",
    "            confusion_matrix[cls1][cls2] = intersection\n",
    "\n",
    "    return(confusion_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable, Function\n",
    "from collections import defaultdict\n",
    "import graphviz\n",
    "\n",
    "\"\"\"\n",
    "This is a rather distorted implementation of graph visualization in PyTorch.\n",
    "This implementation is distorted because PyTorch's autograd is undergoing refactoring right now.\n",
    "- neither func.next_functions nor func.previous_functions can be relied upon\n",
    "- BatchNorm's C backend does not follow the python Function interface\n",
    "- I'm not even sure whether to use var.creator or var.grad_fn (apparently the source tree and wheel builds use different\n",
    "  interface now)\n",
    "As a result, we are forced to manually trace the graph, using 2 redundant mechanisms:\n",
    "- Function.__call__: this allows us to trace all Function creations. Function corresponds to Op in TF\n",
    "- Module.forward_hook: this is needed because the above method doesn't work for BatchNorm, as the current C backend does\n",
    "  not follow the Python Function interface. \n",
    "To do graph visualization, follow these steps:\n",
    "1. register hooks on model: register_vis_hooks(model)\n",
    "2. pass data through model: output = model(input)\n",
    "3. remove hooks           : remove_vis_hooks()\n",
    "4. perform visualization  : save_visualization(name, format='svg') # name is a string without extension\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "old_function__call__ = Function.__call__\n",
    "\n",
    "def register_creator(inputs, creator, output):\n",
    "    \"\"\"\n",
    "    In the forward pass, our Function.__call__ and BatchNorm.forward_hook both call this method to register the creators\n",
    "    inputs: list of input variables\n",
    "    creator: one of\n",
    "        - Function\n",
    "        - BatchNorm module\n",
    "    output: a single output variable\n",
    "    \"\"\"\n",
    "    cid = id(creator)\n",
    "    oid = id(output)\n",
    "    if oid in vars: \n",
    "        return\n",
    "    # connect creator to input\n",
    "    for input in inputs:\n",
    "        iid = id(input)\n",
    "        func_trace[cid][iid] = input\n",
    "        # register input\n",
    "        vars[iid] = input\n",
    "    # connect output to creator\n",
    "    assert type(output) not in [tuple, list, dict]\n",
    "    var_trace[oid][cid] = creator\n",
    "    # register creator and output and all inputs\n",
    "    vars[oid] = output\n",
    "    funcs[cid] = creator\n",
    "\n",
    "hooks = []\n",
    "\n",
    "def register_vis_hooks(model):\n",
    "    global var_trace, func_trace, vars, funcs\n",
    "    remove_vis_hooks()\n",
    "    var_trace  = defaultdict(lambda: {})     # map oid to {cid:creator}\n",
    "    func_trace = defaultdict(lambda: {})     # map cid to {iid:input}\n",
    "    vars  = {}                               # map vid to Variable/Parameter\n",
    "    funcs = {}                               # map cid to Function/BatchNorm module\n",
    "    hooks = []                               # contains the forward hooks, needed for hook removal\n",
    "\n",
    "    def hook_func(module, inputs, output):\n",
    "        assert 'BatchNorm' in mod.__class__.__name__        # batchnorms don't have shared superclass\n",
    "        inputs = list(inputs)\n",
    "        for p in [module.weight, module.bias]:\n",
    "            if p is not None:\n",
    "                inputs.append(p)\n",
    "        register_creator(inputs, module, output)\n",
    "\n",
    "    for mod in model.modules():\n",
    "        if 'BatchNorm' in mod.__class__.__name__:           # batchnorms don't have shared superclass\n",
    "            hook = mod.register_forward_hook(hook_func)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    def new_function__call__(self, *args, **kwargs):\n",
    "        inputs =  [a for a in args            if isinstance(a, Variable)]\n",
    "        inputs += [a for a in kwargs.values() if isinstance(a, Variable)]\n",
    "        output = old_function__call__(self, *args, **kwargs)\n",
    "        register_creator(inputs, self, output)\n",
    "        return output\n",
    "\n",
    "    Function.__call__ = new_function__call__\n",
    "\n",
    "\n",
    "def remove_vis_hooks():\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    Function.__call__ = old_function__call__\n",
    "\n",
    "\n",
    "def save_visualization(name, format='svg'):\n",
    "    g = graphviz.Digraph(format=format)\n",
    "    def sizestr(var):\n",
    "        size = [int(i) for i in list(var.size())]\n",
    "        return str(size)\n",
    "    # add variable nodes\n",
    "    for vid, var in vars.iteritems():\n",
    "        if isinstance(var, nn.Parameter):\n",
    "            g.node(str(vid), label=sizestr(var), shape='ellipse', style='filled', fillcolor='red')\n",
    "        elif isinstance(var, Variable):\n",
    "            g.node(str(vid), label=sizestr(var), shape='ellipse', style='filled', fillcolor='lightblue')\n",
    "        else:\n",
    "            assert False, var.__class__\n",
    "    # add creator nodes\n",
    "    for cid in func_trace:\n",
    "        creator = funcs[cid]\n",
    "        g.node(str(cid), label=str(creator.__class__.__name__), shape='rectangle', style='filled', fillcolor='orange')\n",
    "    # add edges between creator and inputs\n",
    "    for cid in func_trace:\n",
    "        for iid in func_trace[cid]:\n",
    "            g.edge(str(iid), str(cid))\n",
    "    # add edges between outputs and creators\n",
    "    for oid in var_trace:\n",
    "        for cid in var_trace[oid]:\n",
    "            g.edge(str(cid), str(oid))\n",
    "    g.render(name)\n",
    "class subSamplingSequence(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) = nInput : number of features map for the input\n",
    "    (2) = nOutput : number of features map for the output\n",
    "    This class represente a bloc that reduce the resolution of each feature map(factor2)\n",
    "    \"\"\"\n",
    "    def __init__(self, nInputs, nOutputs):\n",
    "        super(subSamplingSequence, self).__init__()\n",
    "        self.Seque\n",
    "        self.batch1 = nn.BatchNorm2d(num_features = nInputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = nInputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(2,2), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        \n",
    "        self.batch2 = nn.BatchNorm2d(num_features = nOutputs, eps=1e-05, momentum=0.1,affine=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = nOutputs, out_channels = nOutputs,\n",
    "                            kernel_size = (3,3), stride=(1,1), padding=(1,1), dilation=1, groups=1, bias=True)\n",
    "\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "network = subSamplingSequence(nInputs = 3,nOutputs = 6)\n",
    "print(network)\n",
    "a = torch.randn(2, 3, 7, 7)\n",
    "inputs = Variable(a)\n",
    "out = network(inputs)\n",
    "print(out.size())\n",
    "\n",
    "def visualize(a,network):\n",
    "    global recon\n",
    "    inputs = Variable(a)\n",
    "    register_vis_hooks(network)\n",
    "    recon = network(inputs)\n",
    "    remove_vis_hooks()\n",
    "    save_visualization('pytorch_model', 'png')\n",
    "    \n",
    "visualize(a,network)\n",
    "#resnet18 = models.resnet18()\n",
    "#y = resnet18(inputs)\n",
    "# print(y)\n",
    "\n",
    "g = make_dot(out)\n",
    "g.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
